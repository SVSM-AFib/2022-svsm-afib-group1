{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path, PurePath\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from joblib import Memory\n",
    "from shutil import rmtree\n",
    "\n",
    "import timeit\n",
    "\n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\n",
    "\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "import catboost as cb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlist = []\n",
    "records = PurePath(Path(os.getcwd()).parents[1], Path('mit-bih-dataframes/subject_list.csv'))\n",
    "with open(records) as rfile:\n",
    "    recordreader = csv.reader(rfile, delimiter=' ', quotechar='|')\n",
    "    for row in recordreader:\n",
    "        rlist.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_dict = {\n",
    "    \"Model name\": [],\n",
    "    \"Avg Accuracy\": [],\n",
    "    \"Std Accuracy\": [],\n",
    "    \"Sensitivity\": [],\n",
    "    \"Specificity\": [],\n",
    "    \"Precision\": [],\n",
    "    \"F1 score\": [],\n",
    "    \"Run time\": [],\n",
    "    \"TPS\": []\n",
    "}\n",
    "\n",
    "moving_accuracy = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dfs = {}\n",
    "for record in tqdm(rlist):\n",
    "    feature_dfs[record] = pd.read_parquet(PurePath(Path(os.getcwd()).parents[1], Path('mit-bih-time-features/' + record+ '.parquet')))\n",
    "\n",
    "combined_features = pd.concat([feature_dfs[key][1:] for key in feature_dfs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = combined_features[['StoS', 'StoR', 'StoL', 'RtoS', 'RtoR', 'RtoL', 'LtoS', 'LtoR', 'LtoL', 'std', 'cov', 'range', 'rrInt_var', 'rmean_var', 'rmssd', 'mad', 'iqr']]\n",
    "y = combined_features['mappedLabel'].map({\"Non-Afib\": 0, \"Afib\": 1})\n",
    "groups = combined_features['subjectID'].astype('int64')\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "splits = list(logo.split(X, y, groups=groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_reporter(initial_results):\n",
    "    initial_columns = initial_results.axes[0].tolist()\n",
    "    results = initial_results.dropna()\n",
    "    changed_columns = results.axes[0].tolist()\n",
    "    \n",
    "    dropped_cols = list(set(initial_columns).difference(changed_columns))\n",
    "    \n",
    "    print(dropped_cols)\n",
    "\n",
    "    acc_scores = []\n",
    "    for x in range(len(rlist)):\n",
    "        col_name = 'split'+str(x)+'_test_accuracy'\n",
    "        if col_name not in dropped_cols:\n",
    "            acc_scores.append(results[col_name])\n",
    "\n",
    "    spec_scores = []\n",
    "    for x in range(len(rlist)):\n",
    "        col_name = 'split'+str(x)+'_test_specificity'\n",
    "        if col_name not in dropped_cols:\n",
    "            spec_scores.append(results[col_name])\n",
    "\n",
    "    sens_scores = []\n",
    "    for x in range(len(rlist)):\n",
    "        col_name = 'split'+str(x)+'_test_sensitivity'\n",
    "        if col_name not in dropped_cols:\n",
    "            sens_scores.append(results[col_name])\n",
    "\n",
    "\n",
    "    prec_scores = []\n",
    "    for x in range(len(rlist)):\n",
    "        col_name = 'split'+str(x)+'_test_precision'\n",
    "        if col_name not in dropped_cols:\n",
    "            prec_scores.append(results[col_name])\n",
    "\n",
    "    f1_scores = []\n",
    "    for x in range(len(rlist)):\n",
    "        col_name = 'split'+str(x)+'_test_f1_score'\n",
    "        if col_name not in dropped_cols:\n",
    "            f1_scores.append(results[col_name])\n",
    "        \n",
    "    elapsed_times = []\n",
    "    for x in range(len(rlist)):\n",
    "        col_name = 'split'+str(x)+'_test_elapsed'\n",
    "        if col_name not in dropped_cols:\n",
    "            elapsed_times.append(results[col_name])\n",
    "\n",
    "    eps_times = []\n",
    "    for x in range(len(rlist)):\n",
    "        col_name = 'split'+str(x)+'_test_eps'\n",
    "        if col_name not in dropped_cols:\n",
    "            eps_times.append(results[col_name])\n",
    "\n",
    "    print('---Run time of each fold: \\n {}'.format(elapsed_times))\n",
    "    print(\"Avg run time: {}\".format(np.mean(elapsed_times)))\n",
    "    print('---Run time per subset of each fold is: \\n {}'.format(eps_times))\n",
    "    print(\"Avg run time per subset: {}\".format(np.mean(eps_times)))\n",
    "    print()\n",
    "    print('Accuracy of each fold: \\n {}'.format(acc_scores))\n",
    "    print(\"Avg accuracy: {}\".format(np.mean(acc_scores)))\n",
    "    print('Std of accuracy : \\n{}'.format(np.std(acc_scores)))\n",
    "    print()\n",
    "    print('Specificity of each fold: \\n {}'.format(spec_scores))\n",
    "    print(\"Avg specificity: {}\".format(np.mean(spec_scores)))\n",
    "    print('Std of specificity: \\n{}'.format(np.std(spec_scores)))\n",
    "    print()\n",
    "    print('Sensitivity of each fold: \\n {}'.format(sens_scores))\n",
    "    print(\"Avg sensitivity: {}\".format(np.mean(sens_scores)))\n",
    "    print('Std of sensitivity: \\n{}'.format(np.std(sens_scores)))\n",
    "    print()\n",
    "    print('Precision of each fold: \\n {}'.format(prec_scores))\n",
    "    print(\"Avg precision: {}\".format(np.mean(prec_scores)))\n",
    "    print('Std of precision : \\n{}'.format(np.std(prec_scores)))\n",
    "    print()\n",
    "    print('F1-scores of each fold: \\n {}'.format(f1_scores))\n",
    "    print(\"Avg F1-scores: {}\".format(np.mean(f1_scores)))\n",
    "    print('Std of F1-scores : \\n{}'.format(np.std(f1_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer(clf, X, y):\n",
    "    global moving_acc\n",
    "\n",
    "    start_time = timeit.default_timer()\n",
    "    y_pred = clf.predict(X)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "    total_seen = 0\n",
    "    total_correct = 0\n",
    "    subject_acc = []\n",
    "    for idx, pred in enumerate(y_pred):\n",
    "        total_seen+=1\n",
    "        if pred==y.iloc[idx]:\n",
    "            total_correct+=1\n",
    "        subject_acc.append(total_correct/total_seen)\n",
    "    moving_acc.append(subject_acc)\n",
    "\n",
    "    fold_size = len(X)\n",
    "\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "\n",
    "    sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "    specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "    precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "    f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "    return {'sensitivity': sensitivity, 'specificity': specificity,\n",
    "            'precision': precision, 'f1_score': f1_score,\n",
    "            'accuracy': accuracy_score(y, y_pred), \n",
    "            'elapsed': elapsed, 'eps': elapsed/fold_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.seterr(all='ignore')\n",
    "moving_accs = []\n",
    "\n",
    "if os.path.exists('saved_gridsearch')==False:\n",
    "    os.mkdir('saved_gridsearch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost\n",
    "feature_selection_clf = XGBClassifier(n_estimators=100, max_depth=4, verbose=None, eval_metric='logloss', learning_rate=0.1, tree_method=\"gpu_hist\")\n",
    "\n",
    "location = \"cache\"\n",
    "memory = Memory(location=location, verbose=10)\n",
    "cb_pipe = Pipeline([\n",
    "    ('rfe', RFE(estimator=feature_selection_clf,\n",
    "                n_features_to_select=14)),\n",
    "    ('clf', CatBoostClassifier(\n",
    "                        learning_rate=0.1,\n",
    "                        loss_function='Logloss',\n",
    "                        task_type=\"GPU\"))\n",
    "], memory=memory)\n",
    "\n",
    "params = {\n",
    "    \"clf__iterations\": np.arange(450, 550, 25),\n",
    "    \"clf__depth\": np.arange(3, 7)\n",
    "}\n",
    "\n",
    "moving_acc = []\n",
    "\n",
    "start_time = timeit.default_timer() #defines start time so computational time can be calculated\n",
    "\n",
    "cb_search = GridSearchCV(cb_pipe, params, scoring=scorer, refit=\"accuracy\", cv=splits, verbose=3)\n",
    "cb_search.fit(X, y)\n",
    "\n",
    "moving_accs.append(moving_acc)\n",
    "with open('saved_gridsearch/cb_search.pickle', 'wb') as handle:\n",
    "    pickle.dump(cb_search, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "cb_scores = cb_search.cv_results_\n",
    "best_results = pd.DataFrame(cb_scores).iloc[cb_search.best_index_]\n",
    "score_reporter(best_results)\n",
    "\n",
    "# Delete the temporary cache before exiting\n",
    "memory.clear(warn=False)\n",
    "rmtree(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "feature_selection_clf = XGBClassifier(n_estimators=100, max_depth=4, verbose=None, eval_metric='logloss', learning_rate=0.1, tree_method=\"gpu_hist\")\n",
    "\n",
    "location = \"cache\"\n",
    "memory = Memory(location=location, verbose=10)\n",
    "xg_pipe = Pipeline([\n",
    "    ('rfe', RFE(estimator=feature_selection_clf,\n",
    "                n_features_to_select=14)),\n",
    "    ('clf', XGBClassifier(verbose=None, \n",
    "                          eval_metric='logloss', \n",
    "                          learning_rate=0.1, \n",
    "                          tree_method=\"gpu_hist\"))\n",
    "], memory=memory)\n",
    "\n",
    "params = {\n",
    "    \"clf__n_estimators\": np.arange(300, 550, 50),\n",
    "    \"clf__max_depth\": np.arange(3, 7)\n",
    "}\n",
    "\n",
    "moving_acc = []\n",
    "\n",
    "start_time = timeit.default_timer() #defines start time so computational time can be calculated\n",
    "\n",
    "xg_search = GridSearchCV(xg_pipe, params, scoring=scorer, refit=\"accuracy\", cv=splits, verbose=3)\n",
    "xg_search.fit(X, y)\n",
    "\n",
    "moving_accs.append(moving_acc)\n",
    "with open('saved_gridsearch/xg_search.pickle', 'wb') as handle:\n",
    "    pickle.dump(xg_search, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "xg_scores = xg_search.cv_results_\n",
    "best_results = pd.DataFrame(xg_scores).iloc[xg_search.best_index_]\n",
    "score_reporter(best_results)\n",
    "\n",
    "# Delete the temporary cache before exiting\n",
    "memory.clear(warn=False)\n",
    "rmtree(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "feature_selection_clf = XGBClassifier(n_estimators=100, max_depth=4, verbose=None, eval_metric='logloss', learning_rate=0.1, tree_method=\"gpu_hist\")\n",
    "\n",
    "location = \"cache\"\n",
    "memory = Memory(location=location, verbose=10)\n",
    "lg_pipe = Pipeline([\n",
    "    ('rfe', RFE(estimator=feature_selection_clf,\n",
    "                n_features_to_select=14)),\n",
    "    ('clf', lightgbm.LGBMClassifier(learning_rate=0.1, \n",
    "                                    random_state=42))\n",
    "], memory=memory)\n",
    "\n",
    "params = {\n",
    "    \"clf__n_estimators\": np.arange(200, 550, 50),\n",
    "    \"clf__max_depth\": np.arange(3, 7)\n",
    "}\n",
    "\n",
    "moving_acc = []\n",
    "\n",
    "start_time = timeit.default_timer() #defines start time so computational time can be calculated\n",
    "\n",
    "lg_search = GridSearchCV(lg_pipe, params, scoring=scorer, refit=\"accuracy\", cv=splits, verbose=3)\n",
    "lg_search.fit(X, y)\n",
    "\n",
    "moving_accs.append(moving_acc)\n",
    "with open('saved_gridsearch/lg_search.pickle', 'wb') as handle:\n",
    "    pickle.dump(lg_search, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "lg_scores = lg_search.cv_results_\n",
    "best_results = pd.DataFrame(lg_scores).iloc[lg_search.best_index_]\n",
    "score_reporter(best_results)\n",
    "\n",
    "# Delete the temporary cache before exiting\n",
    "memory.clear(warn=False)\n",
    "rmtree(location)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "afib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ada57d62bdba33f60f3d98aec0dc1db90359aaf5574901473d57c7ef0e255730"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
