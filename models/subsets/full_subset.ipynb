{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198cd38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import wfdb\n",
    "import copy as cp\n",
    "import scipy.signal as signal\n",
    "import scipy.stats as stats\n",
    "from sklearn import preprocessing\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c512ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lms-puvvalat\\.conda\\envs\\afib\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "import sklearn.linear_model as skl_lm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import neighbors\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import metrics\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import lightgbm\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import catboost as cb\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24e708a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_path = str(pathlib.Path(os.path.normpath('C:\\\\Users\\\\lms-puvvalat\\\\Documents\\\\GitHub\\\\2022-svsm-afib-group1\\\\')))\n",
    "\n",
    "rlist = []\n",
    "records = os.path.normpath(parent_path + '/mit-bih-dataframes/subject_list.csv')\n",
    "with open(records) as rfile:\n",
    "    recordreader = csv.reader(rfile, delimiter=' ', quotechar='|')\n",
    "    for row in recordreader:\n",
    "        rlist.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d4d4d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_dict = {\n",
    "    \"Model name\": [],\n",
    "    \"Avg Accuracy\": [],\n",
    "    \"Std Accuracy\": [],\n",
    "    \"Sensitivity\": [],\n",
    "    \"Specificity\": [],\n",
    "    \"Precision\": [],\n",
    "    \"F1 score\": [],\n",
    "    \"Run time\": [],\n",
    "    \"TPS\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fe20408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [00:00<00:00, 33.46it/s]\n"
     ]
    }
   ],
   "source": [
    "feature_dfs = {}\n",
    "for record in tqdm(rlist):\n",
    "    feature_dfs[record] = pd.read_csv(os.path.normpath(parent_path + '/mit-bih-time-features/'+record+'.csv'), index_col=0, dtype={'subjectID': str})\n",
    "\n",
    "combined_features = pd.concat([feature_dfs[key][1:] for key in feature_dfs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "847e5b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = combined_features[['StoS', 'StoR', 'StoL', 'RtoS', 'RtoR', 'RtoL', 'LtoS', 'LtoR', 'LtoL', 'rmssd', 'iqr', 'mad', 'cov']]\n",
    "y = combined_features['mappedLabel'].map({\"Non-Afib\": 0, \"Afib\": 1})\n",
    "groups = combined_features['subjectID'].astype('int64')\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "splits = list(logo.split(X, y, groups=groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09e620c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [00:18<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 0.09053720001247711 seconds ---\n",
      "---Run time per subset is 4.827183203638207e-07 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.9138755980861244, 0.9256382875448985, 0.9926093514328809, 0.9459573273441887, 0.9861826403718126, 0.9458880778588807, 0.8871329298363596, 0.9667703388443281, 0.9105485232067511, 0.8924560708151671, 0.9702322538436375, 0.9704510108864697, 0.8133986928104575, 0.9198590254367147, 0.4799002493765586, 0.9609475032010243, 0.9864260883959609, 0.9139486467730743, 0.6809977692151693, 0.9409355179704016, 0.9869267694821775, 0.9782543038356992, 0.9949525540076721]\n",
      "Avg accuracy: 0.9114908491554958\n",
      "Std of accuracy : \n",
      "0.1143626744796147\n",
      "confusion matrix: \n",
      "[[94204  8681]\n",
      " [ 9458 75214]]\n",
      "classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.91    102885\n",
      "           1       0.90      0.89      0.89     84672\n",
      "\n",
      "    accuracy                           0.90    187557\n",
      "   macro avg       0.90      0.90      0.90    187557\n",
      "weighted avg       0.90      0.90      0.90    187557\n",
      "\n",
      "0.9156242406570443\n",
      "0.8882983749055178\n",
      "0.9087611660975092\n",
      "0.9121797944293552\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression\n",
    "logreg = LogisticRegression(solver='liblinear')\n",
    "\n",
    "acc_score = []\n",
    "Truth = []\n",
    "Output = []\n",
    "\n",
    "total_time = 0\n",
    "total_tests = 0\n",
    "\n",
    "for train, test in tqdm(splits):\n",
    "    X_train = X.iloc[train]\n",
    "    y_train = y.iloc[train]\n",
    "\n",
    "    X_test = X.iloc[test]\n",
    "    y_test = y.iloc[test]\n",
    "\n",
    "    logreg.fit(X_train, y_train)\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "    pred_values = logreg.predict(X_test)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "    acc = accuracy_score(y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(y_test.values.reshape(y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "    total_time+=elapsed\n",
    "    total_tests+=len(X_test)\n",
    "    \n",
    "tps = total_time/total_tests\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % total_time)\n",
    "print(\"---Run time per subset is %s seconds ---\" % tps)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print(\"confusion matrix: \")\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "print(cm)\n",
    "print(\"classification report: \")\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "performance_dict['Model name'].append(\"Logistic regression\")\n",
    "performance_dict['Avg Accuracy'].append(np.mean(acc_score))\n",
    "performance_dict['Std Accuracy'].append(np.std(acc_score))\n",
    "performance_dict['Sensitivity'].append(sensitivity)\n",
    "performance_dict['Specificity'].append(specificity)\n",
    "performance_dict['Precision'].append(precision)\n",
    "performance_dict['F1 score'].append(f1_score)\n",
    "performance_dict['Run time'].append(total_time)\n",
    "performance_dict['TPS'].append(tps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b62f9e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [00:05<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 0.025081000028876588 seconds ---\n",
      "---Run time per subset is 1.337246811842618e-07 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.9142857142857143, 0.921366857586642, 0.9920060331825038, 0.936692869174621, 0.9860570280115564, 0.9472506082725061, 0.8944182918628111, 0.9661155671959404, 0.9035563592525618, 0.8952305456467169, 0.971540726202159, 0.9709694142042509, 0.8347494553376906, 0.9505056696291756, 0.44119700748129675, 0.947609901835254, 0.9892401920211885, 0.9231089521165857, 0.7191239099574123, 0.9310253699788583, 0.9852926156674497, 0.9620960434913923, 0.9917221885725823]\n",
      "Avg accuracy: 0.9119635356942117\n",
      "Std of accuracy : \n",
      "0.11660166415425957\n",
      "confusion matrix: \n",
      "[[94870  8015]\n",
      " [10071 74601]]\n",
      "classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.92      0.91    102885\n",
      "           1       0.90      0.88      0.89     84672\n",
      "\n",
      "    accuracy                           0.90    187557\n",
      "   macro avg       0.90      0.90      0.90    187557\n",
      "weighted avg       0.90      0.90      0.90    187557\n",
      "\n",
      "0.922097487486028\n",
      "0.8810586734693877\n",
      "0.9040317892911255\n",
      "0.9129752773955135\n"
     ]
    }
   ],
   "source": [
    "# Linear discriminant analysis\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "acc_score = []\n",
    "Truth = []\n",
    "Output = []\n",
    "\n",
    "total_time = 0\n",
    "total_tests = 0\n",
    "\n",
    "for train, test in tqdm(splits):\n",
    "    X_train = X.iloc[train]\n",
    "    y_train = y.iloc[train]\n",
    "\n",
    "    X_test = X.iloc[test]\n",
    "    y_test = y.iloc[test]\n",
    "\n",
    "    lda.fit(X_train, y_train)\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "    pred_values = lda.predict(X_test)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "    acc = accuracy_score(y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(y_test.values.reshape(y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "    total_time+=elapsed\n",
    "    total_tests+=len(X_test)\n",
    "    \n",
    "tps = total_time/total_tests\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % total_time)\n",
    "print(\"---Run time per subset is %s seconds ---\" % tps)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print(\"confusion matrix: \")\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "print(cm)\n",
    "print(\"classification report: \")\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "performance_dict['Model name'].append(\"LDA\")\n",
    "performance_dict['Avg Accuracy'].append(np.mean(acc_score))\n",
    "performance_dict['Std Accuracy'].append(np.std(acc_score))\n",
    "performance_dict['Sensitivity'].append(sensitivity)\n",
    "performance_dict['Specificity'].append(specificity)\n",
    "performance_dict['Precision'].append(precision)\n",
    "performance_dict['F1 score'].append(f1_score)\n",
    "performance_dict['Run time'].append(total_time)\n",
    "performance_dict['TPS'].append(tps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "700069e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/23 [00:00<?, ?it/s]C:\\Users\\lms-puvvalat\\.conda\\envs\\afib\\lib\\site-packages\\sklearn\\discriminant_analysis.py:887: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\lms-puvvalat\\.conda\\envs\\afib\\lib\\site-packages\\sklearn\\discriminant_analysis.py:887: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "  9%|███████▏                                                                           | 2/23 [00:00<00:01, 17.10it/s]C:\\Users\\lms-puvvalat\\.conda\\envs\\afib\\lib\\site-packages\\sklearn\\discriminant_analysis.py:887: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\lms-puvvalat\\.conda\\envs\\afib\\lib\\site-packages\\sklearn\\discriminant_analysis.py:887: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      " 17%|██████████████▍                                                                    | 4/23 [00:00<00:01, 16.44it/s]C:\\Users\\lms-puvvalat\\.conda\\envs\\afib\\lib\\site-packages\\sklearn\\discriminant_analysis.py:887: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\lms-puvvalat\\.conda\\envs\\afib\\lib\\site-packages\\sklearn\\discriminant_analysis.py:887: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      " 26%|█████████████████████▋                                                             | 6/23 [00:00<00:01, 16.24it/s]C:\\Users\\lms-puvvalat\\.conda\\envs\\afib\\lib\\site-packages\\sklearn\\discriminant_analysis.py:887: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\lms-puvvalat\\.conda\\envs\\afib\\lib\\site-packages\\sklearn\\discriminant_analysis.py:887: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      " 35%|████████████████████████████▊                                                      | 8/23 [00:00<00:00, 16.14it/s]C:\\Users\\lms-puvvalat\\.conda\\envs\\afib\\lib\\site-packages\\sklearn\\discriminant_analysis.py:887: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\lms-puvvalat\\.conda\\envs\\afib\\lib\\site-packages\\sklearn\\discriminant_analysis.py:887: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      " 43%|███████████████████████████████████▋                                              | 10/23 [00:00<00:00, 15.39it/s]C:\\Users\\lms-puvvalat\\.conda\\envs\\afib\\lib\\site-packages\\sklearn\\discriminant_analysis.py:887: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\lms-puvvalat\\.conda\\envs\\afib\\lib\\site-packages\\sklearn\\discriminant_analysis.py:887: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      " 52%|██████████████████████████████████████████▊                                       | 12/23 [00:00<00:00, 16.27it/s]C:\\Users\\lms-puvvalat\\.conda\\envs\\afib\\lib\\site-packages\\sklearn\\discriminant_analysis.py:887: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\lms-puvvalat\\.conda\\envs\\afib\\lib\\site-packages\\sklearn\\discriminant_analysis.py:887: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      " 61%|█████████████████████████████████████████████████▉                                | 14/23 [00:00<00:00, 16.18it/s]C:\\Users\\lms-puvvalat\\.conda\\envs\\afib\\lib\\site-packages\\sklearn\\discriminant_analysis.py:887: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\lms-puvvalat\\.conda\\envs\\afib\\lib\\site-packages\\sklearn\\discriminant_analysis.py:887: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      " 70%|█████████████████████████████████████████████████████████                         | 16/23 [00:00<00:00, 16.13it/s]C:\\Users\\lms-puvvalat\\.conda\\envs\\afib\\lib\\site-packages\\sklearn\\discriminant_analysis.py:887: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\lms-puvvalat\\.conda\\envs\\afib\\lib\\site-packages\\sklearn\\discriminant_analysis.py:887: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      " 78%|████████████████████████████████████████████████████████████████▏                 | 18/23 [00:01<00:00, 16.09it/s]C:\\Users\\lms-puvvalat\\.conda\\envs\\afib\\lib\\site-packages\\sklearn\\discriminant_analysis.py:887: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\lms-puvvalat\\.conda\\envs\\afib\\lib\\site-packages\\sklearn\\discriminant_analysis.py:887: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      " 87%|███████████████████████████████████████████████████████████████████████▎          | 20/23 [00:01<00:00, 16.06it/s]C:\\Users\\lms-puvvalat\\.conda\\envs\\afib\\lib\\site-packages\\sklearn\\discriminant_analysis.py:887: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\lms-puvvalat\\.conda\\envs\\afib\\lib\\site-packages\\sklearn\\discriminant_analysis.py:887: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      " 96%|██████████████████████████████████████████████████████████████████████████████▍   | 22/23 [00:01<00:00, 16.68it/s]C:\\Users\\lms-puvvalat\\.conda\\envs\\afib\\lib\\site-packages\\sklearn\\discriminant_analysis.py:887: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [00:01<00:00, 16.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 0.05888950001099147 seconds ---\n",
      "---Run time per subset is 3.13981882899553e-07 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.8829801777170199, 0.93214251043588, 0.9897435897435898, 0.877877596855699, 0.9888204999371938, 0.9279805352798054, 0.9058507061197041, 0.9467998035685055, 0.8230259192284509, 0.7884793235566125, 0.830334750845055, 0.9417660273025747, 0.7855119825708061, 0.3746552252528348, 0.519002493765586, 0.7633376013657703, 0.9566296970700215, 0.8438584316446912, 0.7710403569255729, 0.84223044397463, 0.9836584618527219, 0.9479009362730293, 0.9903089036947305]\n",
      "Avg accuracy: 0.8527798249991515\n",
      "Std of accuracy : \n",
      "0.14613036940370777\n",
      "confusion matrix: \n",
      "[[92884 10001]\n",
      " [17978 66694]]\n",
      "classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.90      0.87    102885\n",
      "           1       0.87      0.79      0.83     84672\n",
      "\n",
      "    accuracy                           0.85    187557\n",
      "   macro avg       0.85      0.85      0.85    187557\n",
      "weighted avg       0.85      0.85      0.85    187557\n",
      "\n",
      "0.9027943820770763\n",
      "0.7876747921390779\n",
      "0.8378344247803575\n",
      "0.8691022564059377\n"
     ]
    }
   ],
   "source": [
    "# Quadratic discriminant analysis\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "acc_score = []\n",
    "Truth = []\n",
    "Output = []\n",
    "\n",
    "total_time = 0\n",
    "total_tests = 0\n",
    "\n",
    "for train, test in tqdm(splits):\n",
    "    X_train = X.iloc[train]\n",
    "    y_train = y.iloc[train]\n",
    "\n",
    "    X_test = X.iloc[test]\n",
    "    y_test = y.iloc[test]\n",
    "\n",
    "    qda.fit(X_train, y_train)\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "    pred_values = qda.predict(X_test)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "    acc = accuracy_score(y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(y_test.values.reshape(y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "    total_time+=elapsed\n",
    "    total_tests+=len(X_test)\n",
    "    \n",
    "tps = total_time/total_tests\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % total_time)\n",
    "print(\"---Run time per subset is %s seconds ---\" % tps)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print(\"confusion matrix: \")\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "print(cm)\n",
    "print(\"classification report: \")\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "performance_dict['Model name'].append(\"QDA\")\n",
    "performance_dict['Avg Accuracy'].append(np.mean(acc_score))\n",
    "performance_dict['Std Accuracy'].append(np.std(acc_score))\n",
    "performance_dict['Sensitivity'].append(sensitivity)\n",
    "performance_dict['Specificity'].append(specificity)\n",
    "performance_dict['Precision'].append(precision)\n",
    "performance_dict['F1 score'].append(f1_score)\n",
    "performance_dict['Run time'].append(total_time)\n",
    "performance_dict['TPS'].append(tps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "808f58e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random search started\n"
     ]
    }
   ],
   "source": [
    "# KNN-CV\n",
    "params = {\n",
    "    \"n_neighbors\": list(range(1, 16))\n",
    "}\n",
    "\n",
    "print(\"random search started\")\n",
    "randomsearch = RandomizedSearchCV(neighbors.KNeighborsClassifier(), param_distributions=params, cv=splits)\n",
    "randomsearch.fit(X, y)\n",
    "print(\"random search completed\")\n",
    "\n",
    "acc_score = []\n",
    "Truth = []\n",
    "Output = []\n",
    "\n",
    "total_time = 0\n",
    "total_tests = 0\n",
    "\n",
    "for train, test in tqdm(splits):\n",
    "    X_test = X.iloc[test]\n",
    "    y_test = y.iloc[test]\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "    pred_values = randomsearch.best_estimator_.predict(X_test)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "    acc = accuracy_score(y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(y_test.values.reshape(y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "    total_time+=elapsed\n",
    "    total_tests+=len(X_test)\n",
    "\n",
    "tps = total_time/total_tests\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % total_time)\n",
    "print(\"---Run time per subset is %s seconds ---\" % tps)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print(\"confusion matrix: \")\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "print(cm)\n",
    "print(\"classification report: \")\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(randomsearch.best_params_)\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "performance_dict['Model name'].append(\"KNN-CV\")\n",
    "performance_dict['Avg Accuracy'].append(np.mean(acc_score))\n",
    "performance_dict['Std Accuracy'].append(np.std(acc_score))\n",
    "performance_dict['Sensitivity'].append(sensitivity)\n",
    "performance_dict['Specificity'].append(specificity)\n",
    "performance_dict['Precision'].append(precision)\n",
    "performance_dict['F1 score'].append(f1_score)\n",
    "performance_dict['Run time'].append(total_time)\n",
    "performance_dict['TPS'].append(tps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2021f307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random search started\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'RandomizedSearchCV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/arisingh/Documents/GitHub/2022-svsm-afib-group1/models/subsets/full_subset.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/Users/arisingh/Documents/GitHub/2022-svsm-afib-group1/models/subsets/full_subset.ipynb#ch0000010?line=1'>2</a>\u001b[0m params \u001b[39m=\u001b[39m {\n\u001b[0;32m      <a href='vscode-notebook-cell:/Users/arisingh/Documents/GitHub/2022-svsm-afib-group1/models/subsets/full_subset.ipynb#ch0000010?line=2'>3</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmax_depth\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m, \u001b[39m16\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/Users/arisingh/Documents/GitHub/2022-svsm-afib-group1/models/subsets/full_subset.ipynb#ch0000010?line=3'>4</a>\u001b[0m }\n\u001b[0;32m      <a href='vscode-notebook-cell:/Users/arisingh/Documents/GitHub/2022-svsm-afib-group1/models/subsets/full_subset.ipynb#ch0000010?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mrandom search started\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/Users/arisingh/Documents/GitHub/2022-svsm-afib-group1/models/subsets/full_subset.ipynb#ch0000010?line=6'>7</a>\u001b[0m randomsearch \u001b[39m=\u001b[39m RandomizedSearchCV(DecisionTreeClassifier(), param_distributions\u001b[39m=\u001b[39mparams, cv\u001b[39m=\u001b[39msplits)\n\u001b[0;32m      <a href='vscode-notebook-cell:/Users/arisingh/Documents/GitHub/2022-svsm-afib-group1/models/subsets/full_subset.ipynb#ch0000010?line=7'>8</a>\u001b[0m randomsearch\u001b[39m.\u001b[39mfit(X, y)\n\u001b[0;32m      <a href='vscode-notebook-cell:/Users/arisingh/Documents/GitHub/2022-svsm-afib-group1/models/subsets/full_subset.ipynb#ch0000010?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mrandom search completed\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RandomizedSearchCV' is not defined"
     ]
    }
   ],
   "source": [
    "# Decision tree\n",
    "params = {\n",
    "    \"max_depth\": list(range(10, 16))\n",
    "}\n",
    "\n",
    "print(\"random search started\")\n",
    "randomsearch = RandomizedSearchCV(DecisionTreeClassifier(), param_distributions=params, cv=splits)\n",
    "randomsearch.fit(X, y)\n",
    "print(\"random search completed\")\n",
    "\n",
    "acc_score = []\n",
    "Truth = []\n",
    "Output = []\n",
    "\n",
    "total_time = 0\n",
    "total_tests = 0\n",
    "\n",
    "for train, test in tqdm(splits):\n",
    "    X_test = X.iloc[test]\n",
    "    y_test = y.iloc[test]\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "    pred_values = randomsearch.best_estimator_.predict(X_test)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "    acc = accuracy_score(y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(y_test.values.reshape(y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "    total_time+=elapsed\n",
    "    total_tests+=len(X_test)\n",
    "\n",
    "tps = total_time/total_tests\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % total_time)\n",
    "print(\"---Run time per subset is %s seconds ---\" % tps)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print(\"confusion matrix: \")\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "print(cm)\n",
    "print(\"classification report: \")\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(randomsearch.best_params_)\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "performance_dict['Model name'].append(\"Decision tree\")\n",
    "performance_dict['Avg Accuracy'].append(np.mean(acc_score))\n",
    "performance_dict['Std Accuracy'].append(np.std(acc_score))\n",
    "performance_dict['Sensitivity'].append(sensitivity)\n",
    "performance_dict['Specificity'].append(specificity)\n",
    "performance_dict['Precision'].append(precision)\n",
    "performance_dict['F1 score'].append(f1_score)\n",
    "performance_dict['Run time'].append(total_time)\n",
    "performance_dict['TPS'].append(tps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1b2d6a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# random forest\n",
    "params = {\n",
    "    \"max_depth\": list(range(1, 16)),\n",
    "    \"n_estimators\": np.linspace(100, 500, 9).astype(int)\n",
    "}\n",
    "\n",
    "print(\"random search started\")\n",
    "randomsearch = RandomizedSearchCV(RandomForestClassifier(max_features='sqrt', random_state=2), param_distributions=params, cv=splits)\n",
    "randomsearch.fit(X, y)\n",
    "print(\"random search completed\")\n",
    "\n",
    "acc_score = []\n",
    "Truth = []\n",
    "Output = []\n",
    "\n",
    "total_time = 0\n",
    "total_tests = 0\n",
    "\n",
    "for train, test in tqdm(splits):\n",
    "    X_test = X.iloc[test]\n",
    "    y_test = y.iloc[test]\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "    pred_values = randomsearch.best_estimator_.predict(X_test)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "    acc = accuracy_score(y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(y_test.values.reshape(y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "    total_time+=elapsed\n",
    "    total_tests+=len(X_test)\n",
    "\n",
    "tps = total_time/total_tests\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % total_time)\n",
    "print(\"---Run time per subset is %s seconds ---\" % tps)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print(\"confusion matrix: \")\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "print(cm)\n",
    "print(\"classification report: \")\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(randomsearch.best_params_)\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "performance_dict['Model name'].append(\"Random forest\")\n",
    "performance_dict['Avg Accuracy'].append(np.mean(acc_score))\n",
    "performance_dict['Std Accuracy'].append(np.std(acc_score))\n",
    "performance_dict['Sensitivity'].append(sensitivity)\n",
    "performance_dict['Specificity'].append(specificity)\n",
    "performance_dict['Precision'].append(precision)\n",
    "performance_dict['F1 score'].append(f1_score)\n",
    "performance_dict['Run time'].append(total_time)\n",
    "performance_dict['TPS'].append(tps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042bf7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bagging\n",
    "params = {\n",
    "    \"max_depth\": list(range(1, 16)),\n",
    "    \"n_estimators\": np.linspace(100, 500, 9).astype(int)\n",
    "}\n",
    "\n",
    "print(\"random search started\")\n",
    "randomsearch = RandomizedSearchCV(RandomForestClassifier(max_features='sqrt', random_state=2), param_distributions=params, cv=splits)\n",
    "randomsearch.fit(X, y)\n",
    "print(\"random search completed\")\n",
    "\n",
    "acc_score = []\n",
    "Truth = []\n",
    "Output = []\n",
    "\n",
    "total_time = 0\n",
    "total_tests = 0\n",
    "\n",
    "for train, test in tqdm(splits):\n",
    "    X_test = X.iloc[test]\n",
    "    y_test = y.iloc[test]\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "    pred_values = randomsearch.best_estimator_.predict(X_test)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "    acc = accuracy_score(y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(y_test.values.reshape(y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "    total_time+=elapsed\n",
    "    total_tests+=len(X_test)\n",
    "\n",
    "tps = total_time/total_tests\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % total_time)\n",
    "print(\"---Run time per subset is %s seconds ---\" % tps)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print(\"confusion matrix: \")\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "print(cm)\n",
    "print(\"classification report: \")\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(randomsearch.best_params_)\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "performance_dict['Model name'].append(\"Bagging\")\n",
    "performance_dict['Avg Accuracy'].append(np.mean(acc_score))\n",
    "performance_dict['Std Accuracy'].append(np.std(acc_score))\n",
    "performance_dict['Sensitivity'].append(sensitivity)\n",
    "performance_dict['Specificity'].append(specificity)\n",
    "performance_dict['Precision'].append(precision)\n",
    "performance_dict['F1 score'].append(f1_score)\n",
    "performance_dict['Run time'].append(total_time)\n",
    "performance_dict['TPS'].append(tps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ce7ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost\n",
    "params = {\n",
    "    \"max_depth\": list(range(1, 16)),\n",
    "    \"n_estimators\": np.linspace(100, 500, 9).astype(int)\n",
    "}\n",
    "\n",
    "print(\"random search started\")\n",
    "randomsearch = RandomizedSearchCV(AdaBoostClassifier(learning_rate = 0.1, algorithm=\"SAMME.R\", random_state=2), param_distributions=params, cv=splits)\n",
    "randomsearch.fit(X, y)\n",
    "print(\"random search completed\")\n",
    "\n",
    "acc_score = []\n",
    "Truth = []\n",
    "Output = []\n",
    "\n",
    "total_time = 0\n",
    "total_tests = 0\n",
    "\n",
    "for train, test in tqdm(splits):\n",
    "    X_test = X.iloc[test]\n",
    "    y_test = y.iloc[test]\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "    pred_values = randomsearch.best_estimator_.predict(X_test)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "    acc = accuracy_score(y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(y_test.values.reshape(y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "    total_time+=elapsed\n",
    "    total_tests+=len(X_test)\n",
    "\n",
    "tps = total_time/total_tests\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % total_time)\n",
    "print(\"---Run time per subset is %s seconds ---\" % tps)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print(\"confusion matrix: \")\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "print(cm)\n",
    "print(\"classification report: \")\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(randomsearch.best_params_)\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "performance_dict['Model name'].append(\"AdaBoost\")\n",
    "performance_dict['Avg Accuracy'].append(np.mean(acc_score))\n",
    "performance_dict['Std Accuracy'].append(np.std(acc_score))\n",
    "performance_dict['Sensitivity'].append(sensitivity)\n",
    "performance_dict['Specificity'].append(specificity)\n",
    "performance_dict['Precision'].append(precision)\n",
    "performance_dict['F1 score'].append(f1_score)\n",
    "performance_dict['Run time'].append(total_time)\n",
    "performance_dict['TPS'].append(tps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1047bee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient boost (sklearn)\n",
    "params = {\n",
    "    \"max_depth\": list(range(1, 16)),\n",
    "    \"n_estimators\": np.linspace(100, 500, 9).astype(int)\n",
    "}\n",
    "\n",
    "print(\"random search started\")\n",
    "randomsearch = RandomizedSearchCV(GradientBoostingClassifier(learning_rate = 0.1, random_state = 2), param_distributions=params, cv=splits)\n",
    "randomsearch.fit(X, y)\n",
    "print(\"random search completed\")\n",
    "\n",
    "acc_score = []\n",
    "Truth = []\n",
    "Output = []\n",
    "\n",
    "total_time = 0\n",
    "total_tests = 0\n",
    "\n",
    "for train, test in tqdm(splits):\n",
    "    X_test = X.iloc[test]\n",
    "    y_test = y.iloc[test]\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "    pred_values = randomsearch.best_estimator_.predict(X_test)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "    acc = accuracy_score(y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(y_test.values.reshape(y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "    total_time+=elapsed\n",
    "    total_tests+=len(X_test)\n",
    "\n",
    "tps = total_time/total_tests\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % total_time)\n",
    "print(\"---Run time per subset is %s seconds ---\" % tps)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print(\"confusion matrix: \")\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "print(cm)\n",
    "print(\"classification report: \")\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(randomsearch.best_params_)\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "performance_dict['Model name'].append(\"Gradient Boost\")\n",
    "performance_dict['Avg Accuracy'].append(np.mean(acc_score))\n",
    "performance_dict['Std Accuracy'].append(np.std(acc_score))\n",
    "performance_dict['Sensitivity'].append(sensitivity)\n",
    "performance_dict['Specificity'].append(specificity)\n",
    "performance_dict['Precision'].append(precision)\n",
    "performance_dict['F1 score'].append(f1_score)\n",
    "performance_dict['Run time'].append(total_time)\n",
    "performance_dict['TPS'].append(tps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0318c718",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "params = {\n",
    "    \"max_depth\": list(range(1, 16)),\n",
    "    \"n_estimators\": np.linspace(100, 500, 9).astype(int)\n",
    "}\n",
    "\n",
    "print(\"random search started\")\n",
    "randomsearch = RandomizedSearchCV(XGBClassifier(learning_rate = 0.1, use_label_encoder=False, verbose=None, eval_metric='logloss'), param_distributions=params, cv=splits)\n",
    "randomsearch.fit(X, y)\n",
    "print(\"random search completed\")\n",
    "\n",
    "acc_score = []\n",
    "Truth = []\n",
    "Output = []\n",
    "\n",
    "total_time = 0\n",
    "total_tests = 0\n",
    "\n",
    "for train, test in tqdm(splits):\n",
    "    X_test = X.iloc[test]\n",
    "    y_test = y.iloc[test]\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "    pred_values = randomsearch.best_estimator_.predict(X_test)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "    acc = accuracy_score(y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(y_test.values.reshape(y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "    total_time+=elapsed\n",
    "    total_tests+=len(X_test)\n",
    "\n",
    "tps = total_time/total_tests\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % total_time)\n",
    "print(\"---Run time per subset is %s seconds ---\" % tps)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print(\"confusion matrix: \")\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "print(cm)\n",
    "print(\"classification report: \")\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(randomsearch.best_params_)\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "performance_dict['Model name'].append(\"XGBoost\")\n",
    "performance_dict['Avg Accuracy'].append(np.mean(acc_score))\n",
    "performance_dict['Std Accuracy'].append(np.std(acc_score))\n",
    "performance_dict['Sensitivity'].append(sensitivity)\n",
    "performance_dict['Specificity'].append(specificity)\n",
    "performance_dict['Precision'].append(precision)\n",
    "performance_dict['F1 score'].append(f1_score)\n",
    "performance_dict['Run time'].append(total_time)\n",
    "performance_dict['TPS'].append(tps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0c13c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "params = {\n",
    "    \"max_depth\": list(range(1, 16)),\n",
    "    \"n_estimators\": np.linspace(100, 500, 9).astype(int)\n",
    "}\n",
    "\n",
    "print(\"random search started\")\n",
    "randomsearch = RandomizedSearchCV(lightgbm.LGBMClassifier(learning_rate=0.09,verbose=0,random_state=2), param_distributions=params, cv=splits)\n",
    "randomsearch.fit(X, y)\n",
    "print(\"random search completed\")\n",
    "\n",
    "acc_score = []\n",
    "Truth = []\n",
    "Output = []\n",
    "\n",
    "total_time = 0\n",
    "total_tests = 0\n",
    "\n",
    "for train, test in tqdm(splits):\n",
    "    X_test = X.iloc[test]\n",
    "    y_test = y.iloc[test]\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "    pred_values = randomsearch.best_estimator_.predict(X_test)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "    acc = accuracy_score(y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(y_test.values.reshape(y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "    \n",
    "    total_time+=elapsed\n",
    "    total_tests+=len(X_test)\n",
    "\n",
    "tps = total_time/total_tests\n",
    "\n",
    "#print(model.feature_importances_)\n",
    "Importance = pd.DataFrame({'Importance':(randomsearch.best_estimator_.feature_importances_*100)[0:10]}, \n",
    "                      index = (X_train.columns)[0:10])\n",
    "Importance.sort_values(by = 'Importance', \n",
    "                   axis = 0, \n",
    "                   ascending = True).plot(kind = 'barh', \n",
    "                                          color = 'r')\n",
    "plt.xlabel('Variable Importance')\n",
    "plt.gca().legend_ = None\n",
    "#plt.savefig('plot1.png')\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % total_time)\n",
    "print(\"---Run time per subset is %s seconds ---\" % tps)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print(\"confusion matrix: \")\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "print(cm)\n",
    "print(\"classification report: \")\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(randomsearch.best_params_)\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "performance_dict['Model name'].append(\"LightGBM\")\n",
    "performance_dict['Avg Accuracy'].append(np.mean(acc_score))\n",
    "performance_dict['Std Accuracy'].append(np.std(acc_score))\n",
    "performance_dict['Sensitivity'].append(sensitivity)\n",
    "performance_dict['Specificity'].append(specificity)\n",
    "performance_dict['Precision'].append(precision)\n",
    "performance_dict['F1 score'].append(f1_score)\n",
    "performance_dict['Run time'].append(total_time)\n",
    "performance_dict['TPS'].append(tps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0347f27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost\n",
    "params = {\n",
    "    \"max_depth\": list(range(1, 16)),\n",
    "    \"iterations\": np.linspace(100, 500, 9).astype(int)\n",
    "}\n",
    "\n",
    "print(\"random search started\")\n",
    "randomsearch = RandomizedSearchCV(CatBoostClassifier(learning_rate=0.1, loss_function='Logloss', verbose=None), param_distributions=params, cv=splits)\n",
    "randomsearch.fit(X, y)\n",
    "print(\"random search completed\")\n",
    "\n",
    "acc_score = []\n",
    "Truth = []\n",
    "Output = []\n",
    "\n",
    "total_time = 0\n",
    "total_tests = 0\n",
    "\n",
    "for train, test in tqdm(splits):\n",
    "    X_test = X.iloc[test]\n",
    "    y_test = y.iloc[test]\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "    pred_values = randomsearch.best_estimator_.predict(X_test)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "    acc = accuracy_score(y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(y_test.values.reshape(y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "    \n",
    "    total_time+=elapsed\n",
    "    total_tests+=len(X_test)\n",
    "\n",
    "tps = total_time/total_tests\n",
    "\n",
    "#print(model.feature_importances_)\n",
    "Importance = pd.DataFrame({'Importance':(randomsearch.best_estimator_.feature_importances_*100)[0:10]}, \n",
    "                      index = (X_train.columns)[0:10])\n",
    "Importance.sort_values(by = 'Importance', \n",
    "                   axis = 0, \n",
    "                   ascending = True).plot(kind = 'barh', \n",
    "                                          color = 'r')\n",
    "plt.xlabel('Variable Importance')\n",
    "plt.gca().legend_ = None\n",
    "#plt.savefig('plot1.png')\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % total_time)\n",
    "print(\"---Run time per subset is %s seconds ---\" % tps)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print(\"confusion matrix: \")\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "print(cm)\n",
    "print(\"classification report: \")\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(randomsearch.best_params_)\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "performance_dict['Model name'].append(\"Catboost\")\n",
    "performance_dict['Avg Accuracy'].append(np.mean(acc_score))\n",
    "performance_dict['Std Accuracy'].append(np.std(acc_score))\n",
    "performance_dict['Sensitivity'].append(sensitivity)\n",
    "performance_dict['Specificity'].append(specificity)\n",
    "performance_dict['Precision'].append(precision)\n",
    "performance_dict['F1 score'].append(f1_score)\n",
    "performance_dict['Run time'].append(total_time)\n",
    "performance_dict['TPS'].append(tps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3ed2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df = pd.DataFrame(performance_dict).set_index(\"Model name\")\n",
    "performance_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
