{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import wfdb\n",
    "import copy as cp\n",
    "import scipy.signal as signal\n",
    "from sklearn import preprocessing\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import statistics as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlist = []\n",
    "records = os.path.normpath('mit-bih-raw/RECORDS')\n",
    "with open(records) as rfile: #Then we open the file \n",
    "                             #The 'with' command only opens the file while we are in it. Automatically closes the file when we're not\n",
    "    for record in rfile:  # Then we iterate through the lines in the file\n",
    "        record = record[0:len(record)-1] # Remove any erronious new line characters at the end ('\\n')\n",
    "        rlist.append(record) # Then build an array with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###### Step 1: Initialize all Arrays\n",
    "             # Below, replace all of the ___ with the command that declares an array/list\n",
    "             # hint: https://stackoverflow.com/questions/1514553/how-to-declare-an-array-in-python\n",
    "samples = [] # will house the samples of all subjects\n",
    "good_list = [] # will list the names of the subjects we successfully extracted\n",
    "bad_list = [] # will house the names of the subjects we failed to extract\n",
    "qrs = [] # will house the indices of R-Peaks for all subjects\n",
    "atr_label = [] # will house the labels for each rhythm annotation for all subjects\n",
    "atr_locs = [] # will house the locations corresponding to the rhythm annotation labels\n",
    "\n",
    "\n",
    "###### Step 2: Extract Information\n",
    "for x in tqdm(rlist): #this will iterate through te records that we found above\n",
    "    try: # A try statement will run the except statement if for some reason the try commands fail\n",
    "         # In this case I use the try statement because one of the subjects has no signal data causing failure\n",
    "         # I then use bad_list and good_list so that all of the indices in rlist match with the arrays we initialized in Step 1, above\n",
    "        ######################################################\n",
    "            # Below find the wfdb function that will return the information that is described below \n",
    "            # Then replace _____ with the correct function call\n",
    "        samp = wfdb.rdsamp(os.path.normpath('mit-bih-raw/'+x)) # wfdb._____(file_location) will read the signal & header data and return a 2 value array\n",
    "            # samp[0] - the signal data is the raw reading from the ecg. Each value is a sample taken.\n",
    "            # samp[1] - the header data includes things about the signal data such as:\n",
    "              # samples per section, denoted 'fs'\n",
    "              # number of signals, denoted 'n_sig'\n",
    "        ######################################################\n",
    "        samples.append(samp) #add it to our array for all subject\n",
    "        \n",
    "            #What is our file extension that has the annotation we want? Find it here and replace _____ with it \n",
    "            #hint: READ THE VARIABLE NAMES!!!!\n",
    "        qrs_tmp = wfdb.rdann(os.path.normpath('mit-bih-raw/'+x), extension=\"qrs\") #extract the QRS Info\n",
    "        qrs_locs = np.array(qrs_tmp.sample, dtype='int') #Get just the loccation of R-Peaks from the QRS Info\n",
    "        qrs.append(qrs_locs) # Add to our array for all subjects\n",
    "        \n",
    "            #Do the same thing here\n",
    "        atr = wfdb.rdann(os.path.normpath('mit-bih-raw/'+x),extension=\"atr\") #extract the atr info which stores the rhythm type(s) over the whole signal\n",
    "        atr_label.append(atr.aux_note) # aux_note stores the type of rhythm - main two are '(N' for normal and '(AFIB' for AFIB\n",
    "        atr_locs.append(np.append(atr.sample, len(samp[0]))) #I add the length of the whole sample to the end for better visualization later\n",
    "        \n",
    "        good_list.append(x) # when all extraction is successful append the record name to good_list\n",
    "    except Exception as exep:\n",
    "        print(exep) # Alert the user of an exception\n",
    "        bad_list.append(x) # add to the bad list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atr_dics = [] #Initialize the array that will hold the dictionary for each subject\n",
    "\n",
    "for idxs,lab in enumerate(atr_label):\n",
    "    atr_dic = {} #Initialize dictionary for each subject\n",
    "    for idx,x in enumerate(lab):\n",
    "        if x not in atr_dic.keys():\n",
    "            atr_dic[x] = [] #Add dictionary key if does not exist\n",
    "        atr_dic[x].append([atr_locs[idxs][idx], atr_locs[idxs][idx+1]]) #Insert range for each rhythm\n",
    "    atr_dics.append(atr_dic) #Add to dictionary array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dfs = {} # Initialize the subject_dataframes - will hold all of our subject dataframes\n",
    "\n",
    "for s, _ in enumerate(tqdm(good_list)): # Iterate through all of the subjects that we have complete data of \n",
    "    subj = pd.DataFrame( # The below statements initialize our datafram. The first to columns will be our given signals, and the rest we initialize to 0\n",
    "        data = np.transpose(np.array([ # First we give our data, for pandas they want the data by row instead of by column, so we use transpose to get the proper format\n",
    "                                               [x[0] for x in samples[s][0]],\n",
    "                                               [x[1] for x in samples[s][0]],\n",
    "                                               np.zeros(len(samples[s][0])), # np.zeros makes an array of zeros with the given lenth\n",
    "                                               np.zeros(len(samples[s][0])), \n",
    "                                               np.zeros(len(samples[s][0])), \n",
    "                                               np.zeros(len(samples[s][0])), \n",
    "                                        ])\n",
    "                           ),\n",
    "        columns = ['Signal 1', 'Signal 2', 'R-Peak', 'Normal', 'AFIB', 'Other'] # Here we name our columns to match the dataframe we outlined above\n",
    "    )\n",
    "    norm = [] # Initialize the norm array which will list every index the person is in a normal rhythm\n",
    "    if '(N' in atr_dics[s].keys():\n",
    "        for x in atr_dics[s]['(N']: # Then we iterate through our ranges we extracted above\n",
    "            norm = norm + list(range(x[0], x[1])) # And add all values in the range to our norm array\n",
    "    af = [] # Then we do the same steps above for AFIB rhythms\n",
    "    if '(AFIB' in atr_dics[s].keys():\n",
    "        for x in atr_dics[s]['(AFIB']:\n",
    "            af = af + list(range(x[0], x[1]))\n",
    "    subj['R-Peak']= subj.index.isin(qrs[s]) # the isin() function of a DataFram index will return true if the index is in that list and false if it is not\n",
    "                                            # then, we can initialize our dataFrame with correct values based on that\n",
    "    subj['Normal']= subj.index.isin(norm)\n",
    "    subj['AFIB'] = subj.index.isin(af)\n",
    "    subj['Other'] = ~subj.index.isin(np.append(norm, af)) # Because we are classifying AFIB specifically we define other as any rhythm not in the norm or AFIB list\n",
    "    \n",
    "    full_dfs[_] = subj # Add the dataframe we built to our to array that holds all of our subjects' dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, x in enumerate(tqdm(good_list)): \n",
    "    if not os.path.exists('mit-bih-dataframes/'+x+ '.csv') or reload_flag:\n",
    "        full_dfs[x].to_csv(os.path.normpath('mit-bih-dataframes/'+x+'.csv')) # Pandas DataFrames have a built in to_csv() function which whill save it at the passed path\n",
    "\n",
    "np.savetxt(os.path.normpath(\"mit-bih-dataframes/subject_list.csv\"), good_list, delimiter=\",\",  fmt='%s') \n",
    "   # We'll load the complete list of subjects as well so that we can easily recreate the file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"mit-bih-extracted/subject_list.csv\", good_list, delimiter=\",\",  fmt='%s') #Save the names in the folder \n",
    "for idx, x in enumerate(tqdm(good_list)): # Iterate through our subjects\n",
    "    if not os.path.exists(\"mit-bih-extracted/\"+x+\"_signals.csv\") or reload_flag:\n",
    "        np.savetxt(os.path.normpath(\"mit-bih-extracted/\"+x+\"_signals.csv\"), np.array(samples[idx][0]), delimiter=\",\") # numPy has a savetxt() function which by setting the delimiter as ',' we can \n",
    "                                                                                            # simulate a to_csv() function \n",
    "    if not os.path.exists(\"mit-bih-extracted/\"+x+\"_rpeaks.csv\") or reload_flag:\n",
    "            np.savetxt(os.path.normpath(\"mit-bih-extracted/\"+x+\"_rpeaks.csv\"), np.array(qrs[idx]), delimiter=\",\")      \n",
    "    if not os.path.exists(\"mit-bih-extracted/\"+x+\"_headers.pkl\") or reload_flag:\n",
    "        with open(os.path.normpath(\"mit-bih-extracted/\"+x+\"_headers.pkl\"), 'wb') as picklefile: # nomPy has no way to save a dictionary as a CSV so we use the pickle package\n",
    "                                    # First we open up the file we would like to write to\n",
    "            pickle.dump(samples[idx][1], picklefile)\n",
    "    if not os.path.exists(\"mit-bih-extracted/\"+x+\"_rhythms.pkl\") or reload_flag:\n",
    "        with open(os.path.normpath(\"mit-bih-extracted/\"+x+\"_rhythms.pkl\"), 'wb') as picklefile:\n",
    "            pickle.dump(atr_dics[idx], picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_rr_ints(df):\n",
    "    #list of types of rr_ints for each subject\n",
    "    subject_types = []\n",
    "    for row in df.itertuples():\n",
    "        if row.rrInt < 0.85*row.rmean: \n",
    "            #if rr_ints is less than 85% of runningmean\n",
    "            #label subject type as short\n",
    "            subject_types.append('short')\n",
    "        elif row.rrInt > 1.15*row.rmean: \n",
    "            #if rr_ints is greater than 115% of runningmean\n",
    "            #label subject type as long\n",
    "            subject_types.append('long')\n",
    "        else:\n",
    "            #label subject type as regular\n",
    "            subject_types.append('regular')\n",
    "    \n",
    "    return subject_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_proportions(int_types):\n",
    "    StoS = 0\n",
    "    StoR = 0\n",
    "    StoL = 0\n",
    "    RtoS = 0\n",
    "    RtoR = 0\n",
    "    RtoL = 0\n",
    "    LtoS = 0\n",
    "    LtoR = 0\n",
    "    LtoL = 0\n",
    "    for idx in range(len(int_types)):\n",
    "        if idx<len(int_types)-1:\n",
    "            if int_types[idx]=='short' and int_types[idx+1]=='short':\n",
    "                StoS+=1\n",
    "            elif int_types[idx]=='short' and int_types[idx+1]=='regular':\n",
    "                StoR+=1\n",
    "            elif int_types[idx]=='short' and int_types[idx+1]=='long':\n",
    "                StoL+=1\n",
    "            elif int_types[idx]=='regular' and int_types[idx+1]=='short':\n",
    "                RtoS+=1\n",
    "            elif int_types[idx]=='regular' and int_types[idx+1]=='regular':\n",
    "                RtoR+=1\n",
    "            elif int_types[idx]=='regular' and int_types[idx+1]=='long':\n",
    "                RtoL+=1\n",
    "            elif int_types[idx]=='long' and int_types[idx+1]=='short':\n",
    "                LtoS+=1\n",
    "            elif int_types[idx]=='long' and int_types[idx+1]=='regular':\n",
    "                LtoR+=1\n",
    "            elif int_types[idx]=='long' and int_types[idx+1]=='long':\n",
    "                LtoL+=1\n",
    "    \n",
    "    count = len(int_types)-1\n",
    "    subject_transitions = [StoS/count, StoR/count, StoL/count, RtoS/count, RtoR/count, RtoL/count, LtoS/count, LtoR/count, LtoL/count]\n",
    "    \n",
    "    return subject_transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpeak_dfs = {}\n",
    "for record in tqdm(rlist):\n",
    "    rpeak_dfs[record] = pd.read_csv(os.path.normpath('mit-bih-extracted/'+record+'_rpeaks.csv'), names=['rpeak'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rmean(rrInts):\n",
    "    rmeans = []\n",
    "    for index, value in enumerate(rrInts):\n",
    "        if index==0:\n",
    "            rmeans.append(value)\n",
    "        else:\n",
    "            rmeans.append(0.75*rmeans[index-1] + 0.25*value)\n",
    "    \n",
    "    return rmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rmssd(subset):\n",
    "    rrInts = subset['rrInt'].tolist()\n",
    "    sum_of_squares = 0\n",
    "    for idx, rrInt in enumerate(rrInts):\n",
    "        if idx<len(rrInts)-1:\n",
    "            square_difference = (rrInt-rrInts[idx-1])**2\n",
    "            sum_of_squares+=square_difference\n",
    "    mean_sum = sum_of_squares/(len(rrInts)-1)\n",
    "    return np.sqrt(mean_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_subject(full_df, rpeak_df, interval_length = 4, calib_length = 100):\n",
    "    rpeaks = rpeak_df['rpeak'].tolist()\n",
    "\n",
    "    current_subset = []\n",
    "    subsets = [] \n",
    "\n",
    "    beginning_idx = 0\n",
    "    outlier_comp = 0\n",
    "    prev_peak = rpeaks[beginning_idx]\n",
    "    for idx, peak in enumerate(rpeaks[:calib_length]):\n",
    "        if idx<len(rpeaks)-1:\n",
    "            if peak-prev_peak<500:\n",
    "                current_subset.append(peak)\n",
    "            else:\n",
    "                outlier_comp+=1\n",
    "            prev_peak = peak\n",
    "        else:\n",
    "            rr_int_column = [current_subset[x]-current_subset[x-1] for x in range(1, len(current_subset))]\n",
    "            rhythm_column = []\n",
    "            for x in range(1, len(current_subset)):\n",
    "                if full_df['Normal'][current_subset[x]]:\n",
    "                    rhythm_column.append('N')\n",
    "                elif full_df['AFIB'][current_subset[x]]:\n",
    "                    rhythm_column.append('A')\n",
    "                elif full_df['Other'][current_subset[x]]:\n",
    "                    rhythm_column.append('O')\n",
    "            \n",
    "            rmean_column = extract_rmean(rr_int_column)\n",
    "\n",
    "            subsets.append(pd.DataFrame({'rhythmLabel': rhythm_column, 'rrInt': rr_int_column, 'rmean': rmean_column}, columns=['rhythmLabel', 'rrInt', 'rmean']))\n",
    "    \n",
    "    current_subset = []\n",
    "    outlier_comp = 0\n",
    "    prev_peak = rpeaks[calib_length-1]\n",
    "\n",
    "    counter = 0\n",
    "    for idx, peak in enumerate(rpeaks[calib_length:], calib_length):\n",
    "        if idx<len(rpeaks)-1:\n",
    "            if counter-outlier_comp<=interval_length:\n",
    "                if peak-prev_peak<500:\n",
    "                    current_subset.append(peak)\n",
    "                    counter+=1\n",
    "                else:\n",
    "                    outlier_comp+=1\n",
    "                prev_peak = peak\n",
    "            elif peak-prev_peak>500:\n",
    "                outlier_comp+=1\n",
    "            else:\n",
    "                rr_int_column = [current_subset[x]-current_subset[x-1] for x in range(1, len(current_subset))]\n",
    "                rhythm_column = []\n",
    "                for x in range(1, len(current_subset)):\n",
    "                    if full_df['Normal'][current_subset[x]]:\n",
    "                        rhythm_column.append('N')\n",
    "                    elif full_df['AFIB'][current_subset[x]]:\n",
    "                        rhythm_column.append('A')\n",
    "                    elif full_df['Other'][current_subset[x]]:\n",
    "                        rhythm_column.append('O')\n",
    "                \n",
    "                rmean_column = extract_rmean(rr_int_column)\n",
    "\n",
    "                subsets.append(pd.DataFrame({'rhythmLabel': rhythm_column, 'rrInt': rr_int_column, 'rmean': rmean_column}, columns=['rhythmLabel', 'rrInt', 'rmean']))\n",
    "                current_subset = []\n",
    "                outlier_comp = 0\n",
    "                counter = 0\n",
    "                prev_peak = rpeaks[idx]\n",
    "\n",
    "    return subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_dfs = {}\n",
    "for record in tqdm(rlist):\n",
    "    subset_dfs[record] = subset_subject(full_dfs[record], rpeak_dfs[record])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in tqdm(rlist):\n",
    "    subsets = subset_dfs[record]\n",
    "    idx_list = list(range(len(subsets)))\n",
    "    \n",
    "    data = {\n",
    "        \"subjectID\": [record]*len(subsets),\n",
    "        \"subsetID\": idx_list,\n",
    "        \"rhythmLabel\": [subsets[x]['rhythmLabel'].mode()[0] for x in idx_list]\n",
    "    }\n",
    "    \n",
    "    subset_list = pd.DataFrame(data)\n",
    "    subset_list['mappedLabel'] = subset_list['rhythmLabel'].map({'N': 'Non-Afib', 'A': 'Afib', 'O': 'Non-Afib'})\n",
    "    subset_list.to_csv(os.path.normpath('mit-bih-time-subsets/'+record+\"_subset_list.csv\"))\n",
    "    \n",
    "    os.makedirs(os.path.normpath('mit-bih-time-subsets/'+str(record)), exist_ok=True)\n",
    "\n",
    "    for x, subset in enumerate(subsets):\n",
    "        subset.to_csv(os.path.normpath('mit-bih-time-subsets/'+str(record)+'/'+str(record)+\"-\"+str(idx_list[x])+\".csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_features(subset_list, current_weight = 0.25, prev_weight = 0.75):\n",
    "    subset_dfs = {}\n",
    "    for x, subset in enumerate(subset_list.itertuples()):\n",
    "        subset_dfs[x] = pd.read_csv(os.path.normpath('mit-bih-time-subsets/'+str(subset.subjectID)+'/'+str(subset.subjectID)+\"-\"+str(x)+\".csv\"), index_col=0)\n",
    "\n",
    "    calib_df = subset_dfs[0]\n",
    "\n",
    "    feature_dict = {}\n",
    "\n",
    "    props = find_proportions(classify_rr_ints(calib_df))\n",
    "    feature_dict['StoS'] = [props[0]]\n",
    "    feature_dict['StoR'] = [props[1]]\n",
    "    feature_dict['StoL'] = [props[2]]\n",
    "    feature_dict['RtoS'] = [props[3]]\n",
    "    feature_dict['RtoR'] = [props[4]]\n",
    "    feature_dict['RtoL'] = [props[5]]\n",
    "    feature_dict['LtoS'] = [props[6]]\n",
    "    feature_dict['LtoR'] = [props[7]]\n",
    "    feature_dict['LtoL'] = [props[8]]\n",
    "\n",
    "    feature_dict['std'] = [np.std(calib_df['rrInt'])]\n",
    "    feature_dict['cov'] = [feature_dict['std'][0]/np.mean(calib_df['rrInt'])]\n",
    "    feature_dict['range'] = [np.max(calib_df['rrInt'])-np.min(calib_df['rrInt'])]\n",
    "    #feature_dict['rmean'] = df['rmean'].tolist()\n",
    "    #feature_dict['rrv'] = df['rr_variance'].tolist()\n",
    "    feature_dict['rrInt_var'] = [calib_df['rrInt'].var()]\n",
    "    feature_dict['rmean_var'] = [calib_df['rmean'].var()]\n",
    "    feature_dict['rmssd'] = [extract_rmssd(calib_df)]\n",
    "    feature_dict['mad'] = [stats.median_abs_deviation(calib_df['rrInt'])]\n",
    "    feature_dict['iqr'] = [stats.iqr(calib_df['rrInt'])]\n",
    "\n",
    "    \n",
    "    for key in subset_dfs:\n",
    "        if key>0:\n",
    "            props = find_proportions(classify_rr_ints(subset_dfs[key]))\n",
    "            feature_dict['StoS'].append(props[0]*current_weight + feature_dict['StoS'][key-1]*prev_weight)\n",
    "            feature_dict['StoR'].append(props[1]*current_weight + feature_dict['StoR'][key-1]*prev_weight)\n",
    "            feature_dict['StoL'].append(props[2]*current_weight + feature_dict['StoL'][key-1]*prev_weight)\n",
    "            feature_dict['RtoS'].append(props[3]*current_weight + feature_dict['RtoS'][key-1]*prev_weight)\n",
    "            feature_dict['RtoR'].append(props[4]*current_weight + feature_dict['RtoR'][key-1]*prev_weight)\n",
    "            feature_dict['RtoL'].append(props[5]*current_weight + feature_dict['RtoL'][key-1]*prev_weight)\n",
    "            feature_dict['LtoS'].append(props[6]*current_weight + feature_dict['LtoS'][key-1]*prev_weight)\n",
    "            feature_dict['LtoR'].append(props[7]*current_weight + feature_dict['LtoR'][key-1]*prev_weight)\n",
    "            feature_dict['LtoL'].append(props[8]*current_weight + feature_dict['LtoL'][key-1]*prev_weight)\n",
    "\n",
    "            feature_dict['std'].append(np.std(subset_dfs[key]['rrInt'])*current_weight + feature_dict['std'][key-1]*prev_weight)\n",
    "            feature_dict['cov'].append((feature_dict['std'][key]/np.mean(subset_dfs[key]['rrInt']))*current_weight + feature_dict['cov'][key-1]*prev_weight)\n",
    "            feature_dict['range'].append(np.max(subset_dfs[key]['rrInt'])-np.min(subset_dfs[key]['rrInt'])*current_weight + feature_dict['range'][key-1]*prev_weight)\n",
    "            #feature_dict['rmean'] = df['rmean'].tolist()\n",
    "            #feature_dict['rrv'] = df['rr_variance'].tolist()\n",
    "            feature_dict['rrInt_var'].append(subset_dfs[key]['rrInt'].var()*current_weight + feature_dict['rrInt_var'][key-1]*prev_weight)\n",
    "            feature_dict['rmean_var'].append(subset_dfs[key]['rmean'].var()*current_weight + feature_dict['rmean_var'][key-1]*prev_weight)\n",
    "            feature_dict['rmssd'].append(extract_rmssd(subset_dfs[key])*current_weight + feature_dict['rmssd'][key-1]*prev_weight)\n",
    "            feature_dict['mad'].append(stats.median_abs_deviation(subset_dfs[key]['rrInt'])*current_weight + feature_dict['mad'][key-1]*prev_weight)\n",
    "            feature_dict['iqr'].append(stats.iqr(subset_dfs[key]['rrInt'])*current_weight + feature_dict['iqr'][key-1]*prev_weight)\n",
    "\n",
    "    feature_df = pd.DataFrame(data=feature_dict)\n",
    "    return pd.concat([subset_list, feature_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = {}\n",
    "for record in tqdm(rlist):\n",
    "    subset_list = pd.read_csv(os.path.normpath('mit-bih-time-subsets/'+record+'_subset_list.csv'), index_col=0, dtype={'subjectID': str})\n",
    "    features = subset_features(subset_list)\n",
    "\n",
    "    features.to_csv(os.path.normpath('mit-bih-time-features/'+record+\".csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
