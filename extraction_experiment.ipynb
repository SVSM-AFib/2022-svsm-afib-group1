{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wfdb\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import msgpack\n",
    "import scipy.stats as stats\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlist = []\n",
    "records = os.path.normpath('mit-bih-raw/RECORDS')\n",
    "with open(records) as rfile:\n",
    "    for record in rfile:\n",
    "        record = record[0:len(record)-1] # Remove any erronious new line characters at the end ('\\n')\n",
    "        rlist.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampto must be greater than sampfrom\n",
      "sampto must be greater than sampfrom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:17<00:00,  1.44it/s]\n"
     ]
    }
   ],
   "source": [
    "samples = [] # will house the samples of all subjects\n",
    "good_list = [] # will list the names of the subjects we successfully extracted\n",
    "bad_list = [] # will house the names of the subjects we failed to extract\n",
    "qrs = [] # will house the indices of R-Peaks for all subjects\n",
    "atr_label = [] # will house the labels for each rhythm annotation for all subjects\n",
    "atr_locs = [] # will house the locations corresponding to the rhythm annotation labels\n",
    "\n",
    "for x in tqdm(rlist): #this will iterate through te records that we found above\n",
    "    try:\n",
    "        samp = wfdb.rdsamp(os.path.normpath('mit-bih-raw/'+x)) # wfdb._____(file_location) will read the signal & header data and return a 2 value array\n",
    "            # samp[0] - the signal data is the raw reading from the ecg. Each value is a sample taken.\n",
    "            # samp[1] - the header data includes things about the signal data such as:\n",
    "              # samples per section, denoted 'fs'\n",
    "              # number of signals, denoted 'n_sig'\n",
    "        ######################################################\n",
    "        samples.append(samp) #add it to our array for all subject\n",
    "        \n",
    "            #What is our file extension that has the annotation we want? Find it here and replace _____ with it \n",
    "            #hint: READ THE VARIABLE NAMES!!!!\n",
    "        qrs_tmp = wfdb.rdann(os.path.normpath('mit-bih-raw/'+x), extension=\"qrs\") #extract the QRS Info\n",
    "        qrs_locs = np.array(qrs_tmp.sample, dtype='int') #Get just the loccation of R-Peaks from the QRS Info\n",
    "        qrs.append(qrs_locs) # Add to our array for all subjects\n",
    "        \n",
    "            #Do the same thing here\n",
    "        atr = wfdb.rdann(os.path.normpath('mit-bih-raw/'+x),extension=\"atr\") #extract the atr info which stores the rhythm type(s) over the whole signal\n",
    "        atr_label.append(atr.aux_note) # aux_note stores the type of rhythm - main two are '(N' for normal and '(AFIB' for AFIB\n",
    "        atr_locs.append(np.append(atr.sample, len(samp[0]))) #I add the length of the whole sample to the end for better visualization later\n",
    "        \n",
    "        good_list.append(x) # when all extraction is successful append the record name to good_list\n",
    "    except Exception as exep:\n",
    "        tqdm.write(str(exep)) # Alert the user of an exception\n",
    "        bad_list.append(x) # add to the bad list\n",
    "\n",
    "rlist = good_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "atr_dics = [] #Initialize the array that will hold the dictionary for each subject\n",
    "\n",
    "for idxs,lab in enumerate(atr_label):\n",
    "    atr_dic = {} #Initialize dictionary for each subject\n",
    "    for idx,x in enumerate(lab):\n",
    "        if x not in atr_dic.keys():\n",
    "            atr_dic[x] = [] #Add dictionary key if does not exist\n",
    "        atr_dic[x].append([atr_locs[idxs][idx], atr_locs[idxs][idx+1]]) #Insert range for each rhythm\n",
    "    atr_dics.append(atr_dic) #Add to dictionary array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [02:59<00:00,  7.79s/it]\n"
     ]
    }
   ],
   "source": [
    "full_dfs = {} # Initialize the subject_dataframes - will hold all of our subject dataframes\n",
    "\n",
    "for s, _ in enumerate(tqdm(good_list)): # Iterate through all of the subjects that we have complete data of \n",
    "    subj = pd.DataFrame( # The below statements initialize our datafram. The first to columns will be our given signals, and the rest we initialize to 0\n",
    "        data = np.transpose(np.array([ # First we give our data, for pandas they want the data by row instead of by column, so we use transpose to get the proper format\n",
    "                                               [x[0] for x in samples[s][0]],\n",
    "                                               [x[1] for x in samples[s][0]],\n",
    "                                               np.zeros(len(samples[s][0])), # np.zeros makes an array of zeros with the given lenth\n",
    "                                               np.zeros(len(samples[s][0])), \n",
    "                                               np.zeros(len(samples[s][0])), \n",
    "                                               np.zeros(len(samples[s][0])), \n",
    "                                        ])\n",
    "                           ),\n",
    "        columns = ['Signal 1', 'Signal 2', 'R-Peak', 'Normal', 'AFIB', 'Other'] # Here we name our columns to match the dataframe we outlined above\n",
    "    )\n",
    "    norm = [] # Initialize the norm array which will list every index the person is in a normal rhythm\n",
    "    if '(N' in atr_dics[s].keys():\n",
    "        for x in atr_dics[s]['(N']: # Then we iterate through our ranges we extracted above\n",
    "            norm = norm + list(range(x[0], x[1])) # And add all values in the range to our norm array\n",
    "    af = [] # Then we do the same steps above for AFIB rhythms\n",
    "    if '(AFIB' in atr_dics[s].keys():\n",
    "        for x in atr_dics[s]['(AFIB']:\n",
    "            af = af + list(range(x[0], x[1]))\n",
    "    subj['R-Peak']= subj.index.isin(qrs[s]) # the isin() function of a DataFram index will return true if the index is in that list and false if it is not\n",
    "                                            # then, we can initialize our dataFrame with correct values based on that\n",
    "    subj['Normal']= subj.index.isin(norm)\n",
    "    subj['AFIB'] = subj.index.isin(af)\n",
    "    subj['Other'] = ~subj.index.isin(np.append(norm, af)) # Because we are classifying AFIB specifically we define other as any rhythm not in the norm or AFIB list\n",
    "    \n",
    "    full_dfs[_] = subj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:15<00:00,  1.51it/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('mit-bih-dataframes/'):\n",
    "    os.mkdir('mit-bih-dataframes/')\n",
    "\n",
    "for idx, x in enumerate(tqdm(good_list)): \n",
    "    if not os.path.exists('mit-bih-dataframes/'+x+ '.parquet') or reload_flag:\n",
    "        full_dfs[x].to_parquet(os.path.normpath('mit-bih-dataframes/'+x+'.parquet'))\n",
    "\n",
    "np.savetxt(os.path.normpath(\"mit-bih-dataframes/subject_list.csv\"), good_list, delimiter=\",\",  fmt='%s') \n",
    "   # We'll load the complete list of subjects as well so that we can easily recreate the file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:11<00:00,  1.95it/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('mit-bih-extracted/'):\n",
    "    os.mkdir('mit-bih-extracted/')\n",
    "\n",
    "def encoder(obj):\n",
    "    if isinstance(obj, datetime.time):\n",
    "        return {'__datetime__': True, 'as_str': obj.strftime(\"%H:%M:%S.%f\")}\n",
    "    if isinstance(obj, np.int64):\n",
    "        return {'__npint64__': True, 'as_int': int(obj)}\n",
    "    return obj\n",
    "\n",
    "np.savetxt(\"mit-bih-extracted/subject_list.csv\", good_list, delimiter=\",\",  fmt='%s')\n",
    "for idx, x in enumerate(tqdm(good_list)):\n",
    "    if not os.path.exists(\"mit-bih-extracted/\"+x+\"_signals.parquet\") or reload_flag:\n",
    "        signaldf = pd.DataFrame(np.array(samples[idx][0]), columns=[\"signal1\", \"signal2\"])\n",
    "        signaldf.to_parquet(os.path.normpath(\"mit-bih-extracted/\"+x+\"_signals.parquet\"))\n",
    "    if not os.path.exists(\"mit-bih-extracted/\"+x+\"_rpeaks.parquet\") or reload_flag:\n",
    "        rpeaksdf = pd.DataFrame(np.array(qrs[idx]), columns=[\"rpeaks\"])\n",
    "        rpeaksdf.to_parquet(os.path.normpath(\"mit-bih-extracted/\"+x+\"_rpeaks.parquet\"))\n",
    "    if not os.path.exists(\"mit-bih-extracted/\"+x+\"_headers.msgpack\") or reload_flag:\n",
    "        with open(os.path.normpath(\"mit-bih-extracted/\"+x+\"_headers.msgpack\"), 'wb') as outfile:\n",
    "            outfile.write(msgpack.packb(samples[idx][1], default=encoder))\n",
    "    if not os.path.exists(\"mit-bih-extracted/\"+x+\"_rhythms.msgpack\") or reload_flag:\n",
    "        with open(os.path.normpath(\"mit-bih-extracted/\"+x+\"_rhythms.msgpack\"), 'wb') as outfile:\n",
    "            outfile.write(msgpack.packb(atr_dics[idx], default=encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_rr_ints(df):\n",
    "    #list of types of rr_ints for each subject\n",
    "    subject_types = []\n",
    "    for row in df.itertuples():\n",
    "        if row.rrInt < 0.85*row.rmean: \n",
    "            #if rr_ints is less than 85% of runningmean\n",
    "            #label subject type as short\n",
    "            subject_types.append('short')\n",
    "        elif row.rrInt > 1.15*row.rmean: \n",
    "            #if rr_ints is greater than 115% of runningmean\n",
    "            #label subject type as long\n",
    "            subject_types.append('long')\n",
    "        else:\n",
    "            #label subject type as regular\n",
    "            subject_types.append('regular')\n",
    "    \n",
    "    return subject_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_proportions(int_types):\n",
    "    StoS = 0\n",
    "    StoR = 0\n",
    "    StoL = 0\n",
    "    RtoS = 0\n",
    "    RtoR = 0\n",
    "    RtoL = 0\n",
    "    LtoS = 0\n",
    "    LtoR = 0\n",
    "    LtoL = 0\n",
    "    for idx in range(len(int_types)):\n",
    "        if idx<len(int_types)-1:\n",
    "            if int_types[idx]=='short' and int_types[idx+1]=='short':\n",
    "                StoS+=1\n",
    "            elif int_types[idx]=='short' and int_types[idx+1]=='regular':\n",
    "                StoR+=1\n",
    "            elif int_types[idx]=='short' and int_types[idx+1]=='long':\n",
    "                StoL+=1\n",
    "            elif int_types[idx]=='regular' and int_types[idx+1]=='short':\n",
    "                RtoS+=1\n",
    "            elif int_types[idx]=='regular' and int_types[idx+1]=='regular':\n",
    "                RtoR+=1\n",
    "            elif int_types[idx]=='regular' and int_types[idx+1]=='long':\n",
    "                RtoL+=1\n",
    "            elif int_types[idx]=='long' and int_types[idx+1]=='short':\n",
    "                LtoS+=1\n",
    "            elif int_types[idx]=='long' and int_types[idx+1]=='regular':\n",
    "                LtoR+=1\n",
    "            elif int_types[idx]=='long' and int_types[idx+1]=='long':\n",
    "                LtoL+=1\n",
    "    \n",
    "    count = len(int_types)-1\n",
    "    subject_transitions = [StoS/count, StoR/count, StoL/count, RtoS/count, RtoR/count, RtoL/count, LtoS/count, LtoR/count, LtoL/count]\n",
    "    \n",
    "    return subject_transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'good_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rlist \u001b[39m=\u001b[39m good_list\n\u001b[1;32m      3\u001b[0m rpeak_dfs \u001b[39m=\u001b[39m {}\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m record \u001b[39min\u001b[39;00m tqdm(rlist):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'good_list' is not defined"
     ]
    }
   ],
   "source": [
    "rlist = good_list\n",
    "\n",
    "rpeak_dfs = {}\n",
    "for record in tqdm(rlist):\n",
    "    rpeak_dfs[record] = pd.read_parquet(os.path.normpath('mit-bih-extracted/'+record+'_rpeaks.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rmean(rrInts):\n",
    "    rmeans = []\n",
    "    for index, value in enumerate(rrInts):\n",
    "        if index==0:\n",
    "            rmeans.append(value)\n",
    "        else:\n",
    "            rmeans.append(0.75*rmeans[index-1] + 0.25*value)\n",
    "    \n",
    "    return rmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rmssd(subset):\n",
    "    rrInts = subset['rrInt'].tolist()\n",
    "    sum_of_squares = 0\n",
    "    for idx, rrInt in enumerate(rrInts):\n",
    "        if idx<len(rrInts)-1:\n",
    "            square_difference = (rrInt-rrInts[idx-1])**2\n",
    "            sum_of_squares+=square_difference\n",
    "    mean_sum = sum_of_squares/(len(rrInts)-1)\n",
    "    return np.sqrt(mean_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_subject(full_df, rpeak_df, interval_length = 4, calib_length = 100):\n",
    "    rpeaks = rpeak_df['rpeaks'].tolist()\n",
    "\n",
    "    current_subset = []\n",
    "    subsets = [] \n",
    "\n",
    "    beginning_idx = 0\n",
    "    outlier_comp = 0\n",
    "    prev_peak = rpeaks[beginning_idx]\n",
    "    for idx, peak in enumerate(rpeaks[:calib_length]):\n",
    "        if idx<len(rpeaks)-1:\n",
    "            if peak-prev_peak<500:\n",
    "                current_subset.append(peak)\n",
    "            else:\n",
    "                outlier_comp+=1\n",
    "            prev_peak = peak\n",
    "        else:\n",
    "            rr_int_column = [current_subset[x]-current_subset[x-1] for x in range(1, len(current_subset))]\n",
    "            rhythm_column = []\n",
    "            for x in range(1, len(current_subset)):\n",
    "                if full_df['Normal'][current_subset[x]]:\n",
    "                    rhythm_column.append('N')\n",
    "                elif full_df['AFIB'][current_subset[x]]:\n",
    "                    rhythm_column.append('A')\n",
    "                elif full_df['Other'][current_subset[x]]:\n",
    "                    rhythm_column.append('O')\n",
    "            \n",
    "            rmean_column = extract_rmean(rr_int_column)\n",
    "\n",
    "            subsets.append(pd.DataFrame({'rhythmLabel': rhythm_column, 'rrInt': rr_int_column, 'rmean': rmean_column}, columns=['rhythmLabel', 'rrInt', 'rmean']))\n",
    "    \n",
    "    current_subset = []\n",
    "    outlier_comp = 0\n",
    "    prev_peak = rpeaks[calib_length-1]\n",
    "\n",
    "    counter = 0\n",
    "    for idx, peak in enumerate(rpeaks[calib_length:], calib_length):\n",
    "        if idx<len(rpeaks)-1:\n",
    "            if counter-outlier_comp<=interval_length:\n",
    "                if peak-prev_peak<500:\n",
    "                    current_subset.append(peak)\n",
    "                    counter+=1\n",
    "                else:\n",
    "                    outlier_comp+=1\n",
    "                prev_peak = peak\n",
    "            elif peak-prev_peak>500:\n",
    "                outlier_comp+=1\n",
    "            else:\n",
    "                rr_int_column = [current_subset[x]-current_subset[x-1] for x in range(1, len(current_subset))]\n",
    "                rhythm_column = []\n",
    "                for x in range(1, len(current_subset)):\n",
    "                    if full_df['Normal'][current_subset[x]]:\n",
    "                        rhythm_column.append('N')\n",
    "                    elif full_df['AFIB'][current_subset[x]]:\n",
    "                        rhythm_column.append('A')\n",
    "                    elif full_df['Other'][current_subset[x]]:\n",
    "                        rhythm_column.append('O')\n",
    "                \n",
    "                rmean_column = extract_rmean(rr_int_column)\n",
    "\n",
    "                subsets.append(pd.DataFrame({'rhythmLabel': rhythm_column, 'rrInt': rr_int_column, 'rmean': rmean_column}, columns=['rhythmLabel', 'rrInt', 'rmean']))\n",
    "                current_subset = []\n",
    "                outlier_comp = 0\n",
    "                counter = 0\n",
    "                prev_peak = rpeaks[idx]\n",
    "\n",
    "    return subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [02:14<00:00,  5.87s/it]\n"
     ]
    }
   ],
   "source": [
    "subset_dfs = {}\n",
    "for record in tqdm(rlist):\n",
    "    subset_dfs[record] = subset_subject(full_dfs[record], rpeak_dfs[record])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [03:16<00:00,  8.54s/it]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('mit-bih-time-subsets/'):\n",
    "    os.mkdir('mit-bih-time-subsets/')\n",
    "\n",
    "for record in tqdm(rlist):\n",
    "    subsets = subset_dfs[record]\n",
    "    idx_list = list(range(len(subsets)))\n",
    "    \n",
    "    data = {\n",
    "        \"subjectID\": [record]*len(subsets),\n",
    "        \"subsetID\": idx_list,\n",
    "        \"rhythmLabel\": [subsets[x]['rhythmLabel'].mode()[0] for x in idx_list]\n",
    "    }\n",
    "    \n",
    "    subset_list = pd.DataFrame(data)\n",
    "    subset_list['mappedLabel'] = subset_list['rhythmLabel'].map({'N': 'Non-Afib', 'A': 'Afib', 'O': 'Non-Afib'})\n",
    "    subset_list.to_parquet(os.path.normpath('mit-bih-time-subsets/'+record+\"_subset_list.parquet\"))\n",
    "    \n",
    "    os.makedirs(os.path.normpath('mit-bih-time-subsets/'+str(record)), exist_ok=True)\n",
    "\n",
    "    for x, subset in enumerate(subsets):\n",
    "        subset.to_parquet(os.path.normpath('mit-bih-time-subsets/'+str(record)+'/'+str(record)+\"-\"+str(idx_list[x])+\".parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_features(subset_list, current_weight = 0.25, prev_weight = 0.75):\n",
    "    subset_dfs = {}\n",
    "    for x, subset in enumerate(subset_list.itertuples()):\n",
    "        subset_dfs[x] = pd.read_parquet(os.path.normpath('mit-bih-time-subsets/'+str(subset.subjectID)+'/'+str(subset.subjectID)+\"-\"+str(x)+\".parquet\"))\n",
    "\n",
    "    calib_df = subset_dfs[0]\n",
    "\n",
    "    feature_dict = {}\n",
    "\n",
    "    props = find_proportions(classify_rr_ints(calib_df))\n",
    "    feature_dict['StoS'] = [props[0]]\n",
    "    feature_dict['StoR'] = [props[1]]\n",
    "    feature_dict['StoL'] = [props[2]]\n",
    "    feature_dict['RtoS'] = [props[3]]\n",
    "    feature_dict['RtoR'] = [props[4]]\n",
    "    feature_dict['RtoL'] = [props[5]]\n",
    "    feature_dict['LtoS'] = [props[6]]\n",
    "    feature_dict['LtoR'] = [props[7]]\n",
    "    feature_dict['LtoL'] = [props[8]]\n",
    "\n",
    "    feature_dict['std'] = [np.std(calib_df['rrInt'])]\n",
    "    feature_dict['cov'] = [feature_dict['std'][0]/np.mean(calib_df['rrInt'])]\n",
    "    feature_dict['range'] = [np.max(calib_df['rrInt'])-np.min(calib_df['rrInt'])]\n",
    "    #feature_dict['rmean'] = df['rmean'].tolist()\n",
    "    #feature_dict['rrv'] = df['rr_variance'].tolist()\n",
    "    feature_dict['rrInt_var'] = [calib_df['rrInt'].var()]\n",
    "    feature_dict['rmean_var'] = [calib_df['rmean'].var()]\n",
    "    feature_dict['rmssd'] = [extract_rmssd(calib_df)]\n",
    "    feature_dict['mad'] = [stats.median_abs_deviation(calib_df['rrInt'])]\n",
    "    feature_dict['iqr'] = [stats.iqr(calib_df['rrInt'])]\n",
    "\n",
    "    \n",
    "    for key in subset_dfs:\n",
    "        if key>0:\n",
    "            props = find_proportions(classify_rr_ints(subset_dfs[key]))\n",
    "            feature_dict['StoS'].append(props[0]*current_weight + feature_dict['StoS'][key-1]*prev_weight)\n",
    "            feature_dict['StoR'].append(props[1]*current_weight + feature_dict['StoR'][key-1]*prev_weight)\n",
    "            feature_dict['StoL'].append(props[2]*current_weight + feature_dict['StoL'][key-1]*prev_weight)\n",
    "            feature_dict['RtoS'].append(props[3]*current_weight + feature_dict['RtoS'][key-1]*prev_weight)\n",
    "            feature_dict['RtoR'].append(props[4]*current_weight + feature_dict['RtoR'][key-1]*prev_weight)\n",
    "            feature_dict['RtoL'].append(props[5]*current_weight + feature_dict['RtoL'][key-1]*prev_weight)\n",
    "            feature_dict['LtoS'].append(props[6]*current_weight + feature_dict['LtoS'][key-1]*prev_weight)\n",
    "            feature_dict['LtoR'].append(props[7]*current_weight + feature_dict['LtoR'][key-1]*prev_weight)\n",
    "            feature_dict['LtoL'].append(props[8]*current_weight + feature_dict['LtoL'][key-1]*prev_weight)\n",
    "\n",
    "            feature_dict['std'].append(np.std(subset_dfs[key]['rrInt'])*current_weight + feature_dict['std'][key-1]*prev_weight)\n",
    "            feature_dict['cov'].append((feature_dict['std'][key]/np.mean(subset_dfs[key]['rrInt']))*current_weight + feature_dict['cov'][key-1]*prev_weight)\n",
    "            feature_dict['range'].append(np.max(subset_dfs[key]['rrInt'])-np.min(subset_dfs[key]['rrInt'])*current_weight + feature_dict['range'][key-1]*prev_weight)\n",
    "            #feature_dict['rmean'] = df['rmean'].tolist()\n",
    "            #feature_dict['rrv'] = df['rr_variance'].tolist()\n",
    "            feature_dict['rrInt_var'].append(subset_dfs[key]['rrInt'].var()*current_weight + feature_dict['rrInt_var'][key-1]*prev_weight)\n",
    "            feature_dict['rmean_var'].append(subset_dfs[key]['rmean'].var()*current_weight + feature_dict['rmean_var'][key-1]*prev_weight)\n",
    "            feature_dict['rmssd'].append(extract_rmssd(subset_dfs[key])*current_weight + feature_dict['rmssd'][key-1]*prev_weight)\n",
    "            feature_dict['mad'].append(stats.median_abs_deviation(subset_dfs[key]['rrInt'])*current_weight + feature_dict['mad'][key-1]*prev_weight)\n",
    "            feature_dict['iqr'].append(stats.iqr(subset_dfs[key]['rrInt'])*current_weight + feature_dict['iqr'][key-1]*prev_weight)\n",
    "\n",
    "    feature_df = pd.DataFrame(data=feature_dict)\n",
    "    return pd.concat([subset_list, feature_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [08:47<00:00, 22.94s/it]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('mit-bih-time-features/'):\n",
    "    os.mkdir('mit-bih-time-features/')\n",
    "features_df = {}\n",
    "for record in tqdm(rlist):\n",
    "    subset_list = pd.read_parquet(os.path.normpath('mit-bih-time-subsets/'+record+'_subset_list.parquet'))\n",
    "    features = subset_features(subset_list)\n",
    "\n",
    "    features.to_parquet(os.path.normpath('mit-bih-time-features/'+record+\".parquet\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "afib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ada57d62bdba33f60f3d98aec0dc1db90359aaf5574901473d57c7ef0e255730"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
