{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import wfdb\n",
    "import copy as cp\n",
    "import scipy.signal as signal\n",
    "import scipy.stats as stats\n",
    "from sklearn import preprocessing\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlist = []\n",
    "records = os.path.normpath('mit-bih-dataframes/subject_list.csv')\n",
    "with open(records) as rfile:\n",
    "    recordreader = csv.reader(rfile, delimiter=' ', quotechar='|')\n",
    "    for row in recordreader:\n",
    "        rlist.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 23/23 [01:29<00:00,  3.89s/it]\n"
     ]
    }
   ],
   "source": [
    "full_dfs = {}\n",
    "for record in tqdm(rlist):\n",
    "    full_dfs[record] = pd.read_csv(os.path.normpath('mit-bih-dataframes/'+record+'.csv'), index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_rr_ints(df):\n",
    "    #list of types of rr_ints for each subject\n",
    "    subject_types = []\n",
    "    for row in df.itertuples():\n",
    "        if row.rrInt < 0.85*row.rmean: \n",
    "            #if rr_ints is less than 85% of runningmean\n",
    "            #label subject type as short\n",
    "            subject_types.append('short')\n",
    "        elif row.rrInt > 1.15*row.rmean: \n",
    "            #if rr_ints is greater than 115% of runningmean\n",
    "            #label subject type as long\n",
    "            subject_types.append('long')\n",
    "        else:\n",
    "            #label subject type as regular\n",
    "            subject_types.append('regular')\n",
    "    \n",
    "    return subject_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_proportions(int_types):\n",
    "    StoS = 0\n",
    "    StoR = 0\n",
    "    StoL = 0\n",
    "    RtoS = 0\n",
    "    RtoR = 0\n",
    "    RtoL = 0\n",
    "    LtoS = 0\n",
    "    LtoR = 0\n",
    "    LtoL = 0\n",
    "    for idx in range(len(int_types)):\n",
    "        if idx<len(int_types)-1:\n",
    "            if int_types[idx]=='short' and int_types[idx+1]=='short':\n",
    "                StoS+=1\n",
    "            elif int_types[idx]=='short' and int_types[idx+1]=='regular':\n",
    "                StoR+=1\n",
    "            elif int_types[idx]=='short' and int_types[idx+1]=='long':\n",
    "                StoL+=1\n",
    "            elif int_types[idx]=='regular' and int_types[idx+1]=='short':\n",
    "                RtoS+=1\n",
    "            elif int_types[idx]=='regular' and int_types[idx+1]=='regular':\n",
    "                RtoR+=1\n",
    "            elif int_types[idx]=='regular' and int_types[idx+1]=='long':\n",
    "                RtoL+=1\n",
    "            elif int_types[idx]=='long' and int_types[idx+1]=='short':\n",
    "                LtoS+=1\n",
    "            elif int_types[idx]=='long' and int_types[idx+1]=='regular':\n",
    "                LtoR+=1\n",
    "            elif int_types[idx]=='long' and int_types[idx+1]=='long':\n",
    "                LtoL+=1\n",
    "    \n",
    "    count = len(int_types)-1\n",
    "    subject_transitions = [StoS/count, StoR/count, StoL/count, RtoS/count, RtoR/count, RtoL/count, LtoS/count, LtoR/count, LtoL/count]\n",
    "    \n",
    "    return subject_transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 23/23 [00:00<00:00, 76.32it/s]\n"
     ]
    }
   ],
   "source": [
    "rpeak_dfs = {}\n",
    "for record in tqdm(rlist):\n",
    "    rpeak_dfs[record] = pd.read_csv(os.path.normpath('mit-bih-extracted/'+record+'_rpeaks.csv'), names=['rpeak'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rmean(rrInts):\n",
    "    rmeans = []\n",
    "    for index, value in enumerate(rrInts):\n",
    "        if index==0:\n",
    "            rmeans.append(value)\n",
    "        else:\n",
    "            rmeans.append(0.75*rmeans[index-1] + 0.25*value)\n",
    "    \n",
    "    return rmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rmssd(subset):\n",
    "    rrInts = subset['rrInt'].tolist()\n",
    "    sum_of_squares = 0\n",
    "    for idx, rrInt in enumerate(rrInts):\n",
    "        if idx<len(rrInts)-1:\n",
    "            square_difference = (rrInt-rrInts[idx-1])**2\n",
    "            sum_of_squares+=square_difference\n",
    "    mean_sum = sum_of_squares/(len(rrInts)-1)\n",
    "    return np.sqrt(mean_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_subject(full_df, rpeak_df, interval_length = 4, calib_length = 100):\n",
    "    rpeaks = rpeak_df['rpeak'].tolist()\n",
    "\n",
    "    current_subset = []\n",
    "    subsets = [] \n",
    "\n",
    "    beginning_idx = 0\n",
    "    outlier_comp = 0\n",
    "    prev_peak = rpeaks[beginning_idx]\n",
    "    for idx, peak in enumerate(rpeaks[:calib_length]):\n",
    "        if idx<len(rpeaks)-1:\n",
    "            if peak-prev_peak<500:\n",
    "                current_subset.append(peak)\n",
    "            else:\n",
    "                outlier_comp+=1\n",
    "            prev_peak = peak\n",
    "        else:\n",
    "            rr_int_column = [current_subset[x]-current_subset[x-1] for x in range(1, len(current_subset))]\n",
    "            rhythm_column = []\n",
    "            for x in range(1, len(current_subset)):\n",
    "                if full_df['Normal'][current_subset[x]]:\n",
    "                    rhythm_column.append('N')\n",
    "                elif full_df['AFIB'][current_subset[x]]:\n",
    "                    rhythm_column.append('A')\n",
    "                elif full_df['Other'][current_subset[x]]:\n",
    "                    rhythm_column.append('O')\n",
    "            \n",
    "            rmean_column = extract_rmean(rr_int_column)\n",
    "\n",
    "            subsets.append(pd.DataFrame({'rhythmLabel': rhythm_column, 'rrInt': rr_int_column, 'rmean': rmean_column}, columns=['rhythmLabel', 'rrInt', 'rmean']))\n",
    "    \n",
    "    current_subset = []\n",
    "    outlier_comp = 0\n",
    "    prev_peak = rpeaks[calib_length-1]\n",
    "\n",
    "    counter = 0\n",
    "    for idx, peak in enumerate(rpeaks[calib_length:], calib_length):\n",
    "        if idx<len(rpeaks)-1:\n",
    "            if counter-outlier_comp<=interval_length:\n",
    "                if peak-prev_peak<500:\n",
    "                    current_subset.append(peak)\n",
    "                    counter+=1\n",
    "                else:\n",
    "                    outlier_comp+=1\n",
    "                prev_peak = peak\n",
    "            elif peak-prev_peak>500:\n",
    "                outlier_comp+=1\n",
    "            else:\n",
    "                rr_int_column = [current_subset[x]-current_subset[x-1] for x in range(1, len(current_subset))]\n",
    "                rhythm_column = []\n",
    "                for x in range(1, len(current_subset)):\n",
    "                    if full_df['Normal'][current_subset[x]]:\n",
    "                        rhythm_column.append('N')\n",
    "                    elif full_df['AFIB'][current_subset[x]]:\n",
    "                        rhythm_column.append('A')\n",
    "                    elif full_df['Other'][current_subset[x]]:\n",
    "                        rhythm_column.append('O')\n",
    "                \n",
    "                rmean_column = extract_rmean(rr_int_column)\n",
    "\n",
    "                subsets.append(pd.DataFrame({'rhythmLabel': rhythm_column, 'rrInt': rr_int_column, 'rmean': rmean_column}, columns=['rhythmLabel', 'rrInt', 'rmean']))\n",
    "                current_subset = []\n",
    "                outlier_comp = 0\n",
    "                counter = 0\n",
    "                prev_peak = rpeaks[idx]\n",
    "\n",
    "    return subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 23/23 [02:40<00:00,  6.97s/it]\n"
     ]
    }
   ],
   "source": [
    "subset_dfs = {}\n",
    "for record in tqdm(rlist):\n",
    "    subset_dfs[record] = subset_subject(full_dfs[record], rpeak_dfs[record])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 23/23 [02:47<00:00,  7.29s/it]\n"
     ]
    }
   ],
   "source": [
    "for record in tqdm(rlist):\n",
    "    subsets = subset_dfs[record]\n",
    "    idx_list = list(range(len(subsets)))\n",
    "    \n",
    "    data = {\n",
    "        \"subjectID\": [record]*len(subsets),\n",
    "        \"subsetID\": idx_list,\n",
    "        \"rhythmLabel\": [subsets[x]['rhythmLabel'].mode()[0] for x in idx_list]\n",
    "    }\n",
    "    \n",
    "    subset_list = pd.DataFrame(data)\n",
    "    subset_list['mappedLabel'] = subset_list['rhythmLabel'].map({'N': 'Non-Afib', 'A': 'Afib', 'O': 'Non-Afib'})\n",
    "    subset_list.to_csv(os.path.normpath('mit-bih-time-subsets/'+record+\"_subset_list.csv\"))\n",
    "    \n",
    "    os.makedirs('mit-bih-time-subsets/'+str(record), exist_ok=True)\n",
    "\n",
    "    for x, subset in enumerate(subsets):\n",
    "        subset.to_csv(os.path.normpath('mit-bih-time-subsets/'+str(record)+'/'+str(record)+\"-\"+str(idx_list[x])+\".csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_features(subset_list, current_weight = 0.25, prev_weight = 0.75):\n",
    "    subset_dfs = {}\n",
    "    for x, subset in enumerate(subset_list.itertuples()):\n",
    "        subset_dfs[x] = pd.read_csv(os.path.normpath('mit-bih-time-subsets/'+str(subset.subjectID)+'/'+str(subset.subjectID)+\"-\"+str(x)+\".csv\"), index_col=0)\n",
    "\n",
    "    calib_df = subset_dfs[0]\n",
    "\n",
    "    feature_dict = {}\n",
    "\n",
    "    props = find_proportions(classify_rr_ints(calib_df))\n",
    "    feature_dict['StoS'] = [props[0]]\n",
    "    feature_dict['StoR'] = [props[1]]\n",
    "    feature_dict['StoL'] = [props[2]]\n",
    "    feature_dict['RtoS'] = [props[3]]\n",
    "    feature_dict['RtoR'] = [props[4]]\n",
    "    feature_dict['RtoL'] = [props[5]]\n",
    "    feature_dict['LtoS'] = [props[6]]\n",
    "    feature_dict['LtoR'] = [props[7]]\n",
    "    feature_dict['LtoL'] = [props[8]]\n",
    "\n",
    "    feature_dict['std'] = [np.std(calib_df['rrInt'])]\n",
    "    feature_dict['cov'] = [feature_dict['std'][0]/np.mean(calib_df['rrInt'])]\n",
    "    feature_dict['range'] = [np.max(calib_df['rrInt'])-np.min(calib_df['rrInt'])]\n",
    "    #feature_dict['rmean'] = df['rmean'].tolist()\n",
    "    #feature_dict['rrv'] = df['rr_variance'].tolist()\n",
    "    feature_dict['rrInt_var'] = [calib_df['rrInt'].var()]\n",
    "    feature_dict['rmean_var'] = [calib_df['rmean'].var()]\n",
    "    feature_dict['rmssd'] = [extract_rmssd(calib_df)]\n",
    "    feature_dict['mad'] = [stats.median_abs_deviation(calib_df['rrInt'])]\n",
    "    feature_dict['iqr'] = [stats.iqr(calib_df['rrInt'])]\n",
    "\n",
    "    \n",
    "    for key in subset_dfs:\n",
    "        if key>0:\n",
    "            props = find_proportions(classify_rr_ints(subset_dfs[key]))\n",
    "            feature_dict['StoS'].append(props[0]*current_weight + feature_dict['StoS'][key-1]*prev_weight)\n",
    "            feature_dict['StoR'].append(props[1]*current_weight + feature_dict['StoR'][key-1]*prev_weight)\n",
    "            feature_dict['StoL'].append(props[2]*current_weight + feature_dict['StoL'][key-1]*prev_weight)\n",
    "            feature_dict['RtoS'].append(props[3]*current_weight + feature_dict['RtoS'][key-1]*prev_weight)\n",
    "            feature_dict['RtoR'].append(props[4]*current_weight + feature_dict['RtoR'][key-1]*prev_weight)\n",
    "            feature_dict['RtoL'].append(props[5]*current_weight + feature_dict['RtoL'][key-1]*prev_weight)\n",
    "            feature_dict['LtoS'].append(props[6]*current_weight + feature_dict['LtoS'][key-1]*prev_weight)\n",
    "            feature_dict['LtoR'].append(props[7]*current_weight + feature_dict['LtoR'][key-1]*prev_weight)\n",
    "            feature_dict['LtoL'].append(props[8]*current_weight + feature_dict['LtoL'][key-1]*prev_weight)\n",
    "\n",
    "            feature_dict['std'].append(np.std(subset_dfs[key]['rrInt'])*current_weight + feature_dict['std'][key-1]*prev_weight)\n",
    "            feature_dict['cov'].append((feature_dict['std'][key]/np.mean(subset_dfs[key]['rrInt']))*current_weight + feature_dict['cov'][key-1]*prev_weight)\n",
    "            feature_dict['range'].append(np.max(subset_dfs[key]['rrInt'])-np.min(subset_dfs[key]['rrInt'])*current_weight + feature_dict['range'][key-1]*prev_weight)\n",
    "            #feature_dict['rmean'] = df['rmean'].tolist()\n",
    "            #feature_dict['rrv'] = df['rr_variance'].tolist()\n",
    "            feature_dict['rrInt_var'].append(subset_dfs[key]['rrInt'].var()*current_weight + feature_dict['rrInt_var'][key-1]*prev_weight)\n",
    "            feature_dict['rmean_var'].append(subset_dfs[key]['rmean'].var()*current_weight + feature_dict['rmean_var'][key-1]*prev_weight)\n",
    "            feature_dict['rmssd'].append(extract_rmssd(subset_dfs[key])*current_weight + feature_dict['rmssd'][key-1]*prev_weight)\n",
    "            feature_dict['mad'].append(stats.median_abs_deviation(subset_dfs[key]['rrInt'])*current_weight + feature_dict['mad'][key-1]*prev_weight)\n",
    "            feature_dict['iqr'].append(stats.iqr(subset_dfs[key]['rrInt'])*current_weight + feature_dict['iqr'][key-1]*prev_weight)\n",
    "\n",
    "    feature_df = pd.DataFrame(data=feature_dict)\n",
    "    return pd.concat([subset_list, feature_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 23/23 [08:48<00:00, 22.99s/it]\n"
     ]
    }
   ],
   "source": [
    "features_df = {}\n",
    "for record in tqdm(rlist):\n",
    "    subset_list = pd.read_csv('mit-bih-time-subsets/'+record+'_subset_list.csv', index_col=0, dtype={'subjectID': str})\n",
    "    features = subset_features(subset_list)\n",
    "\n",
    "    features.to_csv(os.path.normpath('mit-bih-time-features/'+record+\".csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
