{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 70 #display 70 dpi in Jupyter Notebook, may consider100 dpi \n",
    "plt.rcParams['savefig.dpi'] = 300 #define 300 dpi for saving figures\n",
    "\n",
    "import seaborn as sns\n",
    "## here are some settings \n",
    "sns.set_style('whitegrid')\n",
    "sns.set(rc={\"figure.dpi\":70, 'savefig.dpi':300}) #defining dpi setting\n",
    "sns.set_context('notebook')\n",
    "sns.set_style(\"ticks\")\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('retina')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tells matplotlib to display images inline instead of a new window\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import statistics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from time import time\n",
    "import timeit #imports timeit module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "import sklearn.linear_model as skl_lm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import neighbors\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlist = []\n",
    "records = os.path.normpath('mit-bih-dataframes/subject_list.csv')\n",
    "with open(records) as rfile:\n",
    "    recordreader = csv.reader(rfile, delimiter=' ', quotechar='|')\n",
    "    for row in recordreader:\n",
    "        rlist.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dfs = {}\n",
    "for record in tqdm(rlist):\n",
    "    feature_dfs[record] = pd.read_csv(os.path.normpath('mit-bih-time-features/'+record+'.csv'), index_col=0, dtype={'subjectID': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statallfeatures_dict = {\n",
    "    'Run Time': [],\n",
    "    'Accuracy': [],   ## To store the MEAN accuracy for 5-fold CV for each model\n",
    "    'Standard Error': [],\n",
    "    'Sensitivity': [],\n",
    "    'Specificity': [],\n",
    "    'Precision': [],\n",
    "    'F1_Score': []   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "decisionTree = DecisionTreeClassifier(max_depth=6) #criterion='entropy'\n",
    "\n",
    "acc_score = []\n",
    "Truth = []\n",
    "Output = []\n",
    "\n",
    "for idx, record in enumerate(feature_dfs):\n",
    "    test_df = feature_dfs[record][1:]\n",
    "    train_df = pd.concat([value[1:] for key, value in feature_dfs.items() if key != record])\n",
    "\n",
    "    X_train = train_df[['StoS', 'StoR', 'StoL', 'RtoS', 'RtoR', 'RtoL', 'LtoS', 'LtoR', 'LtoL', 'std', 'cov', 'range', 'rrInt_var', 'rmean_var', 'rmssd', 'mad', 'iqr']]\n",
    "    y_train = train_df['mappedLabel']\n",
    "\n",
    "    X_test = test_df[['StoS', 'StoR', 'StoL', 'RtoS', 'RtoR', 'RtoL', 'LtoS', 'LtoR', 'LtoL', 'std', 'cov', 'range', 'rrInt_var', 'rmean_var', 'rmssd', 'mad', 'iqr']]\n",
    "    y_test = test_df['mappedLabel']\n",
    "\n",
    "    decisionTree.fit(X_train, y_train)\n",
    "    pred_values = decisionTree.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(y_test.values.reshape(y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print(\"confusion matrix: \")\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "print(cm)\n",
    "print(\"classification report: \")\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "statallfeatures_dict['Run Time'].append(elapsed)\n",
    "statallfeatures_dict['Accuracy'].append(np.mean(acc_score))\n",
    "statallfeatures_dict['Standard Error'].append(np.std(acc_score))\n",
    "statallfeatures_dict['Sensitivity'].append(sensitivity)\n",
    "statallfeatures_dict['Specificity'].append(specificity)\n",
    "statallfeatures_dict['Precision'].append(precision)\n",
    "statallfeatures_dict['F1_Score'].append(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "bagging = RandomForestClassifier(max_features = 17, random_state = 2)\n",
    "\n",
    "acc_score = []\n",
    "Truth = []\n",
    "Output = []\n",
    "\n",
    "for record in tqdm(feature_dfs):\n",
    "    test_df = feature_dfs[record][1:]\n",
    "    train_df = pd.concat([value[1:] for key, value in feature_dfs.items() if key != record])\n",
    "\n",
    "    X_train = train_df[['StoS', 'StoR', 'StoL', 'RtoS', 'RtoR', 'RtoL', 'LtoS', 'LtoR', 'LtoL', 'std', 'cov', 'range', 'rrInt_var', 'rmean_var', 'rmssd', 'mad', 'iqr']]\n",
    "    y_train = train_df['mappedLabel']\n",
    "\n",
    "    X_test = test_df[['StoS', 'StoR', 'StoL', 'RtoS', 'RtoR', 'RtoL', 'LtoS', 'LtoR', 'LtoL', 'std', 'cov', 'range', 'rrInt_var', 'rmean_var', 'rmssd', 'mad', 'iqr']]\n",
    "    y_test = test_df['mappedLabel']\n",
    "\n",
    "    bagging.fit(X_train, y_train)\n",
    "    pred_values = bagging.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(y_test.values.reshape(y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print(\"confusion matrix: \")\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "print(cm)\n",
    "print(\"classification report: \")\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "statallfeatures_dict['Run Time'].append(elapsed)\n",
    "statallfeatures_dict['Accuracy'].append(np.mean(acc_score))\n",
    "statallfeatures_dict['Standard Error'].append(np.std(acc_score))\n",
    "statallfeatures_dict['Sensitivity'].append(sensitivity)\n",
    "statallfeatures_dict['Specificity'].append(specificity)\n",
    "statallfeatures_dict['Precision'].append(precision)\n",
    "statallfeatures_dict['F1_Score'].append(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "randomForest = RandomForestClassifier(max_features = 4, random_state = 2)\n",
    "\n",
    "acc_score = []\n",
    "Truth = []\n",
    "Output = []\n",
    "\n",
    "for idx, record in enumerate(feature_dfs):\n",
    "    test_df = feature_dfs[record][1:]\n",
    "    train_df = pd.concat([value[1:] for key, value in feature_dfs.items() if key != record])\n",
    "\n",
    "    X_train = train_df[['StoS', 'StoR', 'StoL', 'RtoS', 'RtoR', 'RtoL', 'LtoS', 'LtoR', 'LtoL', 'std', 'cov', 'range', 'rrInt_var', 'rmean_var', 'rmssd', 'mad', 'iqr']]\n",
    "    y_train = train_df['mappedLabel']\n",
    "\n",
    "    X_test = test_df[['StoS', 'StoR', 'StoL', 'RtoS', 'RtoR', 'RtoL', 'LtoS', 'LtoR', 'LtoL', 'std', 'cov', 'range', 'rrInt_var', 'rmean_var', 'rmssd', 'mad', 'iqr']]\n",
    "    y_test = test_df['mappedLabel']\n",
    "\n",
    "    randomForest.fit(X_train, y_train)\n",
    "    pred_values = randomForest.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(y_test.values.reshape(y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print(\"confusion matrix: \")\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "print(cm)\n",
    "print(\"classification report: \")\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "statallfeatures_dict['Run Time'].append(elapsed)\n",
    "statallfeatures_dict['Accuracy'].append(np.mean(acc_score))\n",
    "statallfeatures_dict['Standard Error'].append(np.std(acc_score))\n",
    "statallfeatures_dict['Sensitivity'].append(sensitivity)\n",
    "statallfeatures_dict['Specificity'].append(specificity)\n",
    "statallfeatures_dict['Precision'].append(precision)\n",
    "statallfeatures_dict['F1_Score'].append(f1_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
