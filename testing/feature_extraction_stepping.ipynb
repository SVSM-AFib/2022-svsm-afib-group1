{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from pathlib import Path, PurePath\n",
    "import csv\n",
    "from scipy import stats\n",
    "import math\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlist = []\n",
    "extractedpath = PurePath(Path(os.getcwd()).parents[0], 'mit-bih-extracted-stepping/')\n",
    "records = extractedpath / 'subject_list.csv'\n",
    "with open(records) as rfile: # reads in all of the subject IDs\n",
    "    recordreader = csv.reader(rfile, delimiter=' ', quotechar='|')\n",
    "    for row in recordreader:\n",
    "        rlist.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_rr_ints(df):\n",
    "    #list of types of rr_ints for each subject\n",
    "    subject_types = []\n",
    "    for row in df.itertuples():\n",
    "        if row.rrInt < 0.85*row.rmean: \n",
    "            #if rr_ints is less than 85% of runningmean\n",
    "            #label subject type as short\n",
    "            subject_types.append('short')\n",
    "        elif row.rrInt > 1.15*row.rmean: \n",
    "            #if rr_ints is greater than 115% of runningmean\n",
    "            #label subject type as long\n",
    "            subject_types.append('long')\n",
    "        else:\n",
    "            #label subject type as regular\n",
    "            subject_types.append('regular')\n",
    "    \n",
    "    return subject_types\n",
    "\n",
    "def find_proportions(int_types): # take the interval types and count the transitions/return the proportions\n",
    "    StoS = 0\n",
    "    StoR = 0\n",
    "    StoL = 0\n",
    "    RtoS = 0\n",
    "    RtoR = 0\n",
    "    RtoL = 0\n",
    "    LtoS = 0\n",
    "    LtoR = 0\n",
    "    LtoL = 0\n",
    "    for idx in range(len(int_types)-1):\n",
    "        if int_types[idx]=='short' and int_types[idx+1]=='short':\n",
    "            StoS+=1\n",
    "        elif int_types[idx]=='short' and int_types[idx+1]=='regular':\n",
    "            StoR+=1\n",
    "        elif int_types[idx]=='short' and int_types[idx+1]=='long':\n",
    "            StoL+=1\n",
    "        elif int_types[idx]=='regular' and int_types[idx+1]=='short':\n",
    "            RtoS+=1\n",
    "        elif int_types[idx]=='regular' and int_types[idx+1]=='regular':\n",
    "            RtoR+=1\n",
    "        elif int_types[idx]=='regular' and int_types[idx+1]=='long':\n",
    "            RtoL+=1\n",
    "        elif int_types[idx]=='long' and int_types[idx+1]=='short':\n",
    "            LtoS+=1\n",
    "        elif int_types[idx]=='long' and int_types[idx+1]=='regular':\n",
    "            LtoR+=1\n",
    "        elif int_types[idx]=='long' and int_types[idx+1]=='long':\n",
    "            LtoL+=1\n",
    "    \n",
    "    count = len(int_types)-1\n",
    "    return [StoS/count, StoR/count, StoL/count, RtoS/count, RtoR/count, RtoL/count, LtoS/count, LtoR/count, LtoL/count]\n",
    "\n",
    "def extract_rmssd(subset): # calculate the RMSSD of a subset\n",
    "    rrInts = subset['rrInt'].to_numpy()\n",
    "    diffs = np.diff(rrInts)\n",
    "    sum_of_squares = np.sum(diffs**2)\n",
    "    return np.sqrt(sum_of_squares/len(diffs))\n",
    "\n",
    "def shannon_entropy(subset):\n",
    "    # Get the frequency of each rrint classification in the data\n",
    "    frequencies = Counter(subset)\n",
    "    \n",
    "    # Calculate the probability of each classification\n",
    "    probabilities = [float(frequency) / len(subset) for frequency in frequencies.values()]\n",
    "    \n",
    "    # Calculate the Shannon entropy\n",
    "    entropy = -sum(probability * math.log(probability, 2) for probability in probabilities)\n",
    "    \n",
    "    return entropy\n",
    "def approx_entropy(subset, m=2, r=None):\n",
    "    def _maxdist(x_i, x_j):\n",
    "        return max([abs(ua - va) for ua, va in zip(x_i, x_j)])\n",
    "\n",
    "    def _phi(m):\n",
    "        x = [[subset[j] for j in range(i, i + m - 1 + 1)] for i in range(N - m + 1)]\n",
    "        C = [len([1 for x_j in x if _maxdist(x_i, x_j) <= r]) / (N - m + 1.0) for x_i in x]\n",
    "        return (N - m + 1.0)**(-1) * sum(np.log(C))\n",
    "\n",
    "    N = len(subset)\n",
    "\n",
    "    if r is None:\n",
    "        r = 0.2 * np.std(subset)\n",
    "\n",
    "    return abs(_phi(m+1) - _phi(m))\n",
    "\n",
    "# usage\n",
    "# subset is a list of RR intervals fr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_features(record, subsetpath, current_weight = 0.25, prev_weight = 0.75):\n",
    "    subset_list = pd.read_parquet(subsetpath / (record+'_subset_list.parquet')) # read the subset list for that subject\n",
    "    feature_dict = {} # create a dictionary to store all of the features\n",
    "\n",
    "    calib_df = pd.read_parquet(subsetpath / record / (record+\"-\"+str(0)+\".parquet\")) # read the first subset--the calibration window\n",
    "\n",
    "    # calculate the features for the calibration window\n",
    "    # also initalize all of the lists in the dictionary that will later become columns\n",
    "    props = find_proportions(classify_rr_ints(calib_df))\n",
    "    feature_dict['StoS'] = [props[0]]\n",
    "    feature_dict['StoR'] = [props[1]]\n",
    "    feature_dict['StoL'] = [props[2]]\n",
    "    feature_dict['RtoS'] = [props[3]]\n",
    "    feature_dict['RtoR'] = [props[4]]\n",
    "    feature_dict['RtoL'] = [props[5]]\n",
    "    feature_dict['LtoS'] = [props[6]]\n",
    "    feature_dict['LtoR'] = [props[7]]\n",
    "    feature_dict['LtoL'] = [props[8]]\n",
    "\n",
    "    feature_dict['std'] = [np.std(calib_df['rrInt'])]\n",
    "    feature_dict['cov'] = [feature_dict['std'][0]/np.mean(calib_df['rrInt'])]\n",
    "    feature_dict['range'] = [np.max(calib_df['rrInt'])-np.min(calib_df['rrInt'])]\n",
    "    feature_dict['rrInt_var'] = [calib_df['rrInt'].var()]\n",
    "    feature_dict['rmean_var'] = [calib_df['rmean'].var()]\n",
    "    feature_dict['rmssd'] = [extract_rmssd(calib_df)]\n",
    "    feature_dict['mad'] = [stats.median_abs_deviation(calib_df['rrInt'])]\n",
    "    feature_dict['iqr'] = [stats.iqr(calib_df['rrInt'])]\n",
    "    feature_dict['entropy'] = [shannon_entropy(calib_df)]\n",
    "    feature_dict['approx_entropy'] = [approx_entropy(calib_df['rrInt'])]\n",
    "\n",
    "    # looping over all of the windows\n",
    "    for idx in tqdm(range(1, len(subset_list)), desc=\"Calculating features for each window\"):\n",
    "        window = pd.read_parquet(subsetpath / record / (record+\"-\"+str(idx)+\".parquet\")) # read the window\n",
    "\n",
    "        # calculate the features while including the weights\n",
    "        props = find_proportions(classify_rr_ints(window))\n",
    "        feature_dict['StoS'].append(props[0]*current_weight + feature_dict['StoS'][idx-1]*prev_weight)\n",
    "        feature_dict['StoR'].append(props[1]*current_weight + feature_dict['StoR'][idx-1]*prev_weight)\n",
    "        feature_dict['StoL'].append(props[2]*current_weight + feature_dict['StoL'][idx-1]*prev_weight)\n",
    "        feature_dict['RtoS'].append(props[3]*current_weight + feature_dict['RtoS'][idx-1]*prev_weight)\n",
    "        feature_dict['RtoR'].append(props[4]*current_weight + feature_dict['RtoR'][idx-1]*prev_weight)\n",
    "        feature_dict['RtoL'].append(props[5]*current_weight + feature_dict['RtoL'][idx-1]*prev_weight)\n",
    "        feature_dict['LtoS'].append(props[6]*current_weight + feature_dict['LtoS'][idx-1]*prev_weight)\n",
    "        feature_dict['LtoR'].append(props[7]*current_weight + feature_dict['LtoR'][idx-1]*prev_weight)\n",
    "        feature_dict['LtoL'].append(props[8]*current_weight + feature_dict['LtoL'][idx-1]*prev_weight)\n",
    "\n",
    "        feature_dict['std'].append(np.std(window['rrInt'])*current_weight + feature_dict['std'][idx-1]*prev_weight)\n",
    "        feature_dict['cov'].append((feature_dict['std'][idx]/np.mean(window['rrInt']))*current_weight + feature_dict['cov'][idx-1]*prev_weight)\n",
    "        feature_dict['range'].append(np.max(window['rrInt'])-np.min(window['rrInt'])*current_weight + feature_dict['range'][idx-1]*prev_weight)\n",
    "        feature_dict['rrInt_var'].append(window['rrInt'].var()*current_weight + feature_dict['rrInt_var'][idx-1]*prev_weight)\n",
    "        feature_dict['rmean_var'].append(window['rmean'].var()*current_weight + feature_dict['rmean_var'][idx-1]*prev_weight)\n",
    "        feature_dict['rmssd'].append(extract_rmssd(window)*current_weight + feature_dict['rmssd'][idx-1]*prev_weight)\n",
    "        feature_dict['mad'].append(stats.median_abs_deviation(window['rrInt'])*current_weight + feature_dict['mad'][idx-1]*prev_weight)\n",
    "        feature_dict['iqr'].append(stats.iqr(window['rrInt'])*current_weight + feature_dict['iqr'][idx-1]*prev_weight)\n",
    "        feature_dict['entropy'].append(shannon_entropy(window)*current_weight + feature_dict['entropy'][idx-1]*prev_weight)\n",
    "        feature_dict['approx_entropy'].append(approx_entropy(window['rrInt'])*current_weight + feature_dict['approx_entropy'][idx-1]*prev_weight)\n",
    "\n",
    "    feature_df = pd.DataFrame(data=feature_dict) # make a DataFrame out of the feature dictionary\n",
    "    return pd.concat([subset_list, feature_df], axis=1) # return the features DataFrame combined with the subset list DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsetpath = PurePath(Path(os.getcwd()).parents[0], 'mit-bih-time-subsets-stepping/')\n",
    "featurespath = PurePath(Path(os.getcwd()).parents[0], 'mit-bih-time-features-stepping/')\n",
    "if not os.path.exists(featurespath):\n",
    "    os.mkdir(featurespath)\n",
    "\n",
    "for record in tqdm(rlist): # calculate the features for all of the subjects\n",
    "    features = subset_features(record, subsetpath)\n",
    "    features.to_parquet(featurespath / (record+\".parquet\")) # and then write them to disk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:afib]",
   "language": "python",
   "name": "conda-env-afib-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
