{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from pathlib import Path, PurePath\n",
    "import csv\n",
    "from scipy import stats\n",
    "import math\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlist = []\n",
    "extractedpath = PurePath(Path(os.getcwd()).parents[0], 'mit-bih-extracted/')\n",
    "records = extractedpath / 'subject_list.csv'\n",
    "with open(records) as rfile: # reads in all of the subject IDs\n",
    "    recordreader = csv.reader(rfile, delimiter=' ', quotechar='|')\n",
    "    for row in recordreader:\n",
    "        rlist.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_rr_ints(df):\n",
    "    #list of types of rr_ints for each subject\n",
    "    subject_types = []\n",
    "    for row in df.itertuples():\n",
    "        if row.rrInt < 0.85*row.rmean: \n",
    "            #if rr_ints is less than 85% of runningmean\n",
    "            #label subject type as short\n",
    "            subject_types.append('short')\n",
    "        elif row.rrInt > 1.15*row.rmean: \n",
    "            #if rr_ints is greater than 115% of runningmean\n",
    "            #label subject type as long\n",
    "            subject_types.append('long')\n",
    "        else:\n",
    "            #label subject type as regular\n",
    "            subject_types.append('regular')\n",
    "    \n",
    "    return subject_types\n",
    "\n",
    "def find_proportions(int_types): # take the interval types and count the transitions/return the proportions\n",
    "    StoS = 0\n",
    "    StoR = 0\n",
    "    StoL = 0\n",
    "    RtoS = 0\n",
    "    RtoR = 0\n",
    "    RtoL = 0\n",
    "    LtoS = 0\n",
    "    LtoR = 0\n",
    "    LtoL = 0\n",
    "    for idx in range(len(int_types)-1):\n",
    "        if int_types[idx]=='short' and int_types[idx+1]=='short':\n",
    "            StoS+=1\n",
    "        elif int_types[idx]=='short' and int_types[idx+1]=='regular':\n",
    "            StoR+=1\n",
    "        elif int_types[idx]=='short' and int_types[idx+1]=='long':\n",
    "            StoL+=1\n",
    "        elif int_types[idx]=='regular' and int_types[idx+1]=='short':\n",
    "            RtoS+=1\n",
    "        elif int_types[idx]=='regular' and int_types[idx+1]=='regular':\n",
    "            RtoR+=1\n",
    "        elif int_types[idx]=='regular' and int_types[idx+1]=='long':\n",
    "            RtoL+=1\n",
    "        elif int_types[idx]=='long' and int_types[idx+1]=='short':\n",
    "            LtoS+=1\n",
    "        elif int_types[idx]=='long' and int_types[idx+1]=='regular':\n",
    "            LtoR+=1\n",
    "        elif int_types[idx]=='long' and int_types[idx+1]=='long':\n",
    "            LtoL+=1\n",
    "    \n",
    "    count = len(int_types)-1\n",
    "    return [StoS/count, StoR/count, StoL/count, RtoS/count, RtoR/count, RtoL/count, LtoS/count, LtoR/count, LtoL/count]\n",
    "\n",
    "def extract_rmssd(subset): # calculate the RMSSD of a subset\n",
    "    rrInts = subset['rrInt'].to_numpy()\n",
    "    diffs = np.diff(rrInts)\n",
    "    sum_of_squares = np.sum(diffs**2)\n",
    "    return np.sqrt(sum_of_squares/len(diffs))\n",
    "\n",
    "def shannon_entropy(subset):\n",
    "    # Get the frequency of each rrint classification in the data\n",
    "    frequencies = Counter(subset)\n",
    "    \n",
    "    # Calculate the probability of each classification\n",
    "    probabilities = [float(frequency) / len(subset) for frequency in frequencies.values()]\n",
    "    \n",
    "    # Calculate the Shannon entropy\n",
    "    entropy = -sum(probability * math.log(probability, 2) for probability in probabilities)\n",
    "    \n",
    "    return entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_features(record, subsetpath):\n",
    "    subset_list = pd.read_parquet(subsetpath / (record+'_subset_list.parquet')) # read the subset list for that subject\n",
    "    feature_dict = {} # create a dictionary to store all of the features\n",
    "\n",
    "    # looping over all of the windows\n",
    "    for idx in tqdm(range(len(subset_list)), desc=\"Calculating features for each window\"):\n",
    "        window = pd.read_parquet(subsetpath / record / (record+\"-\"+str(idx)+\".parquet\")) # read the window\n",
    "\n",
    "        # calculate the features for each individual window\n",
    "        props = find_proportions(classify_rr_ints(window))\n",
    "        feature_dict.setdefault('StoS', []).append(props[0])\n",
    "        feature_dict.setdefault('StoR', []).append(props[1])\n",
    "        feature_dict.setdefault('StoL', []).append(props[2])\n",
    "        feature_dict.setdefault('RtoS', []).append(props[3])\n",
    "        feature_dict.setdefault('RtoR', []).append(props[4])\n",
    "        feature_dict.setdefault('RtoL', []).append(props[5])\n",
    "        feature_dict.setdefault('LtoS', []).append(props[6])\n",
    "        feature_dict.setdefault('LtoR', []).append(props[7])\n",
    "        feature_dict.setdefault('LtoL', []).append(props[8])\n",
    "\n",
    "        feature_dict.setdefault('std', []).append(np.std(window['rrInt']))\n",
    "        feature_dict.setdefault('cov', []).append(np.std(window['rrInt'])/np.mean(window['rrInt']))\n",
    "        feature_dict.setdefault('range', []).append(np.max(window['rrInt'])-np.min(window['rrInt']))\n",
    "        feature_dict.setdefault('rrInt_var', []).append(window['rrInt'].var())\n",
    "        feature_dict.setdefault('rmean_var', []).append(window['rmean'].var())\n",
    "        feature_dict.setdefault('rmssd', []).append(extract_rmssd(window))\n",
    "        feature_dict.setdefault('mad', []).append(stats.median_abs_deviation(window['rrInt']))\n",
    "        feature_dict.setdefault('iqr', []).append(stats.iqr(window['rrInt']))\n",
    "        feature_dict.setdefault('entropy', []).append(shannon_entropy(window))\n",
    "\n",
    "    feature_df = pd.DataFrame(data=feature_dict) #data frame outta dict \n",
    "    return pd.concat([subset_list, feature_df], axis=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsetpath = PurePath(Path(os.getcwd()).parents[0], 'mit-bih-time-subsets/')\n",
    "featurespath = PurePath(Path(os.getcwd()).parents[0], 'mit-bih-time-features/')\n",
    "if not os.path.exists(featurespath):\n",
    "    os.mkdir(featurespath)\n",
    "\n",
    "for record in tqdm(rlist): # calculate the features for all of the subjects\n",
    "    features = subset_features(record, subsetpath)\n",
    "    features.to_parquet(featurespath / (record+\".parquet\")) # and then write them to disk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:afib]",
   "language": "python",
   "name": "conda-env-afib-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
