{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import os\n",
    "os.environ['NUMBA_CUDA_DRIVER'] = \"/usr/lib/wsl/lib/libcuda.so.1\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "from pathlib import Path, PurePath\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from joblib import Parallel, delayed, dump, load\n",
    "\n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\n",
    "\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from sklearn.base import clone\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from cuml.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from cuml.linear_model import LogisticRegression\n",
    "from cuml.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from cuml.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlist = []\n",
    "extractedpath = PurePath(Path(os.getcwd()).parents[0], 'mit-bih-extracted/')\n",
    "records = extractedpath / 'subject_list.csv'\n",
    "with open(records) as rfile: # reads in all of the subject IDs\n",
    "    recordreader = csv.reader(rfile, delimiter=' ', quotechar='|')\n",
    "    for row in recordreader:\n",
    "        rlist.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dfs = {}\n",
    "recent_size = 400\n",
    "featurespath = PurePath(Path(os.getcwd()).parents[0], f'mit-bih-time-features-stepping/{recent_size}/')\n",
    "for record in tqdm(rlist):\n",
    "    feature_dfs[record] = pd.read_parquet(featurespath / (record + '.parquet'))\n",
    "\n",
    "combined_features = pd.concat([feature_dfs[key] for key in feature_dfs], ignore_index=True)\n",
    "combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = combined_features[['StoS', 'StoR', 'StoL', 'RtoS', 'RtoR', 'RtoL', 'LtoS', 'LtoR', 'LtoL', 'std', 'cov', 'range', 'rrInt_var', 'rmean_var', 'rmssd', 'mad', 'iqr', 'entropy', 'approx_entropy']]#, 'drrmean', 'drrvar']]\n",
    "y = combined_features['mappedLabel'].map({\"Non-Afib\": 0, \"Afib\": 1})\n",
    "groups = combined_features['subjectID']\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "splits = list(logo.split(X, y, groups=groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.seterr(all='ignore')\n",
    "moving_accs = []\n",
    "\n",
    "current_weight = 0.5\n",
    "saved_results_path = PurePath(f'saved_results_{current_weight}_{recent_size}')\n",
    "if not os.path.exists(saved_results_path):\n",
    "    os.mkdir(saved_results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_reporter(results, importances=True, moving_acc_plot=False, classifier_name=\"idk\"):\n",
    "    bestParams = None\n",
    "    maxScore = 0\n",
    "    for params, scores in results.items():\n",
    "        num_splits = scores['folds']\n",
    "        accuracy = [scores[f\"split{i}_accuracy\"] for i in range(num_splits)]\n",
    "        print(params, np.mean(accuracy))\n",
    "        \n",
    "        if (np.mean(accuracy) > maxScore):\n",
    "            bestParams = params\n",
    "            maxScore = np.mean(accuracy)\n",
    "            \n",
    "    bestScores = results[bestParams]\n",
    "    num_splits = bestScores['folds']\n",
    "    accuracy = [bestScores[f\"split{i}_accuracy\"] for i in range(num_splits)]\n",
    "    sensitivity = [bestScores[f\"split{i}_sensitivity\"] for i in range(num_splits)]\n",
    "    specificity = [bestScores[f\"split{i}_specificity\"] for i in range(num_splits)]\n",
    "    precision = [bestScores[f\"split{i}_precision\"] for i in range(num_splits)]\n",
    "    f1_score = [bestScores[f\"split{i}_f1_score\"] for i in range(num_splits)]\n",
    "    if importances:\n",
    "        feature_importances = [list(bestScores[f\"split{i}_feature_importances\"].values()) for i in range(num_splits)]\n",
    "    \n",
    "        avg_importances = np.mean(np.array(feature_importances), axis=0)\n",
    "        feature_names = list(bestScores[\"split0_feature_importances\"].keys())[0]\n",
    "        mapped_importances = {name: rank for name, rank in zip(feature_names, avg_importances.flatten())}\n",
    "        \n",
    "    if moving_acc_plot:\n",
    "        subject_accs = [bestScores[f\"split{i}_subject_acc\"] for i in range(num_splits)]\n",
    "        minLen = len(subject_accs[0])\n",
    "        for accs in subject_accs:\n",
    "            if len(accs)<minLen:\n",
    "                minLen = len(accs)\n",
    "\n",
    "        avg_list = [sum(sub_list) * 100 / len(sub_list) for sub_list in zip(*subject_accs)]\n",
    "        plt.plot(avg_list)\n",
    "        plt.title(classifier_name + \" Accuracy Over Time\")\n",
    "        plt.xlabel(\"Stepping windows elapsed\")\n",
    "        plt.ylabel(\"Average accuracy across CV folds (%)\")\n",
    "        plt.show()\n",
    "    \n",
    "    print(f\"The best parameters were {bestParams}\")\n",
    "    print(f\"Accuracy for each fold: {accuracy}\")\n",
    "    print(f\"Mean accuracy: {np.nanmean(accuracy)}\")\n",
    "    print(f\"Std accuracy: {np.nanstd(accuracy)}\")\n",
    "    print(f\"Sensitivity for each fold: {sensitivity}\")\n",
    "    print(f\"Mean sensitivity: {np.nanmean(sensitivity)}\")\n",
    "    print(f\"Std sensitivity: {np.nanstd(sensitivity)}\")\n",
    "    print(f\"Specificity for each fold: {specificity}\")\n",
    "    print(f\"Mean specificity: {np.nanmean(specificity)}\")\n",
    "    print(f\"Std specificity: {np.nanstd(specificity)}\")\n",
    "    print(f\"Precision for each fold: {precision}\")\n",
    "    print(f\"Mean precision: {np.nanmean(precision)}\")\n",
    "    print(f\"Std precision: {np.nanstd(precision)}\")\n",
    "    print(f\"F1-score for each fold: {f1_score}\")\n",
    "    print(f\"Mean F1-score: {np.nanmean(f1_score)}\")\n",
    "    print(f\"Std F1-score: {np.nanstd(f1_score)}\")\n",
    "    if importances:\n",
    "        print(\"Average feature importances: \")\n",
    "        print(mapped_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_parallel(clf, X, y, train, test, **fit_params):\n",
    "    np.seterr(all='ignore')\n",
    "    \n",
    "    X_train = X.iloc[train]\n",
    "    y_train = y.iloc[train]\n",
    "\n",
    "    X_test = X.iloc[test]\n",
    "    y_test = y.iloc[test]\n",
    "    \n",
    "    cloned_clf = clone(clf)\n",
    "    cloned_clf.fit(X_train, y_train)\n",
    "\n",
    "    pred_values = cloned_clf.predict(X_test)\n",
    "    \n",
    "    total_seen = 0\n",
    "    total_correct = 0\n",
    "    subject_acc = []\n",
    "    for idx, pred in enumerate(pred_values):\n",
    "        total_seen+=1\n",
    "        if pred==y_test.iloc[idx]:\n",
    "            total_correct+=1\n",
    "        subject_acc.append(total_correct/total_seen)\n",
    "\n",
    "    cm = confusion_matrix(y_test.values.reshape(y_test.shape[0]), pred_values)\n",
    "    sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "    specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "    precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "\n",
    "    results_dict = {\n",
    "        \"accuracy\": accuracy_score(y_test, pred_values),\n",
    "        \"sensitivity\": sensitivity,\n",
    "        \"specificity\": specificity,\n",
    "        \"precision\": precision,\n",
    "        \"f1_score\": (2*precision*sensitivity)/(precision+sensitivity),\n",
    "        \"subject_acc\": subject_acc\n",
    "    }\n",
    "    \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_tree_parallel(clf, X, y, train, test, **fit_params):\n",
    "    np.seterr(all='ignore')\n",
    "    \n",
    "    X_train = X.iloc[train]\n",
    "    y_train = y.iloc[train]\n",
    "\n",
    "    X_test = X.iloc[test]\n",
    "    y_test = y.iloc[test]\n",
    "    \n",
    "    cloned_clf = clone(clf)\n",
    "    cloned_clf.fit(X_train, y_train)\n",
    "\n",
    "    pred_values = cloned_clf.predict(X_test)\n",
    "    \n",
    "    total_seen = 0\n",
    "    total_correct = 0\n",
    "    subject_acc = []\n",
    "    for idx, pred in enumerate(pred_values):\n",
    "        total_seen+=1\n",
    "        if pred==y_test.iloc[idx]:\n",
    "            total_correct+=1\n",
    "        subject_acc.append(total_correct/total_seen)\n",
    "\n",
    "    cm = confusion_matrix(y_test.values.reshape(y_test.shape[0]), pred_values)\n",
    "    sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "    specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "    precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "\n",
    "    results_dict = {\n",
    "        \"accuracy\": accuracy_score(y_test, pred_values),\n",
    "        \"sensitivity\": sensitivity,\n",
    "        \"specificity\": specificity,\n",
    "        \"precision\": precision,\n",
    "        \"f1_score\": (2*precision*sensitivity)/(precision+sensitivity),\n",
    "        \"feature_importances\": {A: B for A, B in zip(cloned_clf.feature_names_in_, cloned_clf['clf'].feature_importances_)},\n",
    "        \"subject_acc\": subject_acc\n",
    "    }\n",
    "    \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "folder = './joblib_memmap'\n",
    "try:\n",
    "    os.mkdir(folder)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "dump(X, os.path.join(folder, 'X'))\n",
    "X_memmap = load(os.path.join(folder, 'X'), mmap_mode='r')\n",
    "dump(y, os.path.join(folder, 'y'))\n",
    "y_memmap = load(os.path.join(folder, 'y'), mmap_mode='r')\n",
    "\n",
    "#num_combinations = 30\n",
    "params = {\n",
    "    \"solver\": [\"qn\"]\n",
    "}\n",
    "param_grid = ParameterGrid(params)\n",
    "print(f'Fitting with {len(param_grid)} different parameter combinations')\n",
    "results = {}\n",
    "\n",
    "with tqdm(param_grid) as pbar:\n",
    "    for fit_params in pbar:\n",
    "        pbar.set_description(f'Fitting parameter combination: {fit_params}')\n",
    "        \n",
    "        pipe = Pipeline([(\"scaler\", StandardScaler()), \n",
    "                         (\"clf\", LogisticRegression(max_iter=3000,\n",
    "                                 **fit_params))])\n",
    "        fold_results = list(tqdm(Parallel(n_jobs=2, return_as=\"generator\")(\n",
    "            delayed(fit_parallel)(pipe, X_memmap, y_memmap, train, test, **fit_params)\n",
    "            for (train, test) in splits\n",
    "        ), \n",
    "        desc=\"Fitting on each fold\",\n",
    "        total=len(splits)))\n",
    "        \n",
    "        current_results = {}\n",
    "        for i, result in enumerate(fold_results):\n",
    "            current_results[f\"split{i}_accuracy\"] = result[\"accuracy\"]\n",
    "            current_results[f\"split{i}_sensitivity\"] = result[\"sensitivity\"]\n",
    "            current_results[f\"split{i}_specificity\"] = result[\"specificity\"]\n",
    "            current_results[f\"split{i}_precision\"] = result[\"precision\"]\n",
    "            current_results[f\"split{i}_f1_score\"] = result[\"f1_score\"]\n",
    "            current_results[f\"split{i}_subject_acc\"] = result[\"subject_acc\"]\n",
    "        results[tuple(sorted(fit_params.items()))] = current_results\n",
    "        results[tuple(sorted(fit_params.items()))]['folds'] = len(splits)\n",
    "\n",
    "with open(f'saved_results_{current_weight}_{recent_size}/lr_results.pickle', 'wb') as handle:\n",
    "    pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_reporter(results, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "folder = './joblib_memmap'\n",
    "try:\n",
    "    os.mkdir(folder)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "dump(X, os.path.join(folder, 'X'))\n",
    "X_memmap = load(os.path.join(folder, 'X'), mmap_mode='r')\n",
    "dump(y, os.path.join(folder, 'y'))\n",
    "y_memmap = load(os.path.join(folder, 'y'), mmap_mode='r')\n",
    "\n",
    "#num_combinations = 30\n",
    "params = {\n",
    "    \"solver\": [\"lsqr\"]\n",
    "}\n",
    "param_grid = ParameterGrid(params)\n",
    "print(f'Fitting with {len(param_grid)} different parameter combinations')\n",
    "results = {}\n",
    "\n",
    "with tqdm(param_grid) as pbar:\n",
    "    for fit_params in pbar:\n",
    "        pbar.set_description(f'Fitting parameter combination: {fit_params}')\n",
    "        \n",
    "        pipe = Pipeline([(\"scaler\", StandardScaler()), \n",
    "                         (\"clf\", LinearDiscriminantAnalysis(**fit_params))])\n",
    "        fold_results = list(tqdm(Parallel(n_jobs=4, return_as=\"generator\")(\n",
    "            delayed(fit_parallel)(pipe, X_memmap, y_memmap, train, test, **fit_params)\n",
    "            for (train, test) in splits\n",
    "        ), \n",
    "        desc=\"Fitting on each fold\",\n",
    "        total=len(splits)))\n",
    "        \n",
    "        current_results = {}\n",
    "        for i, result in enumerate(fold_results):\n",
    "            current_results[f\"split{i}_accuracy\"] = result[\"accuracy\"]\n",
    "            current_results[f\"split{i}_sensitivity\"] = result[\"sensitivity\"]\n",
    "            current_results[f\"split{i}_specificity\"] = result[\"specificity\"]\n",
    "            current_results[f\"split{i}_precision\"] = result[\"precision\"]\n",
    "            current_results[f\"split{i}_f1_score\"] = result[\"f1_score\"]\n",
    "            current_results[f\"split{i}_subject_acc\"] = result[\"subject_acc\"]\n",
    "        results[tuple(sorted(fit_params.items()))] = current_results\n",
    "        results[tuple(sorted(fit_params.items()))]['folds'] = len(splits)\n",
    "\n",
    "with open(f'saved_results_{current_weight}_{recent_size}/lda_results.pickle', 'wb') as handle:\n",
    "    pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_reporter(results, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QDA\n",
    "folder = './joblib_memmap'\n",
    "try:\n",
    "    os.mkdir(folder)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "dump(X, os.path.join(folder, 'X'))\n",
    "X_memmap = load(os.path.join(folder, 'X'), mmap_mode='r')\n",
    "dump(y, os.path.join(folder, 'y'))\n",
    "y_memmap = load(os.path.join(folder, 'y'), mmap_mode='r')\n",
    "\n",
    "#num_combinations = 30\n",
    "params = {\n",
    "    \"fake\": [\"param\"]\n",
    "}\n",
    "param_grid = ParameterGrid(params)\n",
    "print(f'Fitting with {len(param_grid)} different parameter combinations')\n",
    "results = {}\n",
    "\n",
    "with tqdm(param_grid) as pbar:\n",
    "    for fit_params in pbar:\n",
    "        pbar.set_description(f'Fitting parameter combination: {fit_params}')\n",
    "        \n",
    "        pipe = Pipeline([(\"scaler\", StandardScaler()), \n",
    "                         (\"clf\", QuadraticDiscriminantAnalysis())])\n",
    "        fold_results = list(tqdm(Parallel(n_jobs=4, return_as=\"generator\")(\n",
    "            delayed(fit_parallel)(pipe, X_memmap, y_memmap, train, test, **fit_params)\n",
    "            for (train, test) in splits\n",
    "        ), \n",
    "        desc=\"Fitting on each fold\",\n",
    "        total=len(splits)))\n",
    "        \n",
    "        current_results = {}\n",
    "        for i, result in enumerate(fold_results):\n",
    "            current_results[f\"split{i}_accuracy\"] = result[\"accuracy\"]\n",
    "            current_results[f\"split{i}_sensitivity\"] = result[\"sensitivity\"]\n",
    "            current_results[f\"split{i}_specificity\"] = result[\"specificity\"]\n",
    "            current_results[f\"split{i}_precision\"] = result[\"precision\"]\n",
    "            current_results[f\"split{i}_f1_score\"] = result[\"f1_score\"]\n",
    "            current_results[f\"split{i}_subject_acc\"] = result[\"subject_acc\"]\n",
    "        results[tuple(sorted(fit_params.items()))] = current_results\n",
    "        results[tuple(sorted(fit_params.items()))]['folds'] = len(splits)\n",
    "\n",
    "with open(f'saved_results_{current_weight}_{recent_size}/qda_results.pickle', 'wb') as handle:\n",
    "    pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_reporter(results, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "folder = './joblib_memmap'\n",
    "try:\n",
    "    os.mkdir(folder)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "dump(X, os.path.join(folder, 'X'))\n",
    "X_memmap = load(os.path.join(folder, 'X'), mmap_mode='r')\n",
    "dump(y, os.path.join(folder, 'y'))\n",
    "y_memmap = load(os.path.join(folder, 'y'), mmap_mode='r')\n",
    "\n",
    "#num_combinations = 30\n",
    "params = {\n",
    "    \"n_neighbors\": [9]\n",
    "}\n",
    "param_grid = ParameterGrid(params)\n",
    "print(f'Fitting with {len(param_grid)} different parameter combinations')\n",
    "results = {}\n",
    "\n",
    "with tqdm(param_grid) as pbar:\n",
    "    for fit_params in pbar:\n",
    "        pbar.set_description(f'Fitting parameter combination: {fit_params}')\n",
    "\n",
    "        pipe = Pipeline([(\"scaler\", StandardScaler()), \n",
    "                         (\"clf\", KNeighborsClassifier(**fit_params))])\n",
    "        fold_results = list(tqdm(Parallel(n_jobs=2, return_as=\"generator\")(\n",
    "            delayed(fit_parallel)(pipe, X_memmap, y_memmap, train, test, **fit_params)\n",
    "            for (train, test) in splits\n",
    "        ), \n",
    "        desc=\"Fitting on each fold\",\n",
    "        total=len(splits)))\n",
    "        \n",
    "        current_results = {}\n",
    "        for i, result in enumerate(fold_results):\n",
    "            current_results[f\"split{i}_accuracy\"] = result[\"accuracy\"]\n",
    "            current_results[f\"split{i}_sensitivity\"] = result[\"sensitivity\"]\n",
    "            current_results[f\"split{i}_specificity\"] = result[\"specificity\"]\n",
    "            current_results[f\"split{i}_precision\"] = result[\"precision\"]\n",
    "            current_results[f\"split{i}_f1_score\"] = result[\"f1_score\"]\n",
    "            current_results[f\"split{i}_subject_acc\"] = result[\"subject_acc\"]\n",
    "        results[tuple(sorted(fit_params.items()))] = current_results\n",
    "        results[tuple(sorted(fit_params.items()))]['folds'] = len(splits)\n",
    "\n",
    "with open(f'saved_results_{current_weight}_{recent_size}/knn_results.pickle', 'wb') as handle:\n",
    "    pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_reporter(results, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree\n",
    "folder = './joblib_memmap'\n",
    "try:\n",
    "    os.mkdir(folder)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "dump(X, os.path.join(folder, 'X'))\n",
    "X_memmap = load(os.path.join(folder, 'X'), mmap_mode='r')\n",
    "dump(y, os.path.join(folder, 'y'))\n",
    "y_memmap = load(os.path.join(folder, 'y'), mmap_mode='r')\n",
    "\n",
    "#num_combinations = 30\n",
    "params = {\n",
    "    \"max_depth\": [None]\n",
    "}\n",
    "param_grid = ParameterGrid(params)\n",
    "print(f'Fitting with {len(param_grid)} different parameter combinations')\n",
    "results = {}\n",
    "\n",
    "with tqdm(param_grid) as pbar:\n",
    "    for fit_params in pbar:\n",
    "        pbar.set_description(f'Fitting parameter combination: {fit_params}')\n",
    "        \n",
    "        pipe = Pipeline([(\"scaler\", StandardScaler()), \n",
    "                         (\"clf\", DecisionTreeClassifier(**fit_params))])\n",
    "        \n",
    "        fold_results = list(tqdm(Parallel(n_jobs=6, return_as=\"generator\")(\n",
    "            delayed(fit_parallel)(pipe, X_memmap, y_memmap, train, test, **fit_params)\n",
    "            for (train, test) in splits\n",
    "        ), \n",
    "        desc=\"Fitting on each fold\",\n",
    "        total=len(splits)))\n",
    "\n",
    "        # fold_results = []\n",
    "        # for (train, test) in tqdm(splits, desc=\"Fitting for each fold\"):\n",
    "        #     X_train = X.iloc[train]\n",
    "        #     y_train = y.iloc[train]\n",
    "\n",
    "        #     X_test = X.iloc[test]\n",
    "        #     y_test = y.iloc[test]\n",
    "\n",
    "        #     cloned_clf = clone(pipe)\n",
    "        #     cloned_clf.fit(X_train, y_train)\n",
    "\n",
    "        #     pred_values = cloned_clf.predict(X_test)\n",
    "    \n",
    "        #     total_seen = 0\n",
    "        #     total_correct = 0\n",
    "        #     subject_acc = []\n",
    "        #     for idx, pred in enumerate(pred_values):\n",
    "        #         total_seen+=1\n",
    "        #         if pred==y_test.iloc[idx]:\n",
    "        #             total_correct+=1\n",
    "        #         subject_acc.append(total_correct/total_seen)\n",
    "\n",
    "        #     cm = confusion_matrix(y_test.values.reshape(y_test.shape[0]), pred_values)\n",
    "        #     sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "        #     specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "        #     precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "\n",
    "        #     results_dict = {\n",
    "        #         \"accuracy\": accuracy_score(y_test, pred_values),\n",
    "        #         \"sensitivity\": sensitivity,\n",
    "        #         \"specificity\": specificity,\n",
    "        #         \"precision\": precision,\n",
    "        #         \"f1_score\": (2*precision*sensitivity)/(precision+sensitivity),\n",
    "        #         \"subject_acc\": subject_acc\n",
    "        #     }\n",
    "\n",
    "        #     fold_results.append(results_dict)\n",
    "        \n",
    "        current_results = {}\n",
    "        for i, result in enumerate(fold_results):\n",
    "            current_results[f\"split{i}_accuracy\"] = result[\"accuracy\"]\n",
    "            current_results[f\"split{i}_sensitivity\"] = result[\"sensitivity\"]\n",
    "            current_results[f\"split{i}_specificity\"] = result[\"specificity\"]\n",
    "            current_results[f\"split{i}_precision\"] = result[\"precision\"]\n",
    "            current_results[f\"split{i}_f1_score\"] = result[\"f1_score\"]\n",
    "            current_results[f\"split{i}_subject_acc\"] = result[\"subject_acc\"]\n",
    "        results[tuple(sorted(fit_params.items()))] = current_results\n",
    "        results[tuple(sorted(fit_params.items()))]['folds'] = len(splits)\n",
    "\n",
    "with open(f'saved_results_{current_weight}_{recent_size}/dt_results.pickle', 'wb') as handle:\n",
    "    pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_reporter(results, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "folder = './joblib_memmap'\n",
    "try:\n",
    "    os.mkdir(folder)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "dump(X, os.path.join(folder, 'X'))\n",
    "X_memmap = load(os.path.join(folder, 'X'), mmap_mode='r')\n",
    "dump(y, os.path.join(folder, 'y'))\n",
    "y_memmap = load(os.path.join(folder, 'y'), mmap_mode='r')\n",
    "\n",
    "#num_combinations = 30\n",
    "params = {\n",
    "    \"n_estimators\": [300],\n",
    "    \"max_depth\": [30]\n",
    "}\n",
    "param_grid = ParameterGrid(params)\n",
    "print(f'Fitting with {len(param_grid)} different parameter combinations')\n",
    "results = {}\n",
    "\n",
    "with tqdm(param_grid) as pbar:\n",
    "    for fit_params in pbar:\n",
    "        pbar.set_description(f'Fitting parameter combination: {fit_params}')\n",
    "        \n",
    "        clf = RandomForestClassifier(**fit_params)\n",
    "        \n",
    "        fold_results = list(tqdm(Parallel(n_jobs=5, return_as=\"generator\")(\n",
    "            delayed(fit_parallel)(pipe, X_memmap, y_memmap, train, test, **fit_params)\n",
    "            for (train, test) in splits\n",
    "        ), \n",
    "        desc=\"Fitting on each fold\",\n",
    "        total=len(splits)))\n",
    "\n",
    "        # fold_results = []\n",
    "        # for (train, test) in tqdm(splits, desc=\"Fitting for each fold\"):\n",
    "        #     X_train = X.iloc[train]\n",
    "        #     y_train = y.iloc[train]\n",
    "\n",
    "        #     X_test = X.iloc[test]\n",
    "        #     y_test = y.iloc[test]\n",
    "\n",
    "        #     cloned_clf = clone(clf)\n",
    "        #     cloned_clf.fit(X_train.astype(np.float32), y_train.astype(np.float32))\n",
    "\n",
    "        #     pred_values = cloned_clf.predict(X_test)\n",
    "    \n",
    "        #     total_seen = 0\n",
    "        #     total_correct = 0\n",
    "        #     subject_acc = []\n",
    "        #     for idx, pred in enumerate(pred_values):\n",
    "        #         total_seen+=1\n",
    "        #         if pred==y_test.iloc[idx]:\n",
    "        #             total_correct+=1\n",
    "        #         subject_acc.append(total_correct/total_seen)\n",
    "\n",
    "        #     cm = confusion_matrix(y_test.values.reshape(y_test.shape[0]), pred_values)\n",
    "        #     sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "        #     specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "        #     precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "\n",
    "        #     results_dict = {\n",
    "        #         \"accuracy\": accuracy_score(y_test, pred_values),\n",
    "        #         \"sensitivity\": sensitivity,\n",
    "        #         \"specificity\": specificity,\n",
    "        #         \"precision\": precision,\n",
    "        #         \"f1_score\": (2*precision*sensitivity)/(precision+sensitivity),\n",
    "        #         \"subject_acc\": subject_acc\n",
    "        #     }\n",
    "\n",
    "        #     fold_results.append(results_dict)\n",
    "        \n",
    "        current_results = {}\n",
    "        for i, result in enumerate(fold_results):\n",
    "            current_results[f\"split{i}_accuracy\"] = result[\"accuracy\"]\n",
    "            current_results[f\"split{i}_sensitivity\"] = result[\"sensitivity\"]\n",
    "            current_results[f\"split{i}_specificity\"] = result[\"specificity\"]\n",
    "            current_results[f\"split{i}_precision\"] = result[\"precision\"]\n",
    "            current_results[f\"split{i}_f1_score\"] = result[\"f1_score\"]\n",
    "            current_results[f\"split{i}_subject_acc\"] = result[\"subject_acc\"]\n",
    "        results[tuple(sorted(fit_params.items()))] = current_results\n",
    "        results[tuple(sorted(fit_params.items()))]['folds'] = len(splits)\n",
    "\n",
    "with open(f'saved_results_{current_weight}_{recent_size}/rf_results.pickle', 'wb') as handle:\n",
    "    pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_reporter(results, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# AdaBoost\n",
    "folder = './joblib_memmap'\n",
    "try:\n",
    "    os.mkdir(folder)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "dump(X, os.path.join(folder, 'X'))\n",
    "X_memmap = load(os.path.join(folder, 'X'), mmap_mode='r')\n",
    "dump(y, os.path.join(folder, 'y'))\n",
    "y_memmap = load(os.path.join(folder, 'y'), mmap_mode='r')\n",
    "\n",
    "#num_combinations = 30\n",
    "params = {\n",
    "    \"n_estimators\": [300]\n",
    "}\n",
    "param_grid = ParameterGrid(params)\n",
    "print(f'Fitting with {len(param_grid)} different parameter combinations')\n",
    "results = {}\n",
    "\n",
    "with tqdm(param_grid) as pbar:\n",
    "    for fit_params in pbar:\n",
    "        pbar.set_description(f'Fitting parameter combination: {fit_params}')\n",
    "        \n",
    "        clf = AdaBoostClassifier(algorithm=\"SAMME.R\",\n",
    "                                **fit_params)\n",
    "        fold_results = Parallel(n_jobs=4)(\n",
    "            delayed(fit_xgboost_parallel)(clf, X_memmap, y_memmap, train, test, **fit_params)\n",
    "            for (train, test) in splits\n",
    "        )\n",
    "        \n",
    "        current_results = {}\n",
    "        for i, result in enumerate(fold_results):\n",
    "            current_results[f\"split{i}_accuracy\"] = result[\"accuracy\"]\n",
    "            current_results[f\"split{i}_sensitivity\"] = result[\"sensitivity\"]\n",
    "            current_results[f\"split{i}_specificity\"] = result[\"specificity\"]\n",
    "            current_results[f\"split{i}_precision\"] = result[\"precision\"]\n",
    "            current_results[f\"split{i}_f1_score\"] = result[\"f1_score\"]\n",
    "            current_results[f\"split{i}_feature_importances\"] = result[\"feature_importances\"]\n",
    "            current_results[f\"split{i}_subject_acc\"] = result[\"subject_acc\"]\n",
    "        results[tuple(sorted(fit_params.items()))] = current_results\n",
    "        results[tuple(sorted(fit_params.items()))]['folds'] = len(splits)\n",
    "\n",
    "with open(f'saved_results_{current_weight}/ada_results.pickle', 'wb') as handle:\n",
    "    pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_reporter(results, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC\n",
    "folder = './joblib_memmap'\n",
    "try:\n",
    "    os.mkdir(folder)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "dump(X, os.path.join(folder, 'X'))\n",
    "X_memmap = load(os.path.join(folder, 'X'), mmap_mode='r')\n",
    "dump(y, os.path.join(folder, 'y'))\n",
    "y_memmap = load(os.path.join(folder, 'y'), mmap_mode='r')\n",
    "\n",
    "#num_combinations = 30\n",
    "params = {\n",
    "    #\"kernel\": [\"linear\", \"rbf\"]\n",
    "    \"verbose\": [False]\n",
    "}\n",
    "param_grid = ParameterGrid(params)\n",
    "print(f'Fitting with {len(param_grid)} different parameter combinations')\n",
    "results = {}\n",
    "\n",
    "with tqdm(param_grid) as pbar:\n",
    "    for fit_params in pbar:\n",
    "        pbar.set_description(f'Fitting parameter combination: {fit_params}')\n",
    "        \n",
    "        pipe = Pipeline([(\"scaler\", StandardScaler()), \n",
    "                         (\"clf\", LinearSVC(**fit_params))])\n",
    "        \n",
    "        fold_results = list(tqdm(Parallel(n_jobs=1, return_as=\"generator\")(\n",
    "            delayed(fit_parallel)(pipe, X_memmap, y_memmap, train, test, **fit_params)\n",
    "            for (train, test) in splits\n",
    "        ), \n",
    "        desc=\"Fitting on each fold\",\n",
    "        total=len(splits)))\n",
    "\n",
    "        # fold_results = []\n",
    "        # for (train, test) in tqdm(splits, desc=\"Fitting for each fold\"):\n",
    "        #     X_train = X.iloc[train]\n",
    "        #     y_train = y.iloc[train]\n",
    "\n",
    "        #     X_test = X.iloc[test]\n",
    "        #     y_test = y.iloc[test]\n",
    "\n",
    "        #     cloned_clf = clone(pipe)\n",
    "        #     cloned_clf.fit(X_train, y_train)\n",
    "\n",
    "        #     pred_values = cloned_clf.predict(X_test)\n",
    "    \n",
    "        #     total_seen = 0\n",
    "        #     total_correct = 0\n",
    "        #     subject_acc = []\n",
    "        #     for idx, pred in enumerate(pred_values):\n",
    "        #         total_seen+=1\n",
    "        #         if pred==y_test.iloc[idx]:\n",
    "        #             total_correct+=1\n",
    "        #         subject_acc.append(total_correct/total_seen)\n",
    "\n",
    "        #     cm = confusion_matrix(y_test.values.reshape(y_test.shape[0]), pred_values)\n",
    "        #     sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "        #     specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "        #     precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "\n",
    "        #     results_dict = {\n",
    "        #         \"accuracy\": accuracy_score(y_test, pred_values),\n",
    "        #         \"sensitivity\": sensitivity,\n",
    "        #         \"specificity\": specificity,\n",
    "        #         \"precision\": precision,\n",
    "        #         \"f1_score\": (2*precision*sensitivity)/(precision+sensitivity),\n",
    "        #         \"subject_acc\": subject_acc\n",
    "        #     }\n",
    "\n",
    "        #     fold_results.append(results_dict)\n",
    "        \n",
    "        current_results = {}\n",
    "        for i, result in enumerate(fold_results):\n",
    "            current_results[f\"split{i}_accuracy\"] = result[\"accuracy\"]\n",
    "            current_results[f\"split{i}_sensitivity\"] = result[\"sensitivity\"]\n",
    "            current_results[f\"split{i}_specificity\"] = result[\"specificity\"]\n",
    "            current_results[f\"split{i}_precision\"] = result[\"precision\"]\n",
    "            current_results[f\"split{i}_f1_score\"] = result[\"f1_score\"]\n",
    "            current_results[f\"split{i}_subject_acc\"] = result[\"subject_acc\"]\n",
    "        results[tuple(sorted(fit_params.items()))] = current_results\n",
    "        results[tuple(sorted(fit_params.items()))]['folds'] = len(splits)\n",
    "\n",
    "with open(f'saved_results_{current_weight}_{recent_size}/svc_results.pickle', 'wb') as handle:\n",
    "    pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_reporter(results, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "folder = './joblib_memmap'\n",
    "try:\n",
    "    os.mkdir(folder)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "dump(X, os.path.join(folder, 'X'))\n",
    "X_memmap = load(os.path.join(folder, 'X'), mmap_mode='r')\n",
    "dump(y, os.path.join(folder, 'y'))\n",
    "y_memmap = load(os.path.join(folder, 'y'), mmap_mode='r')\n",
    "\n",
    "#num_combinations = 30\n",
    "params = {\n",
    "    \"n_estimators\": [1050],\n",
    "    \"max_depth\": [5]\n",
    "}\n",
    "param_grid = ParameterGrid(params)\n",
    "print(f'Fitting with {len(param_grid)} different parameter combinations')\n",
    "results = {}\n",
    "\n",
    "with tqdm(param_grid) as pbar:\n",
    "    for fit_params in pbar:\n",
    "        pbar.set_description(f'Fitting parameter combination: {fit_params}')\n",
    "        \n",
    "        pipe = Pipeline([(\"scaler\", StandardScaler()), \n",
    "                         (\"clf\", XGBClassifier(learning_rate = 0.1,\n",
    "                            verbose=None, \n",
    "                            eval_metric='logloss',\n",
    "                            tree_method='gpu_hist',\n",
    "                            **fit_params))])\n",
    "        fold_results = list(tqdm(Parallel(n_jobs=1, return_as=\"generator\")(\n",
    "            delayed(fit_tree_parallel)(pipe, X_memmap, y_memmap, train, test, **fit_params)\n",
    "            for (train, test) in splits\n",
    "        ), \n",
    "        desc=\"Fitting on each fold\",\n",
    "        total=len(splits)))\n",
    "        \n",
    "        current_results = {}\n",
    "        for i, result in enumerate(fold_results):\n",
    "            current_results[f\"split{i}_accuracy\"] = result[\"accuracy\"]\n",
    "            current_results[f\"split{i}_sensitivity\"] = result[\"sensitivity\"]\n",
    "            current_results[f\"split{i}_specificity\"] = result[\"specificity\"]\n",
    "            current_results[f\"split{i}_precision\"] = result[\"precision\"]\n",
    "            current_results[f\"split{i}_f1_score\"] = result[\"f1_score\"]\n",
    "            current_results[f\"split{i}_feature_importances\"] = result[\"feature_importances\"]\n",
    "            current_results[f\"split{i}_subject_acc\"] = result[\"subject_acc\"]\n",
    "        results[tuple(sorted(fit_params.items()))] = current_results\n",
    "        results[tuple(sorted(fit_params.items()))]['folds'] = len(splits)\n",
    "\n",
    "with open(f'saved_results_{current_weight}_{recent_size}/xg_results.pickle', 'wb') as handle:\n",
    "    pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_reporter(results, True, True, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost\n",
    "folder = './joblib_memmap'\n",
    "try:\n",
    "    os.mkdir(folder)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "dump(X, os.path.join(folder, 'X'))\n",
    "X_memmap = load(os.path.join(folder, 'X'), mmap_mode='r')\n",
    "dump(y, os.path.join(folder, 'y'))\n",
    "y_memmap = load(os.path.join(folder, 'y'), mmap_mode='r')\n",
    "\n",
    "#num_combinations = 15\n",
    "params = {\n",
    "    \"n_estimators\": [650],\n",
    "    \"max_depth\": [6]\n",
    "}\n",
    "param_grid = ParameterGrid(params)\n",
    "print(f'Fitting with {len(param_grid)} different parameter combinations')\n",
    "results = {}\n",
    "\n",
    "with tqdm(param_grid) as pbar:\n",
    "    for fit_params in pbar:\n",
    "        pbar.set_description(f'Fitting parameter combination: {fit_params}')\n",
    "        \n",
    "        pipe = Pipeline([(\"scaler\", StandardScaler()), \n",
    "                         (\"clf\", CatBoostClassifier(\n",
    "                            learning_rate=0.1,\n",
    "                            loss_function='Logloss',\n",
    "                            task_type=\"GPU\",\n",
    "                            silent=True,\n",
    "                            **fit_params))])\n",
    "        fold_results = list(tqdm(Parallel(n_jobs=1, return_as=\"generator\")(\n",
    "            delayed(fit_tree_parallel)(pipe, X_memmap, y_memmap, train, test, **fit_params)\n",
    "            for (train, test) in splits\n",
    "        ), \n",
    "        desc=\"Fitting on each fold\",\n",
    "        total=len(splits)))\n",
    "        \n",
    "        current_results = {}\n",
    "        for i, result in enumerate(fold_results):\n",
    "            current_results[f\"split{i}_accuracy\"] = result[\"accuracy\"]\n",
    "            current_results[f\"split{i}_sensitivity\"] = result[\"sensitivity\"]\n",
    "            current_results[f\"split{i}_specificity\"] = result[\"specificity\"]\n",
    "            current_results[f\"split{i}_precision\"] = result[\"precision\"]\n",
    "            current_results[f\"split{i}_f1_score\"] = result[\"f1_score\"]\n",
    "            current_results[f\"split{i}_feature_importances\"] = result[\"feature_importances\"]\n",
    "            current_results[f\"split{i}_subject_acc\"] = result[\"subject_acc\"]\n",
    "        results[tuple(sorted(fit_params.items()))] = current_results\n",
    "        results[tuple(sorted(fit_params.items()))]['folds'] = len(splits)\n",
    "\n",
    "with open(f'saved_results_{current_weight}_{recent_size}/cb_results.pickle', 'wb') as handle:\n",
    "    pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_reporter(results, False, True, \"CatBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "folder = './joblib_memmap'\n",
    "try:\n",
    "    os.mkdir(folder)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "dump(X, os.path.join(folder, 'X'))\n",
    "X_memmap = load(os.path.join(folder, 'X'), mmap_mode='r')\n",
    "dump(y, os.path.join(folder, 'y'))\n",
    "y_memmap = load(os.path.join(folder, 'y'), mmap_mode='r')\n",
    "\n",
    "#num_combinations = 15\n",
    "params = {\n",
    "    \"n_estimators\": [1000],\n",
    "    \"max_depth\": [5]\n",
    "}\n",
    "param_grid = ParameterGrid(params)\n",
    "print(f'Fitting with {len(param_grid)} different parameter combinations')\n",
    "results = {}\n",
    "\n",
    "with tqdm(param_grid) as pbar:\n",
    "    for fit_params in pbar:\n",
    "        pbar.set_description(f'Fitting parameter combination: {fit_params}')\n",
    "        \n",
    "        pipe = Pipeline([(\"scaler\", StandardScaler()), \n",
    "                         (\"clf\", lightgbm.LGBMClassifier(\n",
    "                            learning_rate=0.1,\n",
    "                            verbose=-1,\n",
    "                            device=\"cuda\",\n",
    "                            **fit_params))])\n",
    "        fold_results = list(tqdm(Parallel(n_jobs=6, return_as=\"generator\")(\n",
    "            delayed(fit_tree_parallel)(pipe, X_memmap, y_memmap, train, test, **fit_params)\n",
    "            for (train, test) in splits\n",
    "        ), \n",
    "        desc=\"Fitting on each fold\",\n",
    "        total=len(splits)))\n",
    "        # fold_results = []\n",
    "        # for (train, test) in tqdm(splits, desc=\"Fitting for each fold\"):\n",
    "        #     X_train = X.iloc[train]\n",
    "        #     y_train = y.iloc[train]\n",
    "\n",
    "        #     X_test = X.iloc[test]\n",
    "        #     y_test = y.iloc[test]\n",
    "\n",
    "        #     cloned_clf = clone(pipe)\n",
    "        #     cloned_clf.fit(X_train,y_train)\n",
    "\n",
    "        #     pred_values = cloned_clf.predict(X_test)\n",
    "    \n",
    "        #     total_seen = 0\n",
    "        #     total_correct = 0\n",
    "        #     subject_acc = []\n",
    "        #     for idx, pred in enumerate(pred_values):\n",
    "        #         total_seen+=1\n",
    "        #         if pred==y_test.iloc[idx]:\n",
    "        #             total_correct+=1\n",
    "        #         subject_acc.append(total_correct/total_seen)\n",
    "\n",
    "        #     cm = confusion_matrix(y_test.values.reshape(y_test.shape[0]), pred_values)\n",
    "        #     sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "        #     specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "        #     precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "\n",
    "        #     results_dict = {\n",
    "        #         \"accuracy\": accuracy_score(y_test, pred_values),\n",
    "        #         \"sensitivity\": sensitivity,\n",
    "        #         \"specificity\": specificity,\n",
    "        #         \"precision\": precision,\n",
    "        #         \"f1_score\": (2*precision*sensitivity)/(precision+sensitivity),\n",
    "        #         \"feature_importances\": {A: B for A, B in zip(cloned_clf.feature_names_in_, cloned_clf['clf'].feature_importances_)}, \n",
    "        #         \"subject_acc\": subject_acc\n",
    "        #     }\n",
    "\n",
    "        #     fold_results.append(results_dict)\n",
    "        \n",
    "        current_results = {}\n",
    "        for i, result in enumerate(fold_results):\n",
    "            current_results[f\"split{i}_accuracy\"] = result[\"accuracy\"]\n",
    "            current_results[f\"split{i}_sensitivity\"] = result[\"sensitivity\"]\n",
    "            current_results[f\"split{i}_specificity\"] = result[\"specificity\"]\n",
    "            current_results[f\"split{i}_precision\"] = result[\"precision\"]\n",
    "            current_results[f\"split{i}_f1_score\"] = result[\"f1_score\"]\n",
    "            current_results[f\"split{i}_feature_importances\"] = result[\"feature_importances\"]\n",
    "            current_results[f\"split{i}_subject_acc\"] = result[\"subject_acc\"]\n",
    "        results[tuple(sorted(fit_params.items()))] = current_results\n",
    "        results[tuple(sorted(fit_params.items()))]['folds'] = len(splits)\n",
    "\n",
    "with open(f'saved_results_{current_weight}_{recent_size}/lg_results.pickle', 'wb') as handle:\n",
    "    pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_reporter(results, False, True, \"LightGBM\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "afib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
