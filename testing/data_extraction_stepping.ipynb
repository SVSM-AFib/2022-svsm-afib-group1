{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' \n",
    "\n",
    "from pathlib import Path, PurePath\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wfdb\n",
    "\n",
    "import msgpack\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlist = []\n",
    "rawpath = PurePath(Path(os.getcwd()).parents[0], 'mit-bih-raw/') # raw data should be stored in this folder in the root of the repo\n",
    "records = rawpath / 'RECORDS'\n",
    "with open(records) as rfile:\n",
    "    for record in rfile:\n",
    "        record = record[0:len(record)-1] # Remove any erronious new line characters at the end ('\\n')\n",
    "        rlist.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [] # will house the samples of all subjects\n",
    "good_list = [] # will list the names of the subjects we successfully extracted\n",
    "bad_list = [] # will house the names of the subjects we failed to extract\n",
    "qrs = [] # will house the indices of R-Peaks for all subjects\n",
    "atr_label = [] # will house the labels for each rhythm annotation for all subjects\n",
    "atr_locs = [] # will house the locations corresponding to the rhythm annotation labels\n",
    "\n",
    "for x in tqdm(rlist): #this will iterate through te records that we found above\n",
    "    try:\n",
    "        rpath = str(rawpath / x)\n",
    "        samp = wfdb.rdsamp(rpath) # wfdb._____(file_location) will read the signal & header data and return a 2 value array\n",
    "            # samp[0] - the signal data is the raw reading from the ecg. Each value is a sample taken.\n",
    "            # samp[1] - the header data includes things about the signal data such as:\n",
    "              # samples per section, denoted 'fs'\n",
    "              # number of signals, denoted 'n_sig'\n",
    "        ######################################################\n",
    "        samples.append(samp) #add it to our array for all subject\n",
    "        \n",
    "            #What is our file extension that has the annotation we want? Find it here and replace _____ with it \n",
    "            #hint: READ THE VARIABLE NAMES!!!!\n",
    "        qrs_tmp = wfdb.rdann(rpath, extension=\"qrs\") #extract the QRS Info\n",
    "        qrs_locs = np.array(qrs_tmp.sample, dtype='int') #Get just the loccation of R-Peaks from the QRS Info\n",
    "        qrs.append(qrs_locs) # Add to our array for all subjects\n",
    "        \n",
    "            #Do the same thing here\n",
    "        atr = wfdb.rdann(rpath, extension=\"atr\") #extract the atr info which stores the rhythm type(s) over the whole signal\n",
    "        atr_label.append(atr.aux_note) # aux_note stores the type of rhythm - main two are '(N' for normal and '(AFIB' for AFIB\n",
    "        atr_locs.append(np.append(atr.sample, len(samp[0]))) #I add the length of the whole sample to the end for better visualization later\n",
    "        \n",
    "        good_list.append(x) # when all extraction is successful append the record name to good_list\n",
    "    except Exception as exep:\n",
    "        tqdm.write(str(exep)) # Alert the user of an exception\n",
    "        bad_list.append(x) # add to the bad list\n",
    "\n",
    "rlist = good_list # ignore the bad ones from now on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atr_dics = [] #Initialize the array that will hold the dictionary for each subject\n",
    "\n",
    "for idxs,lab in enumerate(atr_label):\n",
    "    atr_dic = {} #Initialize dictionary for each subject\n",
    "    for idx,x in enumerate(lab):\n",
    "        if x not in atr_dic.keys():\n",
    "            atr_dic[x] = [] #Add dictionary key if does not exist\n",
    "        atr_dic[x].append([atr_locs[idxs][idx], atr_locs[idxs][idx+1]]) #Insert range for each rhythm\n",
    "    atr_dics.append(atr_dic) #Add to dictionary array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_flag = True # set to True to rewrite all current data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpath = PurePath(Path(os.getcwd()).parents[0], 'mit-bih-dataframes-stepping/')\n",
    "if not os.path.exists(dfpath):\n",
    "    os.mkdir(dfpath)\n",
    "for s, id in enumerate(tqdm(good_list)): # Iterate through all of the subjects that we have complete data of \n",
    "    subj = pd.DataFrame( # The below statements initialize our datafram. The first to columns will be our given signals, and the rest we initialize to 0\n",
    "        data = np.transpose(np.array([ # First we give our data, for pandas they want the data by row instead of by column, so we use transpose to get the proper format\n",
    "                                               [x[0] for x in samples[s][0]],\n",
    "                                               [x[1] for x in samples[s][0]]\n",
    "                                        ])\n",
    "                           ),\n",
    "        columns = ['Signal 1', 'Signal 2'] # Here we name our columns to match the dataframe we outlined above\n",
    "    )\n",
    "    norm = [] # Initialize the norm array which will list every index the person is in a normal rhythm\n",
    "    if '(N' in atr_dics[s].keys():\n",
    "        for x in atr_dics[s]['(N']: # Then we iterate through our ranges we extracted above\n",
    "            norm = norm + list(range(x[0], x[1])) # And add all values in the range to our norm array\n",
    "    af = [] # Then we do the same steps above for AFIB rhythms\n",
    "    if '(AFIB' in atr_dics[s].keys():\n",
    "        for x in atr_dics[s]['(AFIB']:\n",
    "            af = af + list(range(x[0], x[1]))\n",
    "    subj['R-Peak'] = subj.index.isin(qrs[s]) # the isin() function of a DataFram index will return true if the index is in that list and false if it is not\n",
    "                                            # then, we can initialize our dataFrame with correct values based on that\n",
    "    subj['Normal'] = subj.index.isin(norm)\n",
    "    subj['AFIB'] = subj.index.isin(af)\n",
    "    subj['Other'] = ~subj.index.isin(np.append(norm, af)) # Because we are classifying AFIB specifically we define other as any rhythm not in the norm or AFIB list\n",
    "    \n",
    "    if not os.path.exists(dfpath / (id+'.parquet')) or reload_flag:\n",
    "        subj.to_parquet(dfpath / (id+'.parquet'))\n",
    "\n",
    "np.savetxt(dfpath / \"subject_list.csv\", good_list, delimiter=\",\",  fmt='%s') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractedpath = PurePath(Path(os.getcwd()).parents[0], 'mit-bih-extracted-stepping/')\n",
    "if not os.path.exists(extractedpath):\n",
    "    os.mkdir(extractedpath)\n",
    "\n",
    "def encoder(obj): # encoder function for msgpack--small serialization format used to store headers/rhythms\n",
    "    if isinstance(obj, datetime.time):\n",
    "        return {'__datetime__': True, 'as_str': obj.strftime(\"%H:%M:%S.%f\")}\n",
    "    if isinstance(obj, np.int64):\n",
    "        return {'__npint64__': True, 'as_int': int(obj)}\n",
    "    return obj\n",
    "\n",
    "def get_rhythm_type(samp, rhythms): # helper functions to match R-peaks to rhythm labels based on rhythm dictionaries\n",
    "    if \"(N\" in rhythms.keys():\n",
    "        for span in rhythms[\"(N\"]:\n",
    "            if span[0] <= samp <= span[1]:\n",
    "                return \"N\"\n",
    "    if \"(AFIB\" in rhythms.keys():\n",
    "        for span in rhythms[\"(AFIB\"]:\n",
    "            if span[0] <= samp <= span[1]:\n",
    "                return \"A\"\n",
    "    return \"O\"\n",
    "\n",
    "np.savetxt(extractedpath / \"subject_list.csv\", good_list, delimiter=\",\",  fmt='%s')\n",
    "for idx, x in enumerate(tqdm(good_list)):\n",
    "    if not os.path.exists(extractedpath / x): # creating folder for each subject\n",
    "        os.mkdir(extractedpath / x)\n",
    "\n",
    "    if not os.path.exists(extractedpath / x / \"signals.parquet\") or reload_flag: # saving signals data\n",
    "        signaldf = pd.DataFrame(np.array(samples[idx][0]), columns=[\"signal1\", \"signal2\"])\n",
    "        signaldf.to_parquet(extractedpath / x / \"signals.parquet\")\n",
    "\n",
    "    if not os.path.exists(extractedpath / x / \"rpeaks.parquet\") or reload_flag:\n",
    "        rpeaksdf = pd.DataFrame({\n",
    "            'rpeak': qrs[idx],\n",
    "            'rhythm': [get_rhythm_type(samp, atr_dics[idx]) for samp in qrs[idx]] # matching rhythms to R-peaks \n",
    "        })\n",
    "        rpeaksdf.to_parquet(extractedpath / x / \"rpeaks.parquet\") # saving the data\n",
    "\n",
    "    if not os.path.exists(extractedpath / x / \"headers.msgpack\") or reload_flag: # saving headers data\n",
    "        with open(extractedpath / x / \"headers.msgpack\", 'wb') as outfile:\n",
    "            outfile.write(msgpack.packb(samples[idx][1], default=encoder))\n",
    "\n",
    "    if not os.path.exists(extractedpath / x / \"rhythms.msgpack\") or reload_flag: # saving rhythms data\n",
    "        with open(extractedpath / x / \"rhythms.msgpack\", 'wb') as outfile:\n",
    "            outfile.write(msgpack.packb(atr_dics[idx], default=encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpeak_dfs = {}\n",
    "for record in tqdm(rlist): # reopening R-peaks/rhythms DataFrames\n",
    "    rpeak_dfs[record] = pd.read_parquet(extractedpath / record / 'rpeaks.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rmean(rrInts): # function to calculate R-mean for subset\n",
    "    rmeans = []\n",
    "    for index, value in enumerate(rrInts):\n",
    "        if index==0:\n",
    "            rmeans.append(value)\n",
    "        else:\n",
    "            rmeans.append(0.75*rmeans[index-1] + 0.25*value)\n",
    "    return rmeans\n",
    "\n",
    "def subset_subject(rpeak_df, interval_length = 4, calib_length = 100):\n",
    "    rpeaks = rpeak_df['rpeak'].to_numpy() # get the R-peak\n",
    "    peakrhythms = rpeak_df['rhythm'].to_numpy() # and rhythm columns out of the DataFrame\n",
    "    raw_rr_ints = np.diff(rpeaks) # do the subtraction to get a new array filled with all the RR-intervals\n",
    "    mask = raw_rr_ints <= 500 # use a boolean mask to filter all of the outliers (greater than 500 samples)\n",
    "    rr_ints = raw_rr_ints[mask] # filter both RR-intervals\n",
    "    intrhythms = peakrhythms[1:][mask] # and the rhythms array\n",
    "\n",
    "    subsets = [] # create a list for the subset DataFrames\n",
    "\n",
    "    calib_rr_ints = rr_ints[:calib_length] # take the slice of calib_length intervals from the beginning of the array\n",
    "    calib_rhythms = intrhythms[:calib_length] # take the matching slice of rhythms\n",
    "    calib_rmean = extract_rmean(calib_rr_ints) # calculate the R-mean for the calibration intervals\n",
    "    subsets.append(pd.DataFrame({'rhythmLabel': calib_rhythms, 'rrInt': calib_rr_ints, 'rmean': calib_rmean})) # add the calibration subset to the list\n",
    "\n",
    "    remaining_len = len(rr_ints) - calib_length # get the remaining number of intervals in the array\n",
    "    end = len(rr_ints) - (remaining_len % interval_length)  # and do the math to find where we should cut it off to get \n",
    "                                                            # even subsets of interval_length length\n",
    "    remaining_ints = rr_ints[calib_length:end] # do the slicing on both the RR-intervals\n",
    "    remaining_rhythms = intrhythms[calib_length:end] # and the rhythms\n",
    "\n",
    "    remaining_chunks = np.split(remaining_ints, remaining_len//interval_length) # use numpy to split the RR-intervals\n",
    "    remaining_rhythm_chunks = np.split(remaining_rhythms, remaining_len//interval_length) # and rhythms into the even subsets\n",
    "    subsets.extend([\n",
    "        pd.DataFrame({'rhythmLabel': rhythms, \n",
    "                      'rrInt': chunk, \n",
    "                      'rmean': extract_rmean(chunk)}) for chunk, rhythms in zip(remaining_chunks, remaining_rhythm_chunks)\n",
    "    ]) # calculate the rmean for each chunk and append a new DataFrame for each subset to the list\n",
    "\n",
    "    return subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_dfs = {}\n",
    "for record in tqdm(rlist): # subset each subject\n",
    "    subset_dfs[record] = subset_subject(rpeak_dfs[record])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsetpath = PurePath(Path(os.getcwd()).parents[0], 'mit-bih-time-subsets-stepping/')\n",
    "if not os.path.exists(subsetpath):\n",
    "    os.mkdir(subsetpath)\n",
    "\n",
    "for record in tqdm(rlist): # saving all of the subsets--might take a while\n",
    "    subsets = subset_dfs[record] # get the subsets list out of the dictionary\n",
    "    idx_list = list(range(len(subsets))) # create a list of numbers from 0...len(subsets)-1 to label them\n",
    "    \n",
    "    subset_list = pd.DataFrame({\n",
    "        \"subjectID\": [record]*len(subsets),\n",
    "        \"subsetID\": idx_list,\n",
    "        \"rhythmLabel\": [subsets[x]['rhythmLabel'].mode()[0] for x in idx_list]\n",
    "    })  # create the subset list DataFrame--ID info for each subset as well as a rhythm label\n",
    "        # that is the most common rhythm in each of the subsets\n",
    "    # map the rhythm labels to other values in a new column, will be useful later for classifying\n",
    "    subset_list['mappedLabel'] = subset_list['rhythmLabel'].map({'N': 'Non-Afib', 'A': 'Afib', 'O': 'Non-Afib'})\n",
    "    subset_list.to_parquet(subsetpath / (record+\"_subset_list.parquet\")) # save the subset list\n",
    "    \n",
    "    recordpath = subsetpath / (record)\n",
    "    os.mkdir(recordpath)\n",
    "    for x, subset in enumerate(subsets): # save each of the individual subset DataFrames (this is a lot of files)\n",
    "        subset.to_parquet(recordpath / (str(record)+\"-\"+str(idx_list[x])+\".parquet\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:afib]",
   "language": "python",
   "name": "conda-env-afib-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "136caa03c8b06f0fc9c257b7322286633810b6af46889602f3f7438f2bf5662f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
