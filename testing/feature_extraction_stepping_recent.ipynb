{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import antropy as ant\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from pathlib import Path, PurePath\n",
    "import csv\n",
    "from scipy import stats\n",
    "from joblib import Parallel, delayed, dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlist = []\n",
    "extractedpath = PurePath(Path(os.getcwd()).parents[0], 'mit-bih-extracted/')\n",
    "records = extractedpath / 'subject_list.csv'\n",
    "with open(records) as rfile: # reads in all of the subject IDs\n",
    "    recordreader = csv.reader(rfile, delimiter=' ', quotechar='|')\n",
    "    for row in recordreader:\n",
    "        rlist.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_extract(data, idx, N, window_size, current_weight, prev_weight):\n",
    "    previous_window = data[idx - N*window_size:idx]\n",
    "    current_window = data[idx:idx + window_size]\n",
    "\n",
    "    if len(current_window) == window_size:\n",
    "        return extract_all(current_window[:, 1], current_window[:, 2])*current_weight + extract_all(previous_window[:, 1], previous_window[:, 2])*prev_weight\n",
    "    else:\n",
    "        return extract_all(previous_window[:, 1], previous_window[:, 2])*prev_weight\n",
    "\n",
    "def extract_all(rrInts, rmeans):\n",
    "    conditions = [\n",
    "        rrInts < (0.85*rmeans),\n",
    "        rrInts > (1.15*rmeans)\n",
    "    ]\n",
    "    choices = [0, 2]\n",
    "\n",
    "    int_types = np.select(conditions, choices, default=1)\n",
    "\n",
    "    first = int_types[:-1]\n",
    "    second = int_types[1:]\n",
    "    count = len(int_types)-1\n",
    "    StoS = np.count_nonzero(np.logical_and(first == 0, second == 0)) / count\n",
    "    StoR = np.count_nonzero(np.logical_and(first == 0, second == 1)) / count\n",
    "    StoL = np.count_nonzero(np.logical_and(first == 0, second == 2)) / count\n",
    "    RtoS = np.count_nonzero(np.logical_and(first == 1, second == 0)) / count\n",
    "    RtoR = np.count_nonzero(np.logical_and(first == 1, second == 1)) / count\n",
    "    RtoL = np.count_nonzero(np.logical_and(first == 1, second == 2)) / count\n",
    "    LtoS = np.count_nonzero(np.logical_and(first == 2, second == 0)) / count\n",
    "    LtoR = np.count_nonzero(np.logical_and(first == 2, second == 1)) / count\n",
    "    LtoL = np.count_nonzero(np.logical_and(first == 2, second == 2)) / count\n",
    "\n",
    "    stdev = np.std(rrInts)\n",
    "    cov = stdev / np.mean(rrInts)\n",
    "    rr_range = np.max(rrInts) - np.min(rrInts)\n",
    "    rrInt_var = rrInts.var()\n",
    "    rmean_var = rmeans.var()\n",
    "    rmssd = extract_rmssd(rrInts)\n",
    "    mad = stats.median_abs_deviation(rrInts)\n",
    "    iqr = stats.iqr(rrInts)\n",
    "\n",
    "    shannon = shannon_entropy(int_types)\n",
    "    approx = ant.app_entropy(rrInts)\n",
    "\n",
    "    return np.array([StoS, StoR, StoL, RtoS, RtoR, RtoL, LtoS, LtoR, LtoL, stdev, cov, rr_range, rrInt_var, rmean_var, rmssd, mad, iqr, shannon, approx])\n",
    "\n",
    "def extract_rmssd(rrInts): # calculate the RMSSD of a subset\n",
    "    diffs = np.diff(rrInts)\n",
    "    sum_of_squares = np.sum(diffs**2)\n",
    "    return np.sqrt(sum_of_squares/len(diffs))\n",
    "\n",
    "def shannon_entropy(subset):\n",
    "    # Get the frequency of each rrint classification in the data\n",
    "    unique, frequencies = np.unique(subset, return_counts=True)\n",
    "    \n",
    "    # Calculate the probability of each classification\n",
    "    probabilities = frequencies / len(subset)\n",
    "    \n",
    "    # Calculate the Shannon entropy\n",
    "    entropy = -np.sum(probabilities*np.log2(probabilities))\n",
    "\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_features(record, subsetpath, N = 50, current_weight = 0.4, prev_weight = 0.6):\n",
    "    subset_list = pd.read_parquet(subsetpath / (record+'_subset_list.parquet')) # read the subset list for that subject\n",
    "\n",
    "    all_intervals = pd.concat([pd.read_parquet(subsetpath / record / (record+\"-\"+str(idx)+\".parquet\")) for idx in tqdm(range(1, len(subset_list)), desc=\"Reading all windows\")], ignore_index=True)\n",
    "    filename = Path(\"joblib_memmap\")\n",
    "    if not filename.exists():\n",
    "        filename.mkdir()\n",
    "    dump(all_intervals.to_numpy(), filename / \"all_intervals.memmap\")\n",
    "    all_memmap = load(filename / \"all_intervals.memmap\", mmap_mode=\"r\")\n",
    "\n",
    "    window_size = 4\n",
    "\n",
    "    rows = list(\n",
    "        tqdm(Parallel(n_jobs=6, return_as=\"generator\", max_nbytes=None, batch_size=100)(\n",
    "            delayed(parallel_extract)(all_memmap, idx, N, window_size, current_weight, prev_weight) \n",
    "            for idx in range(N*window_size, (len(subset_list)*window_size) - window_size)),\n",
    "            desc=\"Calculating features for each window\", \n",
    "            total=(len(subset_list)*window_size) - window_size - (N*window_size))\n",
    "    )\n",
    "    \n",
    "    feature_arr = np.vstack(rows)\n",
    "    columns = ['StoS', 'StoR', 'StoL', 'RtoS', 'RtoR', 'RtoL', 'LtoS', 'LtoR', 'LtoL', 'std', 'cov', 'range', 'rrInt_var', 'rmean_var', 'rmssd', 'mad', 'iqr','entropy','approx_entropy']\n",
    "    feature_df = pd.DataFrame(data=feature_arr, columns=columns) # make a DataFrame out of the feature dictionary\n",
    "    return pd.concat([subset_list, feature_df], axis=1) # return the features DataFrame combined with the subset list DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3916d57f144cbdba8558fe3e0b27cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f138d3cf4e4a6698bc182aa3be23cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading all windows:   0%|          | 0/7315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6601fcd6910c40a6a3492a0532dd2d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating features for each window:   0%|          | 0/29060 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd02756ff3e24c059dd047f6c131ad65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading all windows:   0%|          | 0/10301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b394012125146dd86deeee18e6db144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating features for each window:   0%|          | 0/41004 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c15c37c79e75408d8acdc5432683207d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading all windows:   0%|          | 0/6630 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "subsetpath = PurePath(Path(os.getcwd()).parents[0], 'mit-bih-time-subsets/')\n",
    "featurespath = PurePath(Path(os.getcwd()).parents[0], 'mit-bih-time-features/')\n",
    "if not os.path.exists(featurespath):\n",
    "    os.mkdir(featurespath)\n",
    "\n",
    "for record in tqdm(rlist): # calculate the features for all of the subjects\n",
    "    features = subset_features(record, subsetpath)\n",
    "    features.to_parquet(featurespath / (record+\".parquet\")) # and then write them to disk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "afib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
