{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import antropy as ant\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from pathlib import Path, PurePath\n",
    "import csv\n",
    "from scipy import stats\n",
    "from collections import Counter, defaultdict\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlist = []\n",
    "extractedpath = PurePath(Path(os.getcwd()).parents[0], 'mit-bih-extracted/')\n",
    "records = extractedpath / 'subject_list.csv'\n",
    "with open(records) as rfile: # reads in all of the subject IDs\n",
    "    recordreader = csv.reader(rfile, delimiter=' ', quotechar='|')\n",
    "    for row in recordreader:\n",
    "        rlist.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_rr_ints(df):\n",
    "    conditions = [\n",
    "        df['rrInt'] < (0.85*df['rmean']),\n",
    "        df['rrInt'] > (1.15*df['rmean'])\n",
    "    ]\n",
    "    choices = [0, 2]\n",
    "\n",
    "    return np.select(conditions, choices, default=1)\n",
    "\n",
    "def find_proportions(int_types): # take the interval types and count the transitions/return the proportions\n",
    "    first = int_types[:-1]\n",
    "    second = int_types[1:]\n",
    "    StoS = np.count_nonzero(np.logical_and(first == 0, second == 0))\n",
    "    StoR = np.count_nonzero(np.logical_and(first == 0, second == 1))\n",
    "    StoL = np.count_nonzero(np.logical_and(first == 0, second == 2))\n",
    "    RtoS = np.count_nonzero(np.logical_and(first == 1, second == 0))\n",
    "    RtoR = np.count_nonzero(np.logical_and(first == 1, second == 1))\n",
    "    RtoL = np.count_nonzero(np.logical_and(first == 1, second == 2))\n",
    "    LtoS = np.count_nonzero(np.logical_and(first == 2, second == 0))\n",
    "    LtoR = np.count_nonzero(np.logical_and(first == 2, second == 1))\n",
    "    LtoL = np.count_nonzero(np.logical_and(first == 2, second == 2))\n",
    "    count = len(int_types)-1\n",
    "    return [StoS/count, StoR/count, StoL/count, RtoS/count, RtoR/count, RtoL/count, LtoS/count, LtoR/count, LtoL/count]\n",
    "\n",
    "def extract_rmssd(rrInts): # calculate the RMSSD of a subset\n",
    "    diffs = np.diff(rrInts)\n",
    "    sum_of_squares = np.sum(diffs**2)\n",
    "    return np.sqrt(sum_of_squares/len(diffs))\n",
    "\n",
    "def shannon_entropy(subset):\n",
    "    # Get the frequency of each rrint classification in the data\n",
    "    unique, frequencies = np.unique(subset, return_counts=True)\n",
    "    \n",
    "    # Calculate the probability of each classification\n",
    "    probabilities = frequencies / len(subset)\n",
    "    \n",
    "    # Calculate the Shannon entropy\n",
    "    entropy = -np.sum(probabilities*np.log2(probabilities))\n",
    "\n",
    "    return entropy\n",
    "\n",
    "def approx_entropy(subset, m=2, r=None):\n",
    "    def _maxdist(x_i, x_j):\n",
    "        return max([abs(ua - va) for ua, va in zip(x_i, x_j)])\n",
    "\n",
    "    def _phi(m):\n",
    "        x = [[subset[j] for j in range(i, i + m - 1 + 1)] for i in range(N - m + 1)]\n",
    "        C = [len([1 for x_j in x if _maxdist(x_i, x_j) <= r]) / (N - m + 1.0) for x_i in x]\n",
    "        return (N - m + 1.0)**(-1) * np.sum(np.log(C))\n",
    "\n",
    "    N = len(subset)\n",
    "\n",
    "    if r is None:\n",
    "        r = 0.2 * np.std(subset)\n",
    "\n",
    "    return abs(_phi(m+1) - _phi(m))\n",
    "\n",
    "def calc_range(arr):\n",
    "    return np.max(arr) - np.min(arr)\n",
    "\n",
    "def calc_cov(arr):\n",
    "    return np.std(arr)/np.mean(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_features(record, subsetpath, N = 50, current_weight = 0.25, prev_weight = 0.75):\n",
    "    subset_list = pd.read_parquet(subsetpath / (record+'_subset_list.parquet')) # read the subset list for that subject\n",
    "    feature_dict = defaultdict(list) # create a dictionary to store all of the features\n",
    "    all_intervals = pd.concat([pd.read_parquet(subsetpath / record / (record+\"-\"+str(idx)+\".parquet\")) for idx in tqdm(range(1, len(subset_list)), desc=\"Reading all windows\")], ignore_index=True)\n",
    "\n",
    "    window_size = 4\n",
    "\n",
    "    # looping over all of the windows\n",
    "    for idx in tqdm(range(N*window_size, (len(subset_list)*window_size) - window_size), desc=\"Calculating features for each window\"):\n",
    "        previous_window = all_intervals.iloc[idx - N*window_size:idx]\n",
    "        current_window = all_intervals.iloc[idx:idx + window_size]\n",
    "\n",
    "        # calculate the features while including the weights\n",
    "        current_classifications = classify_rr_ints(current_window)\n",
    "        previous_classifications = classify_rr_ints(previous_window)\n",
    "        current_props = find_proportions(current_classifications)\n",
    "        previous_props = find_proportions(previous_classifications)\n",
    "        feature_dict['StoS'].append(current_props[0]*current_weight + previous_props[0]*prev_weight)\n",
    "        feature_dict['StoR'].append(current_props[1]*current_weight + previous_props[1]*prev_weight)\n",
    "        feature_dict['StoL'].append(current_props[2]*current_weight + previous_props[2]*prev_weight)\n",
    "        feature_dict['RtoS'].append(current_props[3]*current_weight + previous_props[3]*prev_weight)\n",
    "        feature_dict['RtoR'].append(current_props[4]*current_weight + previous_props[4]*prev_weight)\n",
    "        feature_dict['RtoL'].append(current_props[5]*current_weight + previous_props[5]*prev_weight)\n",
    "        feature_dict['LtoS'].append(current_props[6]*current_weight + previous_props[6]*prev_weight)\n",
    "        feature_dict['LtoR'].append(current_props[7]*current_weight + previous_props[7]*prev_weight)\n",
    "        feature_dict['LtoL'].append(current_props[8]*current_weight + previous_props[8]*prev_weight)\n",
    "\n",
    "        feature_dict['std'].append(np.std(current_window['rrInt'])*current_weight + np.std(previous_window['rrInt'])*prev_weight)\n",
    "        feature_dict['cov'].append(calc_cov(current_window['rrInt'])*current_weight + calc_cov(previous_window['rrInt'])*prev_weight)\n",
    "        feature_dict['range'].append(calc_range(current_window['rrInt'])*current_weight + calc_range(previous_window['rrInt'])*prev_weight)\n",
    "        feature_dict['rrInt_var'].append(current_window['rrInt'].var()*current_weight + previous_window['rrInt'].var()*prev_weight)\n",
    "        feature_dict['rmean_var'].append(current_window['rmean'].var()*current_weight + previous_window['rmean'].var()*prev_weight)\n",
    "        feature_dict['rmssd'].append(extract_rmssd(current_window['rrInt'])*current_weight + extract_rmssd(previous_window['rrInt'])*prev_weight)\n",
    "        feature_dict['mad'].append(stats.median_abs_deviation(current_window['rrInt'])*current_weight + stats.median_abs_deviation(previous_window['rrInt'])*prev_weight)\n",
    "        feature_dict['iqr'].append(stats.iqr(current_window['rrInt'])*current_weight + stats.iqr(previous_window['rrInt'])*prev_weight)\n",
    "\n",
    "        feature_dict['entropy'].append(shannon_entropy(current_classifications)*current_weight + shannon_entropy(previous_classifications)*prev_weight)\n",
    "        feature_dict['approx_entropy'].append(ant.app_entropy(current_window['rrInt'])*current_weight + ant.app_entropy(previous_window['rrInt'])*prev_weight)\n",
    "\n",
    "    feature_df = pd.DataFrame(data=feature_dict) # make a DataFrame out of the feature dictionary\n",
    "    return pd.concat([subset_list, feature_df], axis=1) # return the features DataFrame combined with the subset list DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsetpath = PurePath(Path(os.getcwd()).parents[0], 'mit-bih-time-subsets/')\n",
    "featurespath = PurePath(Path(os.getcwd()).parents[0], 'mit-bih-time-features/')\n",
    "if not os.path.exists(featurespath):\n",
    "    os.mkdir(featurespath)\n",
    "\n",
    "for record in tqdm(rlist): # calculate the features for all of the subjects\n",
    "    features = subset_features(record, subsetpath)\n",
    "    features.to_parquet(featurespath / (record+\".parquet\")) # and then write them to disk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "afib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
